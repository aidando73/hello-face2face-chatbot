Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0018.flac
Waveform stats - mean: -0.0000, std: 0.0448, min: -0.4029, max: 0.4105
Resampled waveform stats - mean: -0.0000, std: 0.0448, min: -0.4029, max: 0.4105
Raw mel spectrogram stats - mean: 0.7512, std: 4.5781, min: 0.0000, max: 149.2996
Log mel spectrogram stats - mean: -5.2328, std: 3.7545, min: -13.3768, max: 5.0060
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1692, max: 2.7271
Mel spec shape: torch.Size([1, 80, 499])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1699, max: 2.7266
CNN output stats - mean: 0.2830, std: 0.5884, min: -0.1699, max: 3.8066
Transformer output stats - mean: -0.0000, std: 1.0000, min: -2.8965, max: 3.4707
Final output stats - mean: -0.0018, std: 0.1890, min: -0.7397, max: 0.6802
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: -0.0018, std: 0.1890

Sample prediction:
Target: THE CARAVAN WAS DIVIDED INTO THREE COMPARTMENTS PARTITIONED FROM EACH OTHER
Prediction:  is11ll3ll干事创业 in1m1踔//&.&\_.
Loss: 12.6163
outputs.loss tensor(12.6163, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0021.flac
Waveform stats - mean: 0.0000, std: 0.0599, min: -0.6021, max: 0.5816
Resampled waveform stats - mean: 0.0000, std: 0.0599, min: -0.6021, max: 0.5816
Raw mel spectrogram stats - mean: 1.3403, std: 8.6563, min: 0.0000, max: 614.6301
Log mel spectrogram stats - mean: -4.5275, std: 3.4449, min: -12.7123, max: 6.4210
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3759, max: 3.1782
Mel spec shape: torch.Size([1, 80, 1270])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3750, max: 3.1777
CNN output stats - mean: 0.2805, std: 0.5903, min: -0.1699, max: 3.9648
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.0820, max: 3.1816
Final output stats - mean: -0.0021, std: 0.1917, min: -0.7051, max: 0.7656
audio_emb.shape torch.Size([1, 80, 3584])
Audio embedding stats - mean: -0.0021, std: 0.1919

Sample prediction:
Target: THE GREATER THE DANGER THE GREATER IS THE NEED OF AGREEING QUICKLY AND READILY ABOUT WHAT IS NECESSARY NOT TO MISUNDERSTAND ONE ANOTHER IN DANGER THAT IS WHAT CANNOT AT ALL BE DISPENSED WITH IN INTERCOURSE
Prediction:  is1ll3 toll\n\n\n\n\n\n\n\n\n\n   --
\n\n-</-</-</</--

-</</</--

-\'\'-&-</.mutable%s&&-{--len&%s&
Loss: 14.3354
outputs.loss tensor(14.3354, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0027.flac
Waveform stats - mean: -0.0000, std: 0.0398, min: -0.2597, max: 0.2457
Resampled waveform stats - mean: -0.0000, std: 0.0398, min: -0.2597, max: 0.2457
Raw mel spectrogram stats - mean: 0.5905, std: 4.4834, min: 0.0000, max: 188.4676
Log mel spectrogram stats - mean: -6.1385, std: 4.0080, min: -13.8035, max: 5.2389
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9124, max: 2.8387
Mel spec shape: torch.Size([1, 80, 398])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9121, max: 2.8379
CNN output stats - mean: 0.2861, std: 0.5806, min: -0.1699, max: 3.3379
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.2480, max: 2.8750
Final output stats - mean: -0.0013, std: 0.1952, min: -0.7085, max: 0.6987
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: -0.0013, std: 0.1952

Sample prediction:
Target: A MISFORTUNE OF BIRTH PLACED ME HERE AND I CANNOT ESCAPE MY FATE
Prediction:  is is1

1
/d1/d/d
/d

/   


/manual

Loss: 13.7651
outputs.loss tensor(13.7651, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0026.flac
Waveform stats - mean: -0.0000, std: 0.0518, min: -0.2875, max: 0.2834
Resampled waveform stats - mean: -0.0000, std: 0.0518, min: -0.2875, max: 0.2834
Raw mel spectrogram stats - mean: 1.0045, std: 6.6167, min: 0.0000, max: 209.5466
Log mel spectrogram stats - mean: -6.1416, std: 3.9866, min: -13.8155, max: 5.3449
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9249, max: 2.8813
Mel spec shape: torch.Size([1, 80, 936])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9248, max: 2.8809
CNN output stats - mean: 0.2827, std: 0.5835, min: -0.1699, max: 3.8809
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.3848, max: 3.2148
Final output stats - mean: -0.0029, std: 0.1901, min: -0.7505, max: 0.7563
audio_emb.shape torch.Size([1, 59, 3584])
Audio embedding stats - mean: -0.0030, std: 0.1903

Sample prediction:
Target: I SAW THE LADY WHO EREWHILE APPEARED VEILED UNDERNEATH THE ANGELIC FESTIVAL DIRECT HER EYES TO ME ACROSS THE RIVER
Prediction:  is.ll
ll
,ll1\nlly3\n3 isll踔1/

&


{-&&&&/&\&&&/&</&/&&
Loss: 12.9398
outputs.loss tensor(12.9398, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110523/7976-110523-0015.flac
Waveform stats - mean: 0.0000, std: 0.0684, min: -0.9518, max: 0.6764
Resampled waveform stats - mean: 0.0000, std: 0.0684, min: -0.9518, max: 0.6764
Raw mel spectrogram stats - mean: 1.7525, std: 15.9649, min: 0.0000, max: 1249.6906
Log mel spectrogram stats - mean: -6.2006, std: 4.3239, min: -13.8039, max: 7.1307
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7585, max: 3.0832
Mel spec shape: torch.Size([1, 80, 776])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7588, max: 3.0840
CNN output stats - mean: 0.2820, std: 0.5869, min: -0.1699, max: 4.3008
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.0098, max: 2.9980
Final output stats - mean: -0.0030, std: 0.1934, min: -0.7505, max: 0.7480
audio_emb.shape torch.Size([1, 49, 3584])
Audio embedding stats - mean: -0.0030, std: 0.1931

Sample prediction:
Target: GRETHEL SHE CRIED IN A PASSION GET SOME WATER QUICKLY BE HANSEL FAT OR LEAN THIS MORNING I WILL KILL AND COOK HIM
Prediction: ,1
 is1ll\n,1ll1干事创业111y1131111
1*/*/&&&&1&*/&
Loss: 12.8343
outputs.loss tensor(12.8343, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0018.flac
Waveform stats - mean: 0.0000, std: 0.0557, min: -0.2837, max: 0.3327
Resampled waveform stats - mean: 0.0000, std: 0.0557, min: -0.2837, max: 0.3327
Raw mel spectrogram stats - mean: 1.1603, std: 7.5619, min: 0.0000, max: 315.1276
Log mel spectrogram stats - mean: -6.2129, std: 4.2475, min: -13.8155, max: 5.7530
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7899, max: 2.8171
Mel spec shape: torch.Size([1, 80, 947])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7900, max: 2.8164
CNN output stats - mean: 0.2827, std: 0.5879, min: -0.1699, max: 4.2422
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.2812, max: 3.0234
Final output stats - mean: -0.0027, std: 0.1921, min: -0.7505, max: 0.7808
audio_emb.shape torch.Size([1, 60, 3584])
Audio embedding stats - mean: -0.0028, std: 0.1923

Sample prediction:
Target: THE SECOND WAS AS IF HER FLESH AND BONES HAD ALL BEEN FASHIONED OUT OF EMERALD THE THIRD APPEARED AS SNOW BUT NEWLY FALLEN
Prediction:  isy13
11111::ll tô1,1's1133
y1111::ll3::31/&

</</
Loss: 12.4408
outputs.loss tensor(12.4408, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0007.flac
Waveform stats - mean: 0.0000, std: 0.0645, min: -0.4266, max: 0.4142
Resampled waveform stats - mean: 0.0000, std: 0.0645, min: -0.4266, max: 0.4142
Raw mel spectrogram stats - mean: 1.5590, std: 12.4799, min: 0.0000, max: 590.5460
Log mel spectrogram stats - mean: -5.4526, std: 3.7286, min: -13.5012, max: 6.3810
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1586, max: 3.1738
Mel spec shape: torch.Size([1, 80, 538])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1582, max: 3.1738
CNN output stats - mean: 0.2825, std: 0.5884, min: -0.1699, max: 3.6074
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.3066, max: 3.0000
Final output stats - mean: -0.0028, std: 0.1942, min: -0.7471, max: 0.7339
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: -0.0028, std: 0.1943

Sample prediction:
Target: HE WOULD SEE HOW THINGS TURNED OUT TO MORROW AND THEN HE WOULD TALK TO HER THEY WERE GOING TO MEET AS USUAL
Prediction:  isll isllll3 ism西侧
</</</</</\n</</\n1</1\n</1/</</</</</</</
Loss: 13.7223
outputs.loss tensor(13.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0002.flac
Waveform stats - mean: -0.0000, std: 0.0399, min: -0.2627, max: 0.3059
Resampled waveform stats - mean: -0.0000, std: 0.0399, min: -0.2627, max: 0.3059
Raw mel spectrogram stats - mean: 0.5951, std: 4.8547, min: 0.0000, max: 418.5769
Log mel spectrogram stats - mean: -7.5652, std: 4.0969, min: -13.8082, max: 6.0369
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5238, max: 3.3201
Mel spec shape: torch.Size([1, 80, 514])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5234, max: 3.3203
CNN output stats - mean: 0.2822, std: 0.5884, min: -0.1699, max: 3.6113
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.4980, max: 3.2656
Final output stats - mean: -0.0032, std: 0.1968, min: -0.7642, max: 0.7642
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: -0.0033, std: 0.1969

Sample prediction:
Target: THEN THERE WAS SILENCE THEN A SIGH AND THE SOUND OF LIGHT MOVING FEET ON THE GRAVEL
Prediction: 
:: toll11 to1ll1's's1::::'s1'sy.'s
&.
Loss: 12.4700
outputs.loss tensor(12.4700, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=0.0024, std=0.0609
cnn_layers.0.bias: mean=0.0000, std=0.0000
cnn_layers.1.weight: mean=-0.0001, std=0.0286
cnn_layers.1.bias: mean=-0.0000, std=0.0213
cnn_layers.3.weight: mean=0.0008, std=0.0462
cnn_layers.3.bias: mean=0.0000, std=0.0000
cnn_layers.4.weight: mean=-0.0001, std=0.0240
cnn_layers.4.bias: mean=-0.0000, std=0.0176
cnn_layers.6.weight: mean=-0.0000, std=0.0380
cnn_layers.6.bias: mean=-0.0000, std=0.0000
cnn_layers.7.weight: mean=-0.0001, std=0.0194
cnn_layers.7.bias: mean=0.0002, std=0.0143
cnn_layers.9.weight: mean=0.0001, std=0.0329
cnn_layers.9.bias: mean=0.0000, std=0.0001
cnn_layers.10.weight: mean=-0.0003, std=0.1575
cnn_layers.10.bias: mean=0.0004, std=0.1918
transformer.layers.0.self_attn.in_proj_weight: mean=0.0000, std=0.0254
transformer.layers.0.self_attn.in_proj_bias: mean=0.0001, std=0.0903
transformer.layers.0.self_attn.out_proj.weight: mean=0.0000, std=0.0547
transformer.layers.0.self_attn.out_proj.bias: mean=0.0005, std=0.2837
transformer.layers.0.linear1.weight: mean=0.0000, std=0.0057
transformer.layers.0.linear1.bias: mean=-0.0005, std=0.0219
transformer.layers.0.linear2.weight: mean=0.0001, std=0.0211
transformer.layers.0.linear2.bias: mean=0.0004, std=0.1539
transformer.layers.0.norm1.weight: mean=-0.0003, std=0.0343
transformer.layers.0.norm1.bias: mean=0.0009, std=0.1641
transformer.layers.0.norm2.weight: mean=-0.0000, std=0.0332
transformer.layers.0.norm2.bias: mean=0.0001, std=0.1570
transformer.layers.1.self_attn.in_proj_weight: mean=0.0000, std=0.0078
transformer.layers.1.self_attn.in_proj_bias: mean=0.0001, std=0.0371
transformer.layers.1.self_attn.out_proj.weight: mean=0.0000, std=0.0170
transformer.layers.1.self_attn.out_proj.bias: mean=0.0001, std=0.1171
transformer.layers.1.linear1.weight: mean=-0.0000, std=0.0043
transformer.layers.1.linear1.bias: mean=-0.0003, std=0.0156
transformer.layers.1.linear2.weight: mean=0.0000, std=0.0153
transformer.layers.1.linear2.bias: mean=0.0000, std=0.1097
transformer.layers.1.norm1.weight: mean=-0.0002, std=0.0262
transformer.layers.1.norm1.bias: mean=0.0008, std=0.1171
transformer.layers.1.norm2.weight: mean=-0.0000, std=0.0281
transformer.layers.1.norm2.bias: mean=-0.0002, std=0.1158
transformer.layers.2.self_attn.in_proj_weight: mean=-0.0000, std=0.0067
transformer.layers.2.self_attn.in_proj_bias: mean=0.0001, std=0.0274
transformer.layers.2.self_attn.out_proj.weight: mean=-0.0000, std=0.0143
transformer.layers.2.self_attn.out_proj.bias: mean=0.0003, std=0.0864
transformer.layers.2.linear1.weight: mean=0.0000, std=0.0038
transformer.layers.2.linear1.bias: mean=-0.0003, std=0.0117
transformer.layers.2.linear2.weight: mean=0.0000, std=0.0121
transformer.layers.2.linear2.bias: mean=0.0002, std=0.0816
transformer.layers.2.norm1.weight: mean=-0.0002, std=0.0250
transformer.layers.2.norm1.bias: mean=0.0007, std=0.0872
transformer.layers.2.norm2.weight: mean=0.0000, std=0.0268
transformer.layers.2.norm2.bias: mean=-0.0004, std=0.0886
transformer.layers.3.self_attn.in_proj_weight: mean=-0.0000, std=0.0064
transformer.layers.3.self_attn.in_proj_bias: mean=0.0001, std=0.0210
transformer.layers.3.self_attn.out_proj.weight: mean=0.0000, std=0.0135
transformer.layers.3.self_attn.out_proj.bias: mean=0.0000, std=0.0662
transformer.layers.3.linear1.weight: mean=0.0000, std=0.0037
transformer.layers.3.linear1.bias: mean=-0.0001, std=0.0093
transformer.layers.3.linear2.weight: mean=-0.0000, std=0.0103
transformer.layers.3.linear2.bias: mean=-0.0000, std=0.0634
transformer.layers.3.norm1.weight: mean=-0.0002, std=0.0251
transformer.layers.3.norm1.bias: mean=0.0004, std=0.0677
transformer.layers.3.norm2.weight: mean=-0.0000, std=0.0266
transformer.layers.3.norm2.bias: mean=-0.0005, std=0.0699
transformer.layers.4.self_attn.in_proj_weight: mean=0.0000, std=0.0063
transformer.layers.4.self_attn.in_proj_bias: mean=-0.0000, std=0.0167
transformer.layers.4.self_attn.out_proj.weight: mean=-0.0000, std=0.0132
transformer.layers.4.self_attn.out_proj.bias: mean=0.0001, std=0.0523
transformer.layers.4.linear1.weight: mean=0.0000, std=0.0036
transformer.layers.4.linear1.bias: mean=-0.0001, std=0.0076
transformer.layers.4.linear2.weight: mean=0.0000, std=0.0093
transformer.layers.4.linear2.bias: mean=0.0001, std=0.0506
transformer.layers.4.norm1.weight: mean=-0.0001, std=0.0255
transformer.layers.4.norm1.bias: mean=0.0004, std=0.0544
transformer.layers.4.norm2.weight: mean=0.0001, std=0.0266
transformer.layers.4.norm2.bias: mean=-0.0004, std=0.0566
transformer.layers.5.self_attn.in_proj_weight: mean=-0.0000, std=0.0062
transformer.layers.5.self_attn.in_proj_bias: mean=-0.0000, std=0.0136
transformer.layers.5.self_attn.out_proj.weight: mean=0.0000, std=0.0129
transformer.layers.5.self_attn.out_proj.bias: mean=-0.0001, std=0.0424
transformer.layers.5.linear1.weight: mean=-0.0000, std=0.0036
transformer.layers.5.linear1.bias: mean=-0.0001, std=0.0064
transformer.layers.5.linear2.weight: mean=0.0000, std=0.0087
transformer.layers.5.linear2.bias: mean=0.0002, std=0.0417
transformer.layers.5.norm1.weight: mean=-0.0001, std=0.0257
transformer.layers.5.norm1.bias: mean=0.0003, std=0.0451
transformer.layers.5.norm2.weight: mean=0.0000, std=0.0265
transformer.layers.5.norm2.bias: mean=-0.0005, std=0.0471
transformer.layers.6.self_attn.in_proj_weight: mean=0.0000, std=0.0061
transformer.layers.6.self_attn.in_proj_bias: mean=-0.0000, std=0.0114
transformer.layers.6.self_attn.out_proj.weight: mean=0.0000, std=0.0128
transformer.layers.6.self_attn.out_proj.bias: mean=-0.0000, std=0.0353
transformer.layers.6.linear1.weight: mean=0.0000, std=0.0036
transformer.layers.6.linear1.bias: mean=-0.0000, std=0.0056
transformer.layers.6.linear2.weight: mean=0.0000, std=0.0084
transformer.layers.6.linear2.bias: mean=0.0001, std=0.0356
transformer.layers.6.norm1.weight: mean=-0.0001, std=0.0258
transformer.layers.6.norm1.bias: mean=0.0002, std=0.0387
transformer.layers.6.norm2.weight: mean=0.0000, std=0.0264
transformer.layers.6.norm2.bias: mean=-0.0004, std=0.0403
transformer.layers.7.self_attn.in_proj_weight: mean=0.0000, std=0.0061
transformer.layers.7.self_attn.in_proj_bias: mean=-0.0000, std=0.0098
transformer.layers.7.self_attn.out_proj.weight: mean=0.0000, std=0.0128
transformer.layers.7.self_attn.out_proj.bias: mean=-0.0000, std=0.0303
transformer.layers.7.linear1.weight: mean=0.0000, std=0.0038
transformer.layers.7.linear1.bias: mean=-0.0000, std=0.0052
transformer.layers.7.linear2.weight: mean=-0.0000, std=0.0083
transformer.layers.7.linear2.bias: mean=-0.0001, std=0.0316
transformer.layers.7.norm1.weight: mean=-0.0001, std=0.0259
transformer.layers.7.norm1.bias: mean=0.0002, std=0.0342
transformer.layers.7.norm2.weight: mean=-0.0000, std=0.0264
transformer.layers.7.norm2.bias: mean=-0.0003, std=0.0356
transformer.layers.8.self_attn.in_proj_weight: mean=-0.0000, std=0.0061
transformer.layers.8.self_attn.in_proj_bias: mean=-0.0000, std=0.0087
transformer.layers.8.self_attn.out_proj.weight: mean=-0.0000, std=0.0128
transformer.layers.8.self_attn.out_proj.bias: mean=0.0000, std=0.0266
transformer.layers.8.linear1.weight: mean=-0.0000, std=0.0039
transformer.layers.8.linear1.bias: mean=-0.0000, std=0.0048
transformer.layers.8.linear2.weight: mean=-0.0000, std=0.0083
transformer.layers.8.linear2.bias: mean=-0.0001, std=0.0286
transformer.layers.8.norm1.weight: mean=-0.0001, std=0.0259
transformer.layers.8.norm1.bias: mean=0.0000, std=0.0312
transformer.layers.8.norm2.weight: mean=-0.0000, std=0.0264
transformer.layers.8.norm2.bias: mean=-0.0004, std=0.0323
transformer.layers.9.self_attn.in_proj_weight: mean=0.0000, std=0.0062
transformer.layers.9.self_attn.in_proj_bias: mean=-0.0000, std=0.0080
transformer.layers.9.self_attn.out_proj.weight: mean=-0.0000, std=0.0130
transformer.layers.9.self_attn.out_proj.bias: mean=0.0000, std=0.0244
transformer.layers.9.linear1.weight: mean=0.0000, std=0.0040
transformer.layers.9.linear1.bias: mean=0.0000, std=0.0046
transformer.layers.9.linear2.weight: mean=0.0000, std=0.0083
transformer.layers.9.linear2.bias: mean=0.0001, std=0.0268
transformer.layers.9.norm1.weight: mean=-0.0000, std=0.0261
transformer.layers.9.norm1.bias: mean=0.0000, std=0.0292
transformer.layers.9.norm2.weight: mean=-0.0000, std=0.0266
transformer.layers.9.norm2.bias: mean=-0.0003, std=0.0301
transformer.layers.10.self_attn.in_proj_weight: mean=-0.0000, std=0.0063
transformer.layers.10.self_attn.in_proj_bias: mean=-0.0000, std=0.0075
transformer.layers.10.self_attn.out_proj.weight: mean=-0.0000, std=0.0131
transformer.layers.10.self_attn.out_proj.bias: mean=0.0001, std=0.0228
transformer.layers.10.linear1.weight: mean=0.0000, std=0.0040
transformer.layers.10.linear1.bias: mean=0.0000, std=0.0045
transformer.layers.10.linear2.weight: mean=-0.0000, std=0.0084
transformer.layers.10.linear2.bias: mean=-0.0000, std=0.0256
transformer.layers.10.norm1.weight: mean=-0.0001, std=0.0261
transformer.layers.10.norm1.bias: mean=-0.0000, std=0.0278
transformer.layers.10.norm2.weight: mean=-0.0000, std=0.0265
transformer.layers.10.norm2.bias: mean=-0.0002, std=0.0285
transformer.layers.11.self_attn.in_proj_weight: mean=-0.0000, std=0.0062
transformer.layers.11.self_attn.in_proj_bias: mean=-0.0000, std=0.0071
transformer.layers.11.self_attn.out_proj.weight: mean=0.0000, std=0.0129
transformer.layers.11.self_attn.out_proj.bias: mean=-0.0000, std=0.0213
transformer.layers.11.linear1.weight: mean=0.0000, std=0.0041
transformer.layers.11.linear1.bias: mean=0.0000, std=0.0044
transformer.layers.11.linear2.weight: mean=0.0000, std=0.0083
transformer.layers.11.linear2.bias: mean=0.0002, std=0.0245
transformer.layers.11.norm1.weight: mean=0.0000, std=0.0259
transformer.layers.11.norm1.bias: mean=-0.0001, std=0.0269
transformer.layers.11.norm2.weight: mean=-0.0000, std=0.0265
transformer.layers.11.norm2.bias: mean=-0.0001, std=0.0275
transformer.layers.12.self_attn.in_proj_weight: mean=0.0000, std=0.0063
transformer.layers.12.self_attn.in_proj_bias: mean=0.0000, std=0.0069
transformer.layers.12.self_attn.out_proj.weight: mean=0.0000, std=0.0131
transformer.layers.12.self_attn.out_proj.bias: mean=-0.0001, std=0.0209
transformer.layers.12.linear1.weight: mean=0.0000, std=0.0041
transformer.layers.12.linear1.bias: mean=0.0000, std=0.0043
transformer.layers.12.linear2.weight: mean=-0.0000, std=0.0083
transformer.layers.12.linear2.bias: mean=-0.0001, std=0.0241
transformer.layers.12.norm1.weight: mean=0.0000, std=0.0259
transformer.layers.12.norm1.bias: mean=-0.0002, std=0.0263
transformer.layers.12.norm2.weight: mean=0.0000, std=0.0261
transformer.layers.12.norm2.bias: mean=-0.0002, std=0.0266
transformer.layers.13.self_attn.in_proj_weight: mean=-0.0000, std=0.0063
transformer.layers.13.self_attn.in_proj_bias: mean=0.0001, std=0.0067
transformer.layers.13.self_attn.out_proj.weight: mean=0.0000, std=0.0131
transformer.layers.13.self_attn.out_proj.bias: mean=-0.0000, std=0.0202
transformer.layers.13.linear1.weight: mean=-0.0000, std=0.0042
transformer.layers.13.linear1.bias: mean=0.0000, std=0.0043
transformer.layers.13.linear2.weight: mean=-0.0000, std=0.0084
transformer.layers.13.linear2.bias: mean=-0.0001, std=0.0238
transformer.layers.13.norm1.weight: mean=0.0000, std=0.0256
transformer.layers.13.norm1.bias: mean=-0.0002, std=0.0258
transformer.layers.13.norm2.weight: mean=-0.0000, std=0.0259
transformer.layers.13.norm2.bias: mean=-0.0001, std=0.0262
transformer.layers.14.self_attn.in_proj_weight: mean=-0.0000, std=0.0065
transformer.layers.14.self_attn.in_proj_bias: mean=0.0001, std=0.0068
transformer.layers.14.self_attn.out_proj.weight: mean=-0.0000, std=0.0132
transformer.layers.14.self_attn.out_proj.bias: mean=-0.0000, std=0.0199
transformer.layers.14.linear1.weight: mean=0.0000, std=0.0043
transformer.layers.14.linear1.bias: mean=0.0000, std=0.0043
transformer.layers.14.linear2.weight: mean=0.0000, std=0.0083
transformer.layers.14.linear2.bias: mean=0.0003, std=0.0233
transformer.layers.14.norm1.weight: mean=0.0000, std=0.0251
transformer.layers.14.norm1.bias: mean=-0.0002, std=0.0253
transformer.layers.14.norm2.weight: mean=-0.0000, std=0.0252
transformer.layers.14.norm2.bias: mean=-0.0002, std=0.0255
transformer.layers.15.self_attn.in_proj_weight: mean=-0.0000, std=0.0063
transformer.layers.15.self_attn.in_proj_bias: mean=0.0000, std=0.0065
transformer.layers.15.self_attn.out_proj.weight: mean=0.0000, std=0.0132
transformer.layers.15.self_attn.out_proj.bias: mean=-0.0001, std=0.0198
transformer.layers.15.linear1.weight: mean=0.0000, std=0.0043
transformer.layers.15.linear1.bias: mean=0.0000, std=0.0044
transformer.layers.15.linear2.weight: mean=0.0000, std=0.0086
transformer.layers.15.linear2.bias: mean=0.0001, std=0.0239
transformer.layers.15.norm1.weight: mean=0.0000, std=0.0250
transformer.layers.15.norm1.bias: mean=-0.0001, std=0.0252
transformer.layers.15.norm2.weight: mean=0.0000, std=0.0248
transformer.layers.15.norm2.bias: mean=-0.0003, std=0.0254
transformer.layers.16.self_attn.in_proj_weight: mean=-0.0000, std=0.0066
transformer.layers.16.self_attn.in_proj_bias: mean=0.0001, std=0.0068
transformer.layers.16.self_attn.out_proj.weight: mean=0.0000, std=0.0138
transformer.layers.16.self_attn.out_proj.bias: mean=-0.0001, std=0.0204
transformer.layers.16.linear1.weight: mean=0.0000, std=0.0044
transformer.layers.16.linear1.bias: mean=0.0000, std=0.0044
transformer.layers.16.linear2.weight: mean=-0.0000, std=0.0087
transformer.layers.16.linear2.bias: mean=-0.0002, std=0.0235
transformer.layers.16.norm1.weight: mean=0.0000, std=0.0245
transformer.layers.16.norm1.bias: mean=-0.0003, std=0.0251
transformer.layers.16.norm2.weight: mean=-0.0000, std=0.0244
transformer.layers.16.norm2.bias: mean=-0.0002, std=0.0254
transformer.layers.17.self_attn.in_proj_weight: mean=-0.0000, std=0.0064
transformer.layers.17.self_attn.in_proj_bias: mean=0.0000, std=0.0066
transformer.layers.17.self_attn.out_proj.weight: mean=-0.0000, std=0.0138
transformer.layers.17.self_attn.out_proj.bias: mean=0.0001, std=0.0201
transformer.layers.17.linear1.weight: mean=-0.0000, std=0.0047
transformer.layers.17.linear1.bias: mean=-0.0001, std=0.0047
transformer.layers.17.linear2.weight: mean=-0.0000, std=0.0094
transformer.layers.17.linear2.bias: mean=-0.0000, std=0.0253
transformer.layers.17.norm1.weight: mean=0.0000, std=0.0247
transformer.layers.17.norm1.bias: mean=-0.0004, std=0.0255
transformer.layers.17.norm2.weight: mean=0.0000, std=0.0247
transformer.layers.17.norm2.bias: mean=-0.0003, std=0.0256
transformer.layers.18.self_attn.in_proj_weight: mean=-0.0000, std=0.0070
transformer.layers.18.self_attn.in_proj_bias: mean=-0.0001, std=0.0072
transformer.layers.18.self_attn.out_proj.weight: mean=-0.0000, std=0.0145
transformer.layers.18.self_attn.out_proj.bias: mean=0.0009, std=0.0209
transformer.layers.18.linear1.weight: mean=-0.0000, std=0.0050
transformer.layers.18.linear1.bias: mean=-0.0001, std=0.0050
transformer.layers.18.linear2.weight: mean=-0.0000, std=0.0098
transformer.layers.18.linear2.bias: mean=-0.0002, std=0.0258
transformer.layers.18.norm1.weight: mean=0.0000, std=0.0245
transformer.layers.18.norm1.bias: mean=-0.0003, std=0.0255
transformer.layers.18.norm2.weight: mean=-0.0000, std=0.0242
transformer.layers.18.norm2.bias: mean=-0.0003, std=0.0257
transformer.layers.19.self_attn.in_proj_weight: mean=0.0000, std=0.0074
transformer.layers.19.self_attn.in_proj_bias: mean=0.0001, std=0.0076
transformer.layers.19.self_attn.out_proj.weight: mean=-0.0000, std=0.0161
transformer.layers.19.self_attn.out_proj.bias: mean=0.0003, std=0.0229
transformer.layers.19.linear1.weight: mean=0.0000, std=0.0056
transformer.layers.19.linear1.bias: mean=-0.0001, std=0.0055
transformer.layers.19.linear2.weight: mean=0.0001, std=0.0111
transformer.layers.19.linear2.bias: mean=0.0004, std=0.0283
transformer.layers.19.norm1.weight: mean=0.0000, std=0.0264
transformer.layers.19.norm1.bias: mean=-0.0001, std=0.0263
transformer.layers.19.norm2.weight: mean=-0.0000, std=0.0264
transformer.layers.19.norm2.bias: mean=-0.0001, std=0.0264
transformer.layers.20.self_attn.in_proj_weight: mean=0.0000, std=0.0079
transformer.layers.20.self_attn.in_proj_bias: mean=0.0000, std=0.0080
transformer.layers.20.self_attn.out_proj.weight: mean=-0.0000, std=0.0173
transformer.layers.20.self_attn.out_proj.bias: mean=0.0005, std=0.0246
transformer.layers.20.linear1.weight: mean=0.0000, std=0.0065
transformer.layers.20.linear1.bias: mean=-0.0001, std=0.0063
transformer.layers.20.linear2.weight: mean=0.0002, std=0.0128
transformer.layers.20.linear2.bias: mean=0.0019, std=0.0313
transformer.layers.20.norm1.weight: mean=-0.0000, std=0.0276
transformer.layers.20.norm1.bias: mean=-0.0005, std=0.0273
transformer.layers.20.norm2.weight: mean=-0.0000, std=0.0273
transformer.layers.20.norm2.bias: mean=-0.0003, std=0.0271
transformer.layers.21.self_attn.in_proj_weight: mean=0.0000, std=0.0091
transformer.layers.21.self_attn.in_proj_bias: mean=0.0002, std=0.0093
transformer.layers.21.self_attn.out_proj.weight: mean=0.0000, std=0.0208
transformer.layers.21.self_attn.out_proj.bias: mean=-0.0001, std=0.0293
transformer.layers.21.linear1.weight: mean=0.0000, std=0.0079
transformer.layers.21.linear1.bias: mean=-0.0002, std=0.0076
transformer.layers.21.linear2.weight: mean=-0.0001, std=0.0151
transformer.layers.21.linear2.bias: mean=-0.0007, std=0.0347
transformer.layers.21.norm1.weight: mean=-0.0000, std=0.0304
transformer.layers.21.norm1.bias: mean=-0.0007, std=0.0292
transformer.layers.21.norm2.weight: mean=0.0000, std=0.0311
transformer.layers.21.norm2.bias: mean=-0.0008, std=0.0295
transformer.layers.22.self_attn.in_proj_weight: mean=0.0000, std=0.0116
transformer.layers.22.self_attn.in_proj_bias: mean=-0.0000, std=0.0118
transformer.layers.22.self_attn.out_proj.weight: mean=-0.0001, std=0.0252
transformer.layers.22.self_attn.out_proj.bias: mean=0.0019, std=0.0345
transformer.layers.22.linear1.weight: mean=-0.0000, std=0.0097
transformer.layers.22.linear1.bias: mean=-0.0000, std=0.0093
transformer.layers.22.linear2.weight: mean=-0.0002, std=0.0186
transformer.layers.22.linear2.bias: mean=-0.0019, std=0.0410
transformer.layers.22.norm1.weight: mean=-0.0000, std=0.0363
transformer.layers.22.norm1.bias: mean=-0.0003, std=0.0317
transformer.layers.22.norm2.weight: mean=0.0000, std=0.0365
transformer.layers.22.norm2.bias: mean=-0.0014, std=0.0309
transformer.layers.23.self_attn.in_proj_weight: mean=0.0000, std=0.0146
transformer.layers.23.self_attn.in_proj_bias: mean=-0.0001, std=0.0149
transformer.layers.23.self_attn.out_proj.weight: mean=0.0000, std=0.0316
transformer.layers.23.self_attn.out_proj.bias: mean=-0.0013, std=0.0435
transformer.layers.23.linear1.weight: mean=0.0000, std=0.0132
transformer.layers.23.linear1.bias: mean=-0.0001, std=0.0126
transformer.layers.23.linear2.weight: mean=0.0002, std=0.0254
transformer.layers.23.linear2.bias: mean=0.0014, std=0.0520
transformer.layers.23.norm1.weight: mean=0.0000, std=0.0411
transformer.layers.23.norm1.bias: mean=-0.0002, std=0.0344
transformer.layers.23.norm2.weight: mean=0.0036, std=0.0442
transformer.layers.23.norm2.bias: mean=0.0002, std=0.0335
connector.0.weight: mean=-0.0000, std=0.0535
connector.0.bias: mean=0.0003, std=0.0410
connector.2.weight: mean=-0.0002, std=0.0256
connector.2.bias: mean=-0.0018, std=0.0545
Gradients clipped from 1.0000 to 1.0
Gradient norm: 1.0000
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0042.flac
Waveform stats - mean: -0.0000, std: 0.0547, min: -0.2705, max: 0.4191
Resampled waveform stats - mean: -0.0000, std: 0.0547, min: -0.2705, max: 0.4191
Raw mel spectrogram stats - mean: 1.1162, std: 6.5743, min: 0.0000, max: 240.7762
Log mel spectrogram stats - mean: -6.7039, std: 4.7888, min: -13.8127, max: 5.4839
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4845, max: 2.5450
Mel spec shape: torch.Size([1, 80, 285])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4844, max: 2.5449
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT PUT IT ON THE TABLE IN THE CORNER OF THE PASSAGE
Prediction: 000000000000000
Loss: 15.7671
outputs.loss tensor(15.7671, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/57405/6319-57405-0008.flac
Waveform stats - mean: -0.0000, std: 0.0678, min: -0.6406, max: 0.5833
Resampled waveform stats - mean: -0.0000, std: 0.0678, min: -0.6406, max: 0.5833
Raw mel spectrogram stats - mean: 1.7244, std: 15.0301, min: 0.0000, max: 892.1021
Log mel spectrogram stats - mean: -5.6632, std: 3.7654, min: -13.6972, max: 6.7936
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1336, max: 3.3082
Mel spec shape: torch.Size([1, 80, 1247])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1328, max: 3.3086
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 39936
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 39936
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 279552
Inf count: 0
audio_emb.shape torch.Size([1, 78, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WE SHOULD LIKE ABOVE ALL THINGS SAID DEUCALION TO SEE THIS LAND FULL OF PEOPLE ONCE MORE FOR WITHOUT NEIGHBORS AND FRIENDS THE WORLD IS A VERY LONELY PLACE INDEED
Prediction: 00000000000000000000000000000000000000000000
Loss: 15.7573
outputs.loss tensor(15.7573, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/168635/174-168635-0020.flac
Waveform stats - mean: -0.0001, std: 0.0459, min: -0.5032, max: 0.4770
Resampled waveform stats - mean: -0.0001, std: 0.0459, min: -0.5032, max: 0.4770
Raw mel spectrogram stats - mean: 0.7860, std: 5.5449, min: 0.0000, max: 217.6672
Log mel spectrogram stats - mean: -6.0164, std: 4.0192, min: -13.7740, max: 5.3830
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9301, max: 2.8362
Mel spec shape: torch.Size([1, 80, 463])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9297, max: 2.8359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALAS HE WALKED WITH NO LESS INDECISION THAN COSETTE
Prediction: 0000000000000000
Loss: 15.5953
outputs.loss tensor(15.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0035.flac
Waveform stats - mean: 0.0000, std: 0.0634, min: -0.5995, max: 0.6143
Resampled waveform stats - mean: 0.0000, std: 0.0634, min: -0.5995, max: 0.6143
Raw mel spectrogram stats - mean: 1.4971, std: 8.0442, min: 0.0000, max: 350.3127
Log mel spectrogram stats - mean: -4.8463, std: 3.6583, min: -13.7410, max: 5.8588
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4314, max: 2.9262
Mel spec shape: torch.Size([1, 80, 362])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.4316, max: 2.9258
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SUFFICIENT TO SERVE WITH FIVE OR SIX MACKEREL
Prediction: 000000000000000
Loss: 16.3402
outputs.loss tensor(16.3402, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0026.flac
Waveform stats - mean: -0.0001, std: 0.0547, min: -0.5679, max: 0.4191
Resampled waveform stats - mean: -0.0001, std: 0.0547, min: -0.5679, max: 0.4191
Raw mel spectrogram stats - mean: 1.1210, std: 7.0921, min: 0.0000, max: 291.7528
Log mel spectrogram stats - mean: -4.6416, std: 3.5317, min: -13.5850, max: 5.6759
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5323, max: 2.9214
Mel spec shape: torch.Size([1, 80, 911])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5332, max: 2.9219
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I SAW BUT FEW INHABITANTS DURING MY EXCURSION BUT I MET A CROWD ON THE BEACH DRYING SALTING AND LOADING CODFISH THE PRINCIPAL ARTICLE OF EXPORTATION
Prediction: 000000000000000000000000000000000000000000000000
Loss: 15.8049
outputs.loss tensor(15.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0030.flac
Waveform stats - mean: 0.0000, std: 0.0473, min: -0.3051, max: 0.3589
Resampled waveform stats - mean: 0.0000, std: 0.0473, min: -0.3051, max: 0.3589
Raw mel spectrogram stats - mean: 0.8368, std: 6.1253, min: 0.0000, max: 302.1291
Log mel spectrogram stats - mean: -5.6253, std: 3.6704, min: -13.8055, max: 5.7109
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2287, max: 3.0886
Mel spec shape: torch.Size([1, 80, 631])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2285, max: 3.0879
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE BEGAN TO WISH THAT HE HAD COMPROMISED IN SOME WAY OR OTHER THAT HE HAD SENT THE MONEY PERHAPS HE COULD DO IT UP HERE
Prediction: 00000000000000000000000000000000000
Loss: 15.9812
outputs.loss tensor(15.9812, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0029.flac
Waveform stats - mean: -0.0001, std: 0.0722, min: -0.3655, max: 0.5455
Resampled waveform stats - mean: -0.0001, std: 0.0722, min: -0.3655, max: 0.5455
Raw mel spectrogram stats - mean: 1.9514, std: 14.5711, min: 0.0000, max: 781.8242
Log mel spectrogram stats - mean: -5.6760, std: 3.8838, min: -13.5744, max: 6.6616
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0337, max: 3.1767
Mel spec shape: torch.Size([1, 80, 374])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0332, max: 3.1758
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NOW HIS MIDDLE HORSE WAS BEING ALMOST DRAGGED BY THE OTHER TWO
Prediction: 000000000000000000
Loss: 16.5235
outputs.loss tensor(16.5235, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143397/1673-143397-0010.flac
Waveform stats - mean: -0.0000, std: 0.0919, min: -0.8397, max: 0.8614
Resampled waveform stats - mean: -0.0000, std: 0.0919, min: -0.8397, max: 0.8614
Raw mel spectrogram stats - mean: 3.1682, std: 32.8800, min: 0.0000, max: 2798.2454
Log mel spectrogram stats - mean: -4.8309, std: 3.6440, min: -13.6573, max: 7.9367
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4222, max: 3.5038
Mel spec shape: torch.Size([1, 80, 1201])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4219, max: 3.5039
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 38912
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 38912
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 272384
Inf count: 0
audio_emb.shape torch.Size([1, 76, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE VANITY OF CELESTINE WAS FLATTERED BY THE APPEAL AND THE PARTIAL VERSION OF A MONK DECIDED THE FAITH OF THE POPE WHO WITH HIS LATIN CLERGY WAS IGNORANT OF THE LANGUAGE THE ARTS AND THE THEOLOGY OF THE GREEKS
Prediction: 00000000000000000000000000000000000000000000000000000000000000
Loss: 15.9286
outputs.loss tensor(15.9286, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0057.flac
Waveform stats - mean: 0.0000, std: 0.0215, min: -0.2257, max: 0.2419
Resampled waveform stats - mean: 0.0000, std: 0.0215, min: -0.2257, max: 0.2419
Raw mel spectrogram stats - mean: 0.1733, std: 1.1392, min: 0.0000, max: 69.8775
Log mel spectrogram stats - mean: -7.1273, std: 3.8601, min: -13.7861, max: 4.2467
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7250, max: 2.9466
Mel spec shape: torch.Size([1, 80, 1098])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7246, max: 2.9473
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 35328
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 35328
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 247296
Inf count: 0
audio_emb.shape torch.Size([1, 69, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HER BARE FEET AS IF POKED THROUGH THE BOTTOM OF AN UNADORNED SLEEVED CALICO SACK BUTTONED TIGHTLY AT NECK AND WRISTS FELT OVER THE RUG FOR THE SLIPPERS WHILE SHE LOOKED UPWARD INTO HER HUSBAND'S FACE
Prediction: 00000000000000000000000000000000000000000000000000000000000000
Loss: 15.9024
outputs.loss tensor(15.9024, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0017.flac
Waveform stats - mean: -0.0000, std: 0.0719, min: -0.5707, max: 0.5546
Resampled waveform stats - mean: -0.0000, std: 0.0719, min: -0.5707, max: 0.5546
Raw mel spectrogram stats - mean: 1.9381, std: 28.2677, min: 0.0000, max: 2132.9434
Log mel spectrogram stats - mean: -6.5096, std: 4.1071, min: -13.7923, max: 7.6653
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7732, max: 3.4513
Mel spec shape: torch.Size([1, 80, 748])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7734, max: 3.4512
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I SHALL LOCK UP ALL THE DOORS AND WINDOWS IN THE HOUSE AND THEN I SHALL GIVE YOU MY LATCH KEY AND YOU CAN LET YOURSELF IN AND STAY THE NIGHT HERE THERE IS NO ONE IN THE HOUSE
Prediction: 00000000000000000000000000000000000000000000
Loss: 15.7770
outputs.loss tensor(15.7770, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0012.flac
Waveform stats - mean: 0.0003, std: 0.0806, min: -0.6820, max: 0.5958
Resampled waveform stats - mean: 0.0003, std: 0.0806, min: -0.6820, max: 0.5958
Raw mel spectrogram stats - mean: 2.4258, std: 11.6617, min: 0.0000, max: 463.1852
Log mel spectrogram stats - mean: -3.0735, std: 3.5870, min: -13.8146, max: 6.1381
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.9945, max: 2.5681
Mel spec shape: torch.Size([1, 80, 1478])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.9941, max: 2.5684
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 47616
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 47616
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 333312
Inf count: 0
audio_emb.shape torch.Size([1, 93, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN THEY WENT BACK TO THEIR WORK LITTLE DREAMING AS THEY TIED ROSES AND TWINED SMILAX WREATHS HOW NEAR THAT OTHER CHANCE WAS HOW SOON THEY WERE TO BE CALLED UPON TO KEEP THEIR PROMISE AND HOW WELL EACH WAS TO PERFORM THE PART GIVEN THEM IN LIFE AND DEATH
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2575
outputs.loss tensor(16.2575, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0015.flac
Waveform stats - mean: 0.0000, std: 0.0469, min: -0.3609, max: 0.4472
Resampled waveform stats - mean: 0.0000, std: 0.0469, min: -0.3609, max: 0.4472
Raw mel spectrogram stats - mean: 0.8226, std: 9.0890, min: 0.0000, max: 555.7539
Log mel spectrogram stats - mean: -6.7806, std: 3.8434, min: -13.6668, max: 6.3203
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7917, max: 3.4087
Mel spec shape: torch.Size([1, 80, 475])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7920, max: 3.4082
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU'RE GETTING ALTOGETHER TOO UPSET ABOUT THESE PROGRAMS STOP IT AND BEHAVE YOURSELF
Prediction: 0000000000000000000000
Loss: 16.4590
outputs.loss tensor(16.4590, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0042.flac
Waveform stats - mean: -0.0000, std: 0.0824, min: -0.5269, max: 0.5891
Resampled waveform stats - mean: -0.0000, std: 0.0824, min: -0.5269, max: 0.5891
Raw mel spectrogram stats - mean: 2.5515, std: 17.9515, min: 0.0000, max: 916.0705
Log mel spectrogram stats - mean: -6.0430, std: 5.0234, min: -13.8150, max: 6.8201
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5471, max: 2.5606
Mel spec shape: torch.Size([1, 80, 642])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5469, max: 2.5605
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE ASCENT OF LASSEN'S BUTTE IS AN EASY WALK AND THE VIEWS FROM THE SUMMIT ARE EXTREMELY TELLING
Prediction: 000000000000000000000000000000
Loss: 16.3023
outputs.loss tensor(16.3023, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0010.flac
Waveform stats - mean: -0.0001, std: 0.0373, min: -0.6063, max: 0.3764
Resampled waveform stats - mean: -0.0001, std: 0.0373, min: -0.6063, max: 0.3764
Raw mel spectrogram stats - mean: 0.5193, std: 3.6659, min: 0.0000, max: 252.9689
Log mel spectrogram stats - mean: -6.7964, std: 4.7956, min: -13.8154, max: 5.5333
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4636, max: 2.5710
Mel spec shape: torch.Size([1, 80, 795])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4639, max: 2.5703
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE CHILDREN WERE INFINITE IN NUMBER AND EXCEEDINGLY MERRY I NEED HARDLY SAY THAT THEY CAME IN FOR THEIR FULL SHARE OF THE PREVAILING BEAUTY
Prediction: 000000000000000000000000000000000000000
Loss: 16.4010
outputs.loss tensor(16.4010, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0012.flac
Waveform stats - mean: 0.0000, std: 0.0416, min: -0.3885, max: 0.3783
Resampled waveform stats - mean: 0.0000, std: 0.0416, min: -0.3885, max: 0.3783
Raw mel spectrogram stats - mean: 0.6439, std: 3.8166, min: 0.0000, max: 175.7190
Log mel spectrogram stats - mean: -5.6158, std: 3.6524, min: -13.7268, max: 5.1689
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2207, max: 2.9527
Mel spec shape: torch.Size([1, 80, 509])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2207, max: 2.9531
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MINNIE BEGAN TO EXPLAIN BUT HER HUSBAND TOOK THIS PART OF THE CONVERSATION TO HIMSELF
Prediction: 0000000000000000000000000
Loss: 15.9918
outputs.loss tensor(15.9918, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/64726/6319-64726-0005.flac
Waveform stats - mean: -0.0000, std: 0.0493, min: -0.4097, max: 0.3782
Resampled waveform stats - mean: -0.0000, std: 0.0493, min: -0.4097, max: 0.3782
Raw mel spectrogram stats - mean: 0.9072, std: 7.8810, min: 0.0000, max: 490.7502
Log mel spectrogram stats - mean: -6.9278, std: 4.2793, min: -13.8155, max: 6.1959
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6095, max: 3.0668
Mel spec shape: torch.Size([1, 80, 969])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6094, max: 3.0664
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31232
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31232
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 218624
Inf count: 0
audio_emb.shape torch.Size([1, 61, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SHE HAD NO SOONER TAKEN UP THE SPINDLE THAN BEING HASTY AND CARELESS SHE PIERCED HER HAND WITH THE POINT OF IT AND FAINTED AWAY
Prediction: 000000000000000000000000000000000000000000
Loss: 15.7300
outputs.loss tensor(15.7300, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0009.flac
Waveform stats - mean: -0.0001, std: 0.0796, min: -0.5035, max: 0.4930
Resampled waveform stats - mean: -0.0001, std: 0.0796, min: -0.5035, max: 0.4930
Raw mel spectrogram stats - mean: 2.3616, std: 17.2716, min: 0.0000, max: 701.2930
Log mel spectrogram stats - mean: -5.3952, std: 3.5561, min: -12.3119, max: 6.5529
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9450, max: 3.3598
Mel spec shape: torch.Size([1, 80, 192])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9453, max: 3.3594
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6144
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6144
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 43008
Inf count: 0
audio_emb.shape torch.Size([1, 12, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE DOESN'T WORK AT ALL
Prediction: 0000000
Loss: 15.2645
outputs.loss tensor(15.2645, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0032.flac
Waveform stats - mean: -0.0001, std: 0.0569, min: -0.5002, max: 0.6091
Resampled waveform stats - mean: -0.0001, std: 0.0569, min: -0.5002, max: 0.6091
Raw mel spectrogram stats - mean: 1.2151, std: 13.0975, min: 0.0000, max: 953.8449
Log mel spectrogram stats - mean: -6.0603, std: 3.6442, min: -13.7275, max: 6.8605
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1040, max: 3.5456
Mel spec shape: torch.Size([1, 80, 1387])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1035, max: 3.5449
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 44544
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 44544
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 311808
Inf count: 0
audio_emb.shape torch.Size([1, 87, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE FIRST THING EITHER OF THEM NOTICED WAS A NEW SOUND THAT BROKE INTO THE CLEAR AIR LOUDER THAN THEY HAD EVER HEARD IT BEFORE THE BELL OF THE MONASTERY OF THEIR OWN VILLAGE RINGING FOR EARLY PRAYERS
Prediction: 00000000000000000000000000000000000000000000000000000000
Loss: 16.5034
outputs.loss tensor(16.5034, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0058.flac
Waveform stats - mean: -0.0000, std: 0.1271, min: -0.8773, max: 0.7233
Resampled waveform stats - mean: -0.0000, std: 0.1271, min: -0.8773, max: 0.7233
Raw mel spectrogram stats - mean: 6.0262, std: 37.2706, min: 0.0000, max: 821.0695
Log mel spectrogram stats - mean: -5.6701, std: 4.6104, min: -13.6953, max: 6.7106
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7407, max: 2.6854
Mel spec shape: torch.Size([1, 80, 206])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7402, max: 2.6855
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL REPORT THIS TO THE GOVERNMENT
Prediction: 0000000000
Loss: 15.6604
outputs.loss tensor(15.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0008.flac
Waveform stats - mean: -0.0000, std: 0.0665, min: -0.3277, max: 0.5588
Resampled waveform stats - mean: -0.0000, std: 0.0665, min: -0.3277, max: 0.5588
Raw mel spectrogram stats - mean: 1.6567, std: 8.1851, min: 0.0000, max: 342.4556
Log mel spectrogram stats - mean: -6.3063, std: 4.9392, min: -13.8155, max: 5.8361
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5203, max: 2.4584
Mel spec shape: torch.Size([1, 80, 405])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5205, max: 2.4590
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU DISTRESS ME HERBERT MORE THAN WORDS CAN SAY
Prediction: 000000000000
Loss: 16.4731
outputs.loss tensor(16.4731, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0023.flac
Waveform stats - mean: -0.0000, std: 0.0899, min: -0.4941, max: 0.4791
Resampled waveform stats - mean: -0.0000, std: 0.0899, min: -0.4941, max: 0.4791
Raw mel spectrogram stats - mean: 3.0297, std: 26.9026, min: 0.0000, max: 1629.3452
Log mel spectrogram stats - mean: -5.5932, std: 4.6021, min: -13.8149, max: 7.3959
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7865, max: 2.8224
Mel spec shape: torch.Size([1, 80, 516])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7861, max: 2.8223
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MOUNT BREMER IS THE MOST NOTED STRONGHOLD OF THE SHEEP IN THE WHOLE SHASTA REGION
Prediction: 0000000000000000000000000
Loss: 16.0845
outputs.loss tensor(16.0845, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0011.flac
Waveform stats - mean: 0.0000, std: 0.0645, min: -0.6472, max: 0.7628
Resampled waveform stats - mean: 0.0000, std: 0.0645, min: -0.6472, max: 0.7628
Raw mel spectrogram stats - mean: 1.5629, std: 9.9481, min: 0.0000, max: 732.9709
Log mel spectrogram stats - mean: -6.5265, std: 4.9842, min: -13.8150, max: 6.5971
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4623, max: 2.6330
Mel spec shape: torch.Size([1, 80, 1074])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4619, max: 2.6328
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34816
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34816
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 243712
Inf count: 0
audio_emb.shape torch.Size([1, 68, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SLIGHT RAINSTORMS ARE LIKELY TO BE ENCOUNTERED IN A TRIP ROUND THE MOUNTAIN BUT ONE MAY EASILY FIND SHELTER BENEATH WELL THATCHED TREES THAT SHED THE RAIN LIKE A ROOF
Prediction: 0000000000000000000000000000000000000000000000000000000
Loss: 15.8845
outputs.loss tensor(15.8845, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0008.flac
Waveform stats - mean: -0.0001, std: 0.0699, min: -0.5572, max: 0.4217
Resampled waveform stats - mean: -0.0001, std: 0.0699, min: -0.5572, max: 0.4217
Raw mel spectrogram stats - mean: 1.8288, std: 10.5986, min: 0.0000, max: 817.7444
Log mel spectrogram stats - mean: -4.7151, std: 3.8566, min: -13.8149, max: 6.7065
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3596, max: 2.9616
Mel spec shape: torch.Size([1, 80, 1939])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3594, max: 2.9609
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 62464
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 62464
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 437248
Inf count: 0
audio_emb.shape torch.Size([1, 122, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TURN IT THEN ON TO A PASTE BOARD OR VERY CLEAN DRESSER AND WITH A LARGE SHARP KNIFE DIVIDE IT IN TWO MAKE IT UP QUICKLY INTO LOAVES AND DISPATCH IT TO THE OVEN MAKE ONE OR TWO INCISIONS ACROSS THE TOPS OF THE LOAVES AS THEY WILL RISE MORE EASILY IF THIS BE DONE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.7788
outputs.loss tensor(15.7788, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0026.flac
Waveform stats - mean: -0.0000, std: 0.1599, min: -0.6198, max: 0.6003
Resampled waveform stats - mean: -0.0000, std: 0.1599, min: -0.6198, max: 0.6003
Raw mel spectrogram stats - mean: 9.4350, std: 74.1879, min: 0.0000, max: 1729.3409
Log mel spectrogram stats - mean: -4.4968, std: 3.5140, min: -12.7809, max: 7.4555
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3575, max: 3.4013
Mel spec shape: torch.Size([1, 80, 814])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3574, max: 3.4004
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: O HUSBAND WHOSE UNHAPPY FATE IN BEING MINE HATH BORNE THEE FROM THE MARRIAGE BED TO THE GRAVE
Prediction: 00000000000000000000000000000000000
Loss: 14.9830
outputs.loss tensor(14.9830, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93302/6345-93302-0009.flac
Waveform stats - mean: 0.0000, std: 0.0540, min: -0.3458, max: 0.3925
Resampled waveform stats - mean: 0.0000, std: 0.0540, min: -0.3458, max: 0.3925
Raw mel spectrogram stats - mean: 1.0938, std: 11.6623, min: 0.0000, max: 589.6346
Log mel spectrogram stats - mean: -6.6156, std: 4.2917, min: -13.8128, max: 6.3795
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6770, max: 3.0280
Mel spec shape: torch.Size([1, 80, 613])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6768, max: 3.0273
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT HERE THE ONLY THING THAT OCCURRED TO HER WAS TO STOP AND LOOK IN ONE OF THE SHOPS TILL HE SHOULD ASK HER WHAT SHE WAS LOOKING AT
Prediction: 000000000000000000000000000000000000
Loss: 16.1396
outputs.loss tensor(16.1396, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0002.flac
Waveform stats - mean: 0.0000, std: 0.0202, min: -0.1534, max: 0.3007
Resampled waveform stats - mean: 0.0000, std: 0.0202, min: -0.1534, max: 0.3007
Raw mel spectrogram stats - mean: 0.1526, std: 1.1806, min: 0.0000, max: 64.7108
Log mel spectrogram stats - mean: -7.1810, std: 3.3614, min: -13.5726, max: 4.1699
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9014, max: 3.3768
Mel spec shape: torch.Size([1, 80, 292])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9014, max: 3.3770
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT WAS THIS NOTHING
Prediction: 0000
Loss: 16.1804
outputs.loss tensor(16.1804, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0019.flac
Waveform stats - mean: -0.0000, std: 0.0730, min: -0.5097, max: 0.7509
Resampled waveform stats - mean: -0.0000, std: 0.0730, min: -0.5097, max: 0.7509
Raw mel spectrogram stats - mean: 1.9970, std: 9.9482, min: 0.0000, max: 595.7486
Log mel spectrogram stats - mean: -5.2682, std: 4.5972, min: -13.8155, max: 6.3898
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8593, max: 2.5359
Mel spec shape: torch.Size([1, 80, 1230])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8594, max: 2.5352
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 39424
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 39424
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 275968
Inf count: 0
audio_emb.shape torch.Size([1, 77, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE TELEPHONE LINE WAS SOON REPAIRED AND A STEADY STREAM OF RESCUE VEHICLES BEGAN ARRIVING FROM HARKNESS FIRE TRUCKS THREE AMBULANCES AND PRIVATE CARS DRIVEN BY VOLUNTEERS
Prediction: 0000000000000000000000000000000000000000000000000000
Loss: 15.9188
outputs.loss tensor(15.9188, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0009.flac
Waveform stats - mean: -0.0000, std: 0.0679, min: -0.4167, max: 0.4140
Resampled waveform stats - mean: -0.0000, std: 0.0679, min: -0.4167, max: 0.4140
Raw mel spectrogram stats - mean: 1.7236, std: 9.3259, min: 0.0000, max: 342.9941
Log mel spectrogram stats - mean: -5.7234, std: 4.4993, min: -13.8151, max: 5.8377
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7984, max: 2.5695
Mel spec shape: torch.Size([1, 80, 403])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7988, max: 2.5703
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WE HAD TO PASS OVER THE GROUND WHERE TROOPS HAD BEEN FIGHTING ALL DAY
Prediction: 000000000000000000000
Loss: 15.2748
outputs.loss tensor(15.2748, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/24615/5338-24615-0010.flac
Waveform stats - mean: 0.0000, std: 0.0528, min: -0.3878, max: 0.3679
Resampled waveform stats - mean: 0.0000, std: 0.0528, min: -0.3878, max: 0.3679
Raw mel spectrogram stats - mean: 1.0384, std: 7.7696, min: 0.0000, max: 316.4945
Log mel spectrogram stats - mean: -5.5808, std: 3.9703, min: -13.6936, max: 5.7573
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0433, max: 2.8557
Mel spec shape: torch.Size([1, 80, 390])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0430, max: 2.8555
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: STABLES AND OTHER OFFICES OCCUPIED ANOTHER SIDE OF THE SQUARE
Prediction: 0000000000000000
Loss: 16.5167
outputs.loss tensor(16.5167, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/136532/251-136532-0015.flac
Waveform stats - mean: -0.0000, std: 0.0670, min: -0.4572, max: 0.5809
Resampled waveform stats - mean: -0.0000, std: 0.0670, min: -0.4572, max: 0.5809
Raw mel spectrogram stats - mean: 1.6803, std: 11.3101, min: 0.0000, max: 818.8759
Log mel spectrogram stats - mean: -5.8134, std: 4.5032, min: -13.8090, max: 6.7079
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7755, max: 2.7805
Mel spec shape: torch.Size([1, 80, 764])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7754, max: 2.7812
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24576
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24576
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 172032
Inf count: 0
audio_emb.shape torch.Size([1, 48, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SO THEY JUST CAME IN HERE AND LIT THE CHARCOAL AND SAT DRINKING TOGETHER TILL THEY ALL FELL ASLEEP
Prediction: 000000000000000000000000000000
Loss: 15.8162
outputs.loss tensor(15.8162, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0033.flac
Waveform stats - mean: -0.0000, std: 0.0689, min: -0.3850, max: 0.3601
Resampled waveform stats - mean: -0.0000, std: 0.0689, min: -0.3850, max: 0.3601
Raw mel spectrogram stats - mean: 1.7783, std: 12.0966, min: 0.0000, max: 582.5898
Log mel spectrogram stats - mean: -6.2178, std: 4.4796, min: -13.7600, max: 6.3675
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6837, max: 2.8095
Mel spec shape: torch.Size([1, 80, 247])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6836, max: 2.8086
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE THREE FRIENDS WERE ASTOUNDED
Prediction: 000000000
Loss: 17.1234
outputs.loss tensor(17.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0044.flac
Waveform stats - mean: -0.0000, std: 0.0767, min: -0.4193, max: 0.3795
Resampled waveform stats - mean: -0.0000, std: 0.0767, min: -0.4193, max: 0.3795
Raw mel spectrogram stats - mean: 2.2045, std: 15.4173, min: 0.0000, max: 1032.6133
Log mel spectrogram stats - mean: -5.3546, std: 4.3344, min: -13.7802, max: 6.9398
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9439, max: 2.8365
Mel spec shape: torch.Size([1, 80, 1671])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9443, max: 2.8359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 53760
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 53760
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 376320
Inf count: 0
audio_emb.shape torch.Size([1, 105, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BESIDES I FOUND IT VERY FLATTERING TO MY VANITY TO BECOME THE SUBJECT OF THE SPECULATIVE CHATTERING OF EMPTY FOOLS WHO HAVING NOTHING ELSE TO DO ARE ALWAYS TRYING TO FIND OUT THE CAUSE OF EVERY MORAL PHENOMENON THEY MEET WITH WHICH THEIR NARROW INTELLECT CANNOT UNDERSTAND
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.6918
outputs.loss tensor(16.6918, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0007.flac
Waveform stats - mean: -0.0000, std: 0.0910, min: -0.5151, max: 0.4296
Resampled waveform stats - mean: -0.0000, std: 0.0910, min: -0.5151, max: 0.4296
Raw mel spectrogram stats - mean: 3.0980, std: 15.0947, min: 0.0000, max: 384.1810
Log mel spectrogram stats - mean: -5.6245, std: 4.8512, min: -13.8008, max: 5.9511
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6854, max: 2.3861
Mel spec shape: torch.Size([1, 80, 392])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6855, max: 2.3867
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THAT'S RIGHT MY BRAVE FIRST TENNESSEE GIVE EM HAIL COLUMBIA
Prediction: 00000000000000000000
Loss: 16.3497
outputs.loss tensor(16.3497, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0013.flac
Waveform stats - mean: -0.0000, std: 0.0466, min: -0.3156, max: 0.3167
Resampled waveform stats - mean: -0.0000, std: 0.0466, min: -0.3156, max: 0.3167
Raw mel spectrogram stats - mean: 0.8116, std: 5.2397, min: 0.0000, max: 244.3999
Log mel spectrogram stats - mean: -8.4800, std: 5.0867, min: -13.8137, max: 5.4988
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.0486, max: 2.7481
Mel spec shape: torch.Size([1, 80, 345])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.0488, max: 2.7480
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ON MONDAY THE TIDE WAS REVERSED
Prediction: 0000000000
Loss: 16.6204
outputs.loss tensor(16.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0006.flac
Waveform stats - mean: -0.0001, std: 0.0733, min: -0.2886, max: 0.4547
Resampled waveform stats - mean: -0.0001, std: 0.0733, min: -0.2886, max: 0.4547
Raw mel spectrogram stats - mean: 2.0047, std: 8.9858, min: 0.0000, max: 183.4171
Log mel spectrogram stats - mean: -5.1577, std: 4.1913, min: -13.7928, max: 5.2118
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0602, max: 2.4740
Mel spec shape: torch.Size([1, 80, 435])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0605, max: 2.4746
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE FAMILY WAS NOT ONLY THE SOCIAL UNIT BUT ALSO THE UNIT OF GOVERNMENT
Prediction: 0000000000000000
Loss: 16.7787
outputs.loss tensor(16.7787, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0039.flac
Waveform stats - mean: 0.0009, std: 0.0211, min: -0.1355, max: 0.1555
Resampled waveform stats - mean: 0.0009, std: 0.0211, min: -0.1355, max: 0.1555
Raw mel spectrogram stats - mean: 0.1586, std: 0.8468, min: 0.0000, max: 44.9145
Log mel spectrogram stats - mean: -5.7988, std: 3.1224, min: -13.2536, max: 3.8048
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3875, max: 3.0757
Mel spec shape: torch.Size([1, 80, 421])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3867, max: 3.0762
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NO WORD NO CRY JUST A COLLAPSE AND SUDDEN FALL
Prediction: 000000000000000
Loss: 15.2027
outputs.loss tensor(15.2027, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0003.flac
Waveform stats - mean: 0.0000, std: 0.0691, min: -0.3859, max: 0.4051
Resampled waveform stats - mean: 0.0000, std: 0.0691, min: -0.3859, max: 0.4051
Raw mel spectrogram stats - mean: 1.7874, std: 11.2893, min: 0.0000, max: 318.0626
Log mel spectrogram stats - mean: -5.5125, std: 4.3973, min: -13.5945, max: 5.7622
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8379, max: 2.5640
Mel spec shape: torch.Size([1, 80, 543])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8379, max: 2.5645
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS INSTANT OF PANIC WAS FOLLOWED BY A SMALL SHARP BLOW HIGH ON HIS CHEST
Prediction: 0000000000000000000000
Loss: 16.1689
outputs.loss tensor(16.1689, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0002.flac
Waveform stats - mean: -0.0001, std: 0.0464, min: -0.3621, max: 0.2853
Resampled waveform stats - mean: -0.0001, std: 0.0464, min: -0.3621, max: 0.2853
Raw mel spectrogram stats - mean: 0.8059, std: 5.2408, min: 0.0000, max: 167.5026
Log mel spectrogram stats - mean: -5.9432, std: 3.8956, min: -13.7729, max: 5.1210
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0099, max: 2.8401
Mel spec shape: torch.Size([1, 80, 274])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0098, max: 2.8398
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WELL AND HAVE WE A FAIR WIND
Prediction: 0000000000
Loss: 15.4569
outputs.loss tensor(15.4569, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0005.flac
Waveform stats - mean: -0.0000, std: 0.0706, min: -0.7238, max: 0.6483
Resampled waveform stats - mean: -0.0000, std: 0.0706, min: -0.7238, max: 0.6483
Raw mel spectrogram stats - mean: 1.8643, std: 15.3905, min: 0.0000, max: 1014.3279
Log mel spectrogram stats - mean: -5.0028, std: 3.9691, min: -13.8088, max: 6.9220
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2186, max: 3.0044
Mel spec shape: torch.Size([1, 80, 465])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2188, max: 3.0039
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I COULD NOT THINK OF ALLOWING ANY OF MY CHARGES TO TAKE SO TERRIBLE A RISK AND
Prediction: 000000000000000000000000
Loss: 15.9764
outputs.loss tensor(15.9764, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0034.flac
Waveform stats - mean: 0.0000, std: 0.0680, min: -0.3412, max: 0.4308
Resampled waveform stats - mean: 0.0000, std: 0.0680, min: -0.3412, max: 0.4308
Raw mel spectrogram stats - mean: 1.7278, std: 11.7222, min: 0.0000, max: 342.2070
Log mel spectrogram stats - mean: -5.0180, std: 3.4835, min: -13.0755, max: 5.8354
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3130, max: 3.1157
Mel spec shape: torch.Size([1, 80, 369])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3125, max: 3.1152
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SIMMER FOR A MINUTE OR TWO AND SERVE IN A TUREEN
Prediction: 0000000000000000
Loss: 15.2160
outputs.loss tensor(15.2160, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0011.flac
Waveform stats - mean: -0.0000, std: 0.0583, min: -0.5416, max: 0.4538
Resampled waveform stats - mean: -0.0000, std: 0.0583, min: -0.5416, max: 0.4538
Raw mel spectrogram stats - mean: 1.2103, std: 6.4514, min: 0.0000, max: 334.2474
Log mel spectrogram stats - mean: -4.7188, std: 3.9455, min: -13.7856, max: 5.8119
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2980, max: 2.6691
Mel spec shape: torch.Size([1, 80, 1893])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2988, max: 2.6699
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 60928
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 60928
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 426496
Inf count: 0
audio_emb.shape torch.Size([1, 119, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE RESTAURANTS OF THE PRESENT DAY THAT APPROACH NEAREST THE OLD BOHEMIAN RESTAURANTS OF PRE FIRE DAYS OF THE FRENCH CLASS ARE JACK'S IN SACRAMENTO STREET BETWEEN MONTGOMERY AND KEARNY FELIX IN MONTGOMERY STREET BETWEEN CLAY AND WASHINGTON AND THE POODLE DOG BERGEZ FRANKS IN BUSH STREET BETWEEN KEARNY AND GRANT AVENUE
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.8674
outputs.loss tensor(15.8674, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/64726/6319-64726-0010.flac
Waveform stats - mean: -0.0000, std: 0.0499, min: -0.3686, max: 0.3879
Resampled waveform stats - mean: -0.0000, std: 0.0499, min: -0.3686, max: 0.3879
Raw mel spectrogram stats - mean: 0.9324, std: 7.6643, min: 0.0000, max: 549.7763
Log mel spectrogram stats - mean: -6.7419, std: 4.1585, min: -13.7408, max: 6.3095
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6830, max: 3.1385
Mel spec shape: torch.Size([1, 80, 497])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6826, max: 3.1387
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE ENTERED A LARGE FORECOURT AND STOOD STILL WITH AMAZEMENT AND AWE
Prediction: 0000000000000000000
Loss: 15.3862
outputs.loss tensor(15.3862, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0013.flac
Waveform stats - mean: 0.0000, std: 0.0523, min: -0.4337, max: 0.4182
Resampled waveform stats - mean: 0.0000, std: 0.0523, min: -0.4337, max: 0.4182
Raw mel spectrogram stats - mean: 1.0227, std: 8.5879, min: 0.0000, max: 386.4965
Log mel spectrogram stats - mean: -5.8533, std: 3.7588, min: -13.6852, max: 5.9571
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0837, max: 3.1421
Mel spec shape: torch.Size([1, 80, 610])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0840, max: 3.1426
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE WOULD GET ONE TO DAY IT WOULD PROBABLY BE ON HIS DESK WHEN HE GOT BACK HE WOULD LOOK FOR IT AT ONCE
Prediction: 0000000000000000000000000000000
Loss: 16.1041
outputs.loss tensor(16.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/50561/174-50561-0013.flac
Waveform stats - mean: -0.0001, std: 0.0146, min: -0.1310, max: 0.1504
Resampled waveform stats - mean: -0.0001, std: 0.0146, min: -0.1310, max: 0.1504
Raw mel spectrogram stats - mean: 0.0796, std: 0.4554, min: 0.0000, max: 26.6559
Log mel spectrogram stats - mean: -7.7886, std: 3.7537, min: -13.7836, max: 3.2830
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5971, max: 2.9495
Mel spec shape: torch.Size([1, 80, 1062])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5967, max: 2.9492
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34304
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34304
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 240128
Inf count: 0
audio_emb.shape torch.Size([1, 67, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL PLAY FOR YOU NOW NEATH THE APPLE BOUGH AND YOU SHALL DREAM ON THE LAWN SO SHADY LADY LADY MY FAIR LADY O MY APPLE GOLD LADY
Prediction: 000000000000000000000000000000000000000000000
Loss: 15.3578
outputs.loss tensor(15.3578, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0049.flac
Waveform stats - mean: 0.0000, std: 0.1377, min: -0.9296, max: 0.6316
Resampled waveform stats - mean: 0.0000, std: 0.1377, min: -0.9296, max: 0.6316
Raw mel spectrogram stats - mean: 7.0552, std: 35.4361, min: 0.0000, max: 1247.8286
Log mel spectrogram stats - mean: -4.3908, std: 4.6616, min: -13.4540, max: 7.1292
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9442, max: 2.4713
Mel spec shape: torch.Size([1, 80, 277])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9443, max: 2.4707
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: DAMN YOUR IMPERTINENCE SIR BURST OUT BURGESS
Prediction: 000000000000000000
Loss: 15.2383
outputs.loss tensor(15.2383, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0025.flac
Waveform stats - mean: -0.0006, std: 0.0491, min: -0.3248, max: 0.3913
Resampled waveform stats - mean: -0.0006, std: 0.0491, min: -0.3248, max: 0.3913
Raw mel spectrogram stats - mean: 0.9025, std: 7.6273, min: 0.0000, max: 367.1991
Log mel spectrogram stats - mean: -6.9634, std: 3.8234, min: -13.7832, max: 5.9059
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7837, max: 3.3659
Mel spec shape: torch.Size([1, 80, 556])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7842, max: 3.3652
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HILDA'S FACE QUIVERED BUT SHE WHISPERED YES I THINK IT MUST HAVE BEEN
Prediction: 000000000000000000000
Loss: 16.3721
outputs.loss tensor(16.3721, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0017.flac
Waveform stats - mean: -0.0001, std: 0.0584, min: -0.3536, max: 0.4509
Resampled waveform stats - mean: -0.0001, std: 0.0584, min: -0.3536, max: 0.4509
Raw mel spectrogram stats - mean: 1.2787, std: 6.3676, min: 0.0000, max: 233.9500
Log mel spectrogram stats - mean: -5.2422, std: 4.0183, min: -13.6776, max: 5.4551
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0993, max: 2.6621
Mel spec shape: torch.Size([1, 80, 1381])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0996, max: 2.6621
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 44544
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 44544
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 311808
Inf count: 0
audio_emb.shape torch.Size([1, 87, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HERE IS THE SUPREME MYSTERY THAT IS THE ESSENCE OF WORSHIP WITHOUT WHICH THERE CAN BE NO RELIGION AND IN THE PRESENCE OF THIS MYSTERY OUR ATTITUDE CANNOT BE VERY UNLIKE THAT OF THE NATURAL PHILOSOPHER WHO BEHOLDS WITH AWE THE DIVINE IN ALL CREATION
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2357
outputs.loss tensor(16.2357, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0001.flac
Waveform stats - mean: -0.0000, std: 0.0749, min: -0.6593, max: 0.6059
Resampled waveform stats - mean: -0.0000, std: 0.0749, min: -0.6593, max: 0.6059
Raw mel spectrogram stats - mean: 2.1018, std: 17.4293, min: 0.0000, max: 1739.2380
Log mel spectrogram stats - mean: -4.1266, std: 3.4587, min: -12.8324, max: 7.4612
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5170, max: 3.3503
Mel spec shape: torch.Size([1, 80, 1246])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5176, max: 3.3496
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 39936
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 39936
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 279552
Inf count: 0
audio_emb.shape torch.Size([1, 78, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE DISTINCTIONS OF MORAL VALUES HAVE EITHER ORIGINATED IN A RULING CASTE PLEASANTLY CONSCIOUS OF BEING DIFFERENT FROM THE RULED OR AMONG THE RULED CLASS THE SLAVES AND DEPENDENTS OF ALL SORTS
Prediction: 000000000000000000000000000000000000000000000000000000000
Loss: 15.7108
outputs.loss tensor(15.7108, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0008.flac
Waveform stats - mean: 0.0000, std: 0.0614, min: -0.3311, max: 0.2885
Resampled waveform stats - mean: 0.0000, std: 0.0614, min: -0.3311, max: 0.2885
Raw mel spectrogram stats - mean: 1.4051, std: 7.3553, min: 0.0000, max: 150.8205
Log mel spectrogram stats - mean: -8.7845, std: 5.6170, min: -13.8154, max: 5.0161
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -0.8957, max: 2.4569
Mel spec shape: torch.Size([1, 80, 196])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -0.8955, max: 2.4570
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A MAN IN THE WELL
Prediction: 00000
Loss: 15.5618
outputs.loss tensor(15.5618, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0008.flac
Waveform stats - mean: -0.0001, std: 0.0380, min: -0.3748, max: 0.3634
Resampled waveform stats - mean: -0.0001, std: 0.0380, min: -0.3748, max: 0.3634
Raw mel spectrogram stats - mean: 0.4722, std: 6.8027, min: 0.0000, max: 939.2111
Log mel spectrogram stats - mean: -6.9168, std: 4.1090, min: -13.7716, max: 6.8450
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6682, max: 3.3492
Mel spec shape: torch.Size([1, 80, 502])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6680, max: 3.3496
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE PATERNAL PARENT HAS A RIGHT TO HIS INFANTS NO DOUBT THAT WAS BOZZLE'S LAW
Prediction: 000000000000000000000000
Loss: 16.1211
outputs.loss tensor(16.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0034.flac
Waveform stats - mean: -0.0000, std: 0.0657, min: -0.3951, max: 0.3511
Resampled waveform stats - mean: -0.0000, std: 0.0657, min: -0.3951, max: 0.3511
Raw mel spectrogram stats - mean: 1.6183, std: 10.6510, min: 0.0000, max: 433.0760
Log mel spectrogram stats - mean: -6.0020, std: 4.7091, min: -13.7997, max: 6.0709
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6559, max: 2.5637
Mel spec shape: torch.Size([1, 80, 787])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6562, max: 2.5645
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I DECLARED MYSELF QUITE WILLING FOR IT WAS NECESSARY TO BRAZEN IT OUT AFTER HAVING VENTURED AS FAR AS I HAD DONE
Prediction: 0000000000000000000000000000000000
Loss: 16.1774
outputs.loss tensor(16.1774, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0039.flac
Waveform stats - mean: -0.0007, std: 0.0734, min: -0.5034, max: 0.4095
Resampled waveform stats - mean: -0.0007, std: 0.0734, min: -0.5034, max: 0.4095
Raw mel spectrogram stats - mean: 2.0168, std: 13.2429, min: 0.0000, max: 485.7460
Log mel spectrogram stats - mean: -6.1570, std: 3.9475, min: -13.7091, max: 6.1857
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9131, max: 3.1267
Mel spec shape: torch.Size([1, 80, 472])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9131, max: 3.1270
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU SEE LOVING SOME ONE AS I LOVE YOU MAKES THE WHOLE WORLD DIFFERENT
Prediction: 00000000000000000000
Loss: 15.9827
outputs.loss tensor(15.9827, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/281318/7850-281318-0022.flac
Waveform stats - mean: -0.0000, std: 0.0548, min: -0.3927, max: 0.4217
Resampled waveform stats - mean: -0.0000, std: 0.0548, min: -0.3927, max: 0.4217
Raw mel spectrogram stats - mean: 1.1212, std: 12.0075, min: 0.0000, max: 526.2380
Log mel spectrogram stats - mean: -6.5205, std: 3.9824, min: -13.7504, max: 6.2658
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8154, max: 3.2107
Mel spec shape: torch.Size([1, 80, 386])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8154, max: 3.2109
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT THE WOOD PIGEON WAS IN THE WORST CASE OF THEM ALL
Prediction: 0000000000000000000
Loss: 15.1225
outputs.loss tensor(15.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0030.flac
Waveform stats - mean: 0.0011, std: 0.0237, min: -0.3944, max: 0.4897
Resampled waveform stats - mean: 0.0011, std: 0.0237, min: -0.3944, max: 0.4897
Raw mel spectrogram stats - mean: 0.1959, std: 3.4168, min: 0.0000, max: 647.8518
Log mel spectrogram stats - mean: -5.4281, std: 2.9648, min: -12.7786, max: 6.4737
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4793, max: 4.0144
Mel spec shape: torch.Size([1, 80, 1399])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4785, max: 4.0156
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 45056
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 45056
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 315392
Inf count: 0
audio_emb.shape torch.Size([1, 88, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEIR GREETING WAS CORDIAL AND THE LINES ON THE LATTER'S FACE RELAXED A LITTLE AS HE MET THE STILL BRIGHT EYE OF THE MAN UPON WHOSE INSTINCT AND JUDGMENT SO MUCH RELIANCE HAD ALWAYS BEEN PLACED
Prediction: 000000000000000000000000000000000000000000000000000000000000
Loss: 16.0304
outputs.loss tensor(16.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/57405/6319-57405-0010.flac
Waveform stats - mean: -0.0000, std: 0.0670, min: -0.4858, max: 0.5674
Resampled waveform stats - mean: -0.0000, std: 0.0670, min: -0.4858, max: 0.5674
Raw mel spectrogram stats - mean: 1.6760, std: 17.3892, min: 0.0000, max: 563.3270
Log mel spectrogram stats - mean: -6.6826, std: 3.7520, min: -13.7965, max: 6.3339
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8960, max: 3.4692
Mel spec shape: torch.Size([1, 80, 335])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8965, max: 3.4688
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT DID HE MEAN ASKED PYRRHA
Prediction: 00000000000
Loss: 14.9569
outputs.loss tensor(14.9569, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0007.flac
Waveform stats - mean: -0.0000, std: 0.0749, min: -0.4085, max: 0.3907
Resampled waveform stats - mean: -0.0000, std: 0.0749, min: -0.4085, max: 0.3907
Raw mel spectrogram stats - mean: 2.1010, std: 15.5754, min: 0.0000, max: 838.2988
Log mel spectrogram stats - mean: -5.6958, std: 4.5216, min: -13.8033, max: 6.7314
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7931, max: 2.7484
Mel spec shape: torch.Size([1, 80, 1786])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7930, max: 2.7480
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 57344
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 57344
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 401408
Inf count: 0
audio_emb.shape torch.Size([1, 112, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHENEVER WE COULD CONTRIVE TO GET INTO A CHURCH TOWER WE THOUGHT IT GREAT FUN TO FRIGHTEN ALL THE PARISH BY RINGING THE ALARM BELL AS IF SOME FIRE HAD BROKEN OUT BUT THAT WAS NOT ALL WE ALWAYS CUT THE BELL ROPES SO THAT IN THE MORNING THE CHURCHWARDENS HAD NO MEANS OF SUMMONING THE FAITHFUL TO EARLY MASS
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.0829
outputs.loss tensor(16.0829, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0019.flac
Waveform stats - mean: -0.0000, std: 0.0363, min: -0.3672, max: 0.3505
Resampled waveform stats - mean: -0.0000, std: 0.0363, min: -0.3672, max: 0.3505
Raw mel spectrogram stats - mean: 0.4927, std: 3.9738, min: 0.0000, max: 228.2330
Log mel spectrogram stats - mean: -5.1110, std: 3.3403, min: -13.8057, max: 5.4304
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.6030, max: 3.1558
Mel spec shape: torch.Size([1, 80, 310])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.6035, max: 3.1562
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: STACY BROWN'S LEFT LEG SWUNG OVER THE SADDLE
Prediction: 00000000000000
Loss: 16.4325
outputs.loss tensor(16.4325, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0021.flac
Waveform stats - mean: -0.0000, std: 0.0741, min: -0.4777, max: 0.4269
Resampled waveform stats - mean: -0.0000, std: 0.0741, min: -0.4777, max: 0.4269
Raw mel spectrogram stats - mean: 2.0132, std: 16.4459, min: 0.0000, max: 755.1814
Log mel spectrogram stats - mean: -5.1351, std: 3.4763, min: -13.0143, max: 6.6270
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2666, max: 3.3835
Mel spec shape: torch.Size([1, 80, 619])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2656, max: 3.3828
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY MADE HASTE TO OVERTAKE THEM WHICH AS THE PARTY MOVED SLOWLY THEY WERE ABLE TO DO WITH EASE
Prediction: 00000000000000000000000000000
Loss: 16.1429
outputs.loss tensor(16.1429, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0088.flac
Waveform stats - mean: 0.0008, std: 0.0155, min: -0.2108, max: 0.2081
Resampled waveform stats - mean: 0.0008, std: 0.0155, min: -0.2108, max: 0.2081
Raw mel spectrogram stats - mean: 0.0862, std: 0.5860, min: 0.0000, max: 52.5991
Log mel spectrogram stats - mean: -6.1804, std: 2.8073, min: -13.0367, max: 3.9627
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4423, max: 3.6131
Mel spec shape: torch.Size([1, 80, 1082])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4414, max: 3.6133
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34816
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34816
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 243712
Inf count: 0
audio_emb.shape torch.Size([1, 68, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SHE STRUCK THE BLOW HERSELF AND THE STRENGTH OF PURPOSE WHICH LED HER TO DO THIS GAVE HER THE ADDITIONAL FORCE TO PULL THE WEAPON OUT AND FLING IT FROM HER
Prediction: 000000000000000000000000000000000000000000
Loss: 15.8823
outputs.loss tensor(15.8823, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0029.flac
Waveform stats - mean: -0.0001, std: 0.0354, min: -0.3301, max: 0.3195
Resampled waveform stats - mean: -0.0001, std: 0.0354, min: -0.3301, max: 0.3195
Raw mel spectrogram stats - mean: 0.4568, std: 4.1397, min: 0.0000, max: 190.6158
Log mel spectrogram stats - mean: -6.8606, std: 3.9753, min: -13.7692, max: 5.2503
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7378, max: 3.0465
Mel spec shape: torch.Size([1, 80, 336])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7383, max: 3.0469
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'VE WATCHED AS SHARP AS WATCHING CAN GO PRETTY NEAR
Prediction: 00000000000000000
Loss: 15.8826
outputs.loss tensor(15.8826, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0005.flac
Waveform stats - mean: 0.0013, std: 0.0107, min: -0.1141, max: 0.0858
Resampled waveform stats - mean: 0.0013, std: 0.0107, min: -0.1141, max: 0.0858
Raw mel spectrogram stats - mean: 0.0373, std: 0.3165, min: 0.0000, max: 25.5048
Log mel spectrogram stats - mean: -6.9412, std: 2.7997, min: -12.4884, max: 3.2389
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9814, max: 3.6361
Mel spec shape: torch.Size([1, 80, 203])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9814, max: 3.6367
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GEORGE NODDED
Prediction: 000000
Loss: 16.0409
outputs.loss tensor(16.0409, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0027.flac
Waveform stats - mean: -0.0000, std: 0.0202, min: -0.1448, max: 0.1736
Resampled waveform stats - mean: -0.0000, std: 0.0202, min: -0.1448, max: 0.1736
Raw mel spectrogram stats - mean: 0.1526, std: 1.1016, min: 0.0000, max: 53.2287
Log mel spectrogram stats - mean: -7.2290, std: 3.9611, min: -13.7757, max: 3.9746
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6527, max: 2.8284
Mel spec shape: torch.Size([1, 80, 434])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6523, max: 2.8281
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS OWN SKIN HAD SIZZLED UNDER THE RED HOT BRAND HE MURMURED SOFTLY
Prediction: 000000000000000000000000
Loss: 15.9421
outputs.loss tensor(15.9421, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0021.flac
Waveform stats - mean: 0.0000, std: 0.0439, min: -0.4115, max: 0.4011
Resampled waveform stats - mean: 0.0000, std: 0.0439, min: -0.4115, max: 0.4011
Raw mel spectrogram stats - mean: 0.7210, std: 4.0058, min: 0.0000, max: 116.8147
Log mel spectrogram stats - mean: -6.7310, std: 4.5268, min: -13.8013, max: 4.7606
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5619, max: 2.5386
Mel spec shape: torch.Size([1, 80, 282])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5615, max: 2.5391
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN IT I WAS DEPOSITED WITH MY LUGGAGE
Prediction: 0000000000000
Loss: 15.5166
outputs.loss tensor(15.5166, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0026.flac
Waveform stats - mean: 0.0016, std: 0.2557, min: -0.8283, max: 0.8185
Resampled waveform stats - mean: 0.0016, std: 0.2557, min: -0.8283, max: 0.8185
Raw mel spectrogram stats - mean: 24.4937, std: 201.7679, min: 0.0000, max: 7152.2852
Log mel spectrogram stats - mean: -2.3433, std: 3.6639, min: -13.0640, max: 8.8752
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.9261, max: 3.0619
Mel spec shape: torch.Size([1, 80, 584])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.9258, max: 3.0625
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WOULD HAVE TAKEN MANY KNAPSACKS TO HOLD ALL THE GIFTS SHOWERED UPON HIM BY HIS FRIENDS AND NEIGHBORS
Prediction: 000000000000000000000000000000000
Loss: 16.2459
outputs.loss tensor(16.2459, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0013.flac
Waveform stats - mean: 0.0000, std: 0.0482, min: -0.3381, max: 0.4018
Resampled waveform stats - mean: 0.0000, std: 0.0482, min: -0.3381, max: 0.4018
Raw mel spectrogram stats - mean: 0.8693, std: 5.3675, min: 0.0000, max: 201.3511
Log mel spectrogram stats - mean: -5.7246, std: 3.9871, min: -13.7613, max: 5.3051
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0157, max: 2.7663
Mel spec shape: torch.Size([1, 80, 317])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0156, max: 2.7656
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU COULD GET HOME EASY TOO IT ISN'T VERY FAR
Prediction: 00000000000000
Loss: 16.1496
outputs.loss tensor(16.1496, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/136532/251-136532-0006.flac
Waveform stats - mean: -0.0000, std: 0.1003, min: -0.5841, max: 0.5581
Resampled waveform stats - mean: -0.0000, std: 0.1003, min: -0.5841, max: 0.5581
Raw mel spectrogram stats - mean: 3.7630, std: 25.4503, min: 0.0000, max: 898.3008
Log mel spectrogram stats - mean: -5.1524, std: 4.5997, min: -13.8101, max: 6.8005
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8822, max: 2.5986
Mel spec shape: torch.Size([1, 80, 257])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8818, max: 2.5996
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TONY'S FOUND THE MARTIANS
Prediction: 00000000
Loss: 15.6709
outputs.loss tensor(15.6709, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0005.flac
Waveform stats - mean: -0.0001, std: 0.0644, min: -0.3430, max: 0.5117
Resampled waveform stats - mean: -0.0001, std: 0.0644, min: -0.3430, max: 0.5117
Raw mel spectrogram stats - mean: 1.5567, std: 9.6033, min: 0.0000, max: 541.1391
Log mel spectrogram stats - mean: -5.2742, std: 3.9190, min: -13.6352, max: 6.2937
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1334, max: 2.9518
Mel spec shape: torch.Size([1, 80, 676])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1328, max: 2.9512
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: INDEED THE DISTINCTIVE WORK OF BOTH GRANDPARENTS IS THAT OF ACQUAINTING THE YOUTH WITH THE NATIONAL TRADITIONS AND BELIEFS
Prediction: 00000000000000000000000000000000000
Loss: 15.4274
outputs.loss tensor(15.4274, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0013.flac
Waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Resampled waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Raw mel spectrogram stats - mean: 1.2006, std: 7.3066, min: 0.0000, max: 190.1952
Log mel spectrogram stats - mean: -6.7061, std: 4.8185, min: -13.8154, max: 5.2481
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4754, max: 2.4809
Mel spec shape: torch.Size([1, 80, 334])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4756, max: 2.4805
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SIT DOWN SAID MISSUS PRESTY
Prediction: 0000000000
Loss: 15.8070
outputs.loss tensor(15.8070, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34629/5895-34629-0010.flac
Waveform stats - mean: 0.0000, std: 0.0327, min: -0.3240, max: 0.4580
Resampled waveform stats - mean: 0.0000, std: 0.0327, min: -0.3240, max: 0.4580
Raw mel spectrogram stats - mean: 0.3998, std: 2.3602, min: 0.0000, max: 79.7825
Log mel spectrogram stats - mean: -6.3374, std: 3.8429, min: -13.7325, max: 4.3793
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9244, max: 2.7887
Mel spec shape: torch.Size([1, 80, 216])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9248, max: 2.7891
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY BEGAN THEIR PERFORMANCES
Prediction: 00000000
Loss: 17.3517
outputs.loss tensor(17.3517, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34629/5895-34629-0025.flac
Waveform stats - mean: -0.0000, std: 0.0512, min: -0.4189, max: 0.5045
Resampled waveform stats - mean: -0.0000, std: 0.0512, min: -0.4189, max: 0.5045
Raw mel spectrogram stats - mean: 0.9799, std: 6.6749, min: 0.0000, max: 336.0774
Log mel spectrogram stats - mean: -4.9296, std: 3.8593, min: -13.4755, max: 5.8173
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2144, max: 2.7847
Mel spec shape: torch.Size([1, 80, 483])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2148, max: 2.7852
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS CONNOISSEUR WAS SUDDENLY FASCINATED AND HAD ADOPTED THE LAUGHING MAN
Prediction: 000000000000000000000000000
Loss: 15.6165
outputs.loss tensor(15.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121123/84-121123-0006.flac
Waveform stats - mean: -0.0000, std: 0.0391, min: -0.2155, max: 0.2166
Resampled waveform stats - mean: -0.0000, std: 0.0391, min: -0.2155, max: 0.2166
Raw mel spectrogram stats - mean: 0.5722, std: 3.9660, min: 0.0000, max: 104.4294
Log mel spectrogram stats - mean: -5.9615, std: 3.2575, min: -13.2414, max: 4.6485
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2348, max: 3.2571
Mel spec shape: torch.Size([1, 80, 563])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2344, max: 3.2578
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: FOR SOME TIME NOTHING WAS HEARD IN THAT CHAMBER BUT SOBS EXCLAMATIONS AND PRAYERS
Prediction: 00000000000000000000000
Loss: 16.0402
outputs.loss tensor(16.0402, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0000.flac
Waveform stats - mean: -0.0000, std: 0.0334, min: -0.3972, max: 0.2646
Resampled waveform stats - mean: -0.0000, std: 0.0334, min: -0.3972, max: 0.2646
Raw mel spectrogram stats - mean: 0.4180, std: 3.9502, min: 0.0000, max: 184.7757
Log mel spectrogram stats - mean: -5.6809, std: 3.3325, min: -13.7763, max: 5.2191
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4293, max: 3.2709
Mel spec shape: torch.Size([1, 80, 282])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4297, max: 3.2715
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE NO DOUBT WOULD BRING FOOD OF SOME KIND WITH HIM
Prediction: 000000000000000
Loss: 16.2357
outputs.loss tensor(16.2357, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0037.flac
Waveform stats - mean: 0.0011, std: 0.2364, min: -0.8328, max: 0.8192
Resampled waveform stats - mean: 0.0011, std: 0.2364, min: -0.8328, max: 0.8192
Raw mel spectrogram stats - mean: 20.9441, std: 156.7751, min: 0.0000, max: 6726.5884
Log mel spectrogram stats - mean: -2.9339, std: 4.4628, min: -13.8154, max: 8.8138
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4383, max: 2.6324
Mel spec shape: torch.Size([1, 80, 667])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4375, max: 2.6328
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN THE GOOD SOUL OPENLY SHOULDERED THE BURDEN SHE HAD BORNE SO LONG IN SECRET AND BRAVELY TRUDGED ON ALONE
Prediction: 000000000000000000000000000000000000
Loss: 15.5616
outputs.loss tensor(15.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0015.flac
Waveform stats - mean: -0.0001, std: 0.0482, min: -0.4833, max: 0.4001
Resampled waveform stats - mean: -0.0001, std: 0.0482, min: -0.4833, max: 0.4001
Raw mel spectrogram stats - mean: 0.8652, std: 7.4454, min: 0.0000, max: 429.4356
Log mel spectrogram stats - mean: -5.2660, std: 3.7467, min: -12.8057, max: 6.0625
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0123, max: 3.0236
Mel spec shape: torch.Size([1, 80, 250])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0117, max: 3.0234
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE PROFESSOR KNEW WHOM HE HAD TO DEAL WITH
Prediction: 00000000000000
Loss: 15.5906
outputs.loss tensor(15.5906, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0004.flac
Waveform stats - mean: 0.0000, std: 0.0690, min: -0.5465, max: 0.4142
Resampled waveform stats - mean: 0.0000, std: 0.0690, min: -0.5465, max: 0.4142
Raw mel spectrogram stats - mean: 1.7786, std: 10.5638, min: 0.0000, max: 533.2426
Log mel spectrogram stats - mean: -4.2733, std: 3.5340, min: -12.7005, max: 6.2790
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3846, max: 2.9859
Mel spec shape: torch.Size([1, 80, 699])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3848, max: 2.9863
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HONOURS WHATEVER HE RECOGNIZES IN HIMSELF SUCH MORALITY EQUALS SELF GLORIFICATION
Prediction: 0000000000000000000000000
Loss: 16.5555
outputs.loss tensor(16.5555, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170138/1462-170138-0023.flac
Waveform stats - mean: -0.0006, std: 0.0756, min: -0.5367, max: 0.6733
Resampled waveform stats - mean: -0.0006, std: 0.0756, min: -0.5367, max: 0.6733
Raw mel spectrogram stats - mean: 2.1431, std: 18.2104, min: 0.0000, max: 1742.0795
Log mel spectrogram stats - mean: -6.0125, std: 4.0046, min: -13.7526, max: 7.4628
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9328, max: 3.3649
Mel spec shape: torch.Size([1, 80, 951])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9326, max: 3.3652
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30720
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30720
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 215040
Inf count: 0
audio_emb.shape torch.Size([1, 60, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN THE HALF LIGHT HE LOOKED ABOUT AT THE STALLS AND BOXES AND SMILED A LITTLE CONSCIOUSLY RECALLING WITH AMUSEMENT SIR HARRY'S JUDICIAL FROWN
Prediction: 0000000000000000000000000000000000000000000
Loss: 16.3081
outputs.loss tensor(16.3081, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/161169/2803-161169-0000.flac
Waveform stats - mean: -0.0000, std: 0.0304, min: -0.3062, max: 0.2680
Resampled waveform stats - mean: -0.0000, std: 0.0304, min: -0.3062, max: 0.2680
Raw mel spectrogram stats - mean: 0.3431, std: 1.8969, min: 0.0000, max: 107.6582
Log mel spectrogram stats - mean: -7.5316, std: 4.2110, min: -13.8098, max: 4.6790
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4909, max: 2.8997
Mel spec shape: torch.Size([1, 80, 1171])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4912, max: 2.9004
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ONCE THERE WAS A FATHER WHO THOUGHT HE WOULD BUILD FOR HIS CHILDREN A BEAUTIFUL HOME PUTTING INTO IT EVERY THING THEY COULD NEED OR DESIRE THROUGHOUT THEIR LIVES
Prediction: 00000000000000000000000000000000000000000000
Loss: 16.6284
outputs.loss tensor(16.6284, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0069.flac
Waveform stats - mean: -0.0000, std: 0.0302, min: -0.2385, max: 0.2039
Resampled waveform stats - mean: -0.0000, std: 0.0302, min: -0.2385, max: 0.2039
Raw mel spectrogram stats - mean: 0.3405, std: 1.9945, min: 0.0000, max: 82.2956
Log mel spectrogram stats - mean: -6.2492, std: 3.9959, min: -13.7915, max: 4.4103
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8875, max: 2.6676
Mel spec shape: torch.Size([1, 80, 495])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8877, max: 2.6680
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IF I HAD KNOWN THEY WERE COMING TO NIGHT I WOULD HAVE SEEN TO IT THAT HE WENT TO BED AT THE SAME TIME I DID
Prediction: 000000000000000000000000000000000
Loss: 15.8841
outputs.loss tensor(15.8841, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0001.flac
Waveform stats - mean: -0.0000, std: 0.0753, min: -0.4690, max: 0.6448
Resampled waveform stats - mean: -0.0000, std: 0.0753, min: -0.4690, max: 0.6448
Raw mel spectrogram stats - mean: 2.1271, std: 10.3380, min: 0.0000, max: 449.4019
Log mel spectrogram stats - mean: -5.6251, std: 4.6853, min: -13.8127, max: 6.1079
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7475, max: 2.5042
Mel spec shape: torch.Size([1, 80, 1035])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7471, max: 2.5039
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE SAILING MASTER ANNOUNCED THAT HE HAD ORDERS TO TAKE THE VESSEL BACK TO HER PORT WITH NO OTHER EXPLANATION THAN THAT THE CRUISE WAS OVER
Prediction: 00000000000000000000000000000000000000
Loss: 16.1960
outputs.loss tensor(16.1960, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0040.flac
Waveform stats - mean: -0.0000, std: 0.1255, min: -0.6328, max: 0.6664
Resampled waveform stats - mean: -0.0000, std: 0.1255, min: -0.6328, max: 0.6664
Raw mel spectrogram stats - mean: 5.8839, std: 26.2130, min: 0.0000, max: 641.0500
Log mel spectrogram stats - mean: -4.3794, std: 4.5555, min: -13.6946, max: 6.4631
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0448, max: 2.3801
Mel spec shape: torch.Size([1, 80, 279])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0449, max: 2.3809
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: DOCTOR WE ALL HAVE OUR CROSSES HAVE WE NOT
Prediction: 000000000000
Loss: 16.6214
outputs.loss tensor(16.6214, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0004.flac
Waveform stats - mean: -0.0000, std: 0.0295, min: -0.3117, max: 0.3279
Resampled waveform stats - mean: -0.0000, std: 0.0295, min: -0.3117, max: 0.3279
Raw mel spectrogram stats - mean: 0.3203, std: 3.2516, min: 0.0000, max: 215.3509
Log mel spectrogram stats - mean: -6.4113, std: 3.7111, min: -13.7783, max: 5.3723
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9851, max: 3.1752
Mel spec shape: torch.Size([1, 80, 1463])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9854, max: 3.1758
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 47104
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 47104
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 329728
Inf count: 0
audio_emb.shape torch.Size([1, 92, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MISSUS BOZZLE WAS DISPOSED TO THINK THAT LADIES OF QUALITY AMONG WHOM MADAME T WAS ENTITLED IN HER ESTIMATION TO TAKE RANK WERE SELDOM BETTER THAN THEY OUGHT TO BE AND SHE WAS QUITE WILLING THAT HER HUSBAND SHOULD EARN HIS BREAD BY WATCHING THE LADY OR THE LADY'S LOVER
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.4031
outputs.loss tensor(16.4031, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0038.flac
Waveform stats - mean: -0.0000, std: 0.0623, min: -0.4187, max: 0.2643
Resampled waveform stats - mean: -0.0000, std: 0.0623, min: -0.4187, max: 0.2643
Raw mel spectrogram stats - mean: 1.4360, std: 9.1187, min: 0.0000, max: 350.7572
Log mel spectrogram stats - mean: -5.1382, std: 3.7587, min: -13.5451, max: 5.8601
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2367, max: 2.9261
Mel spec shape: torch.Size([1, 80, 453])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2363, max: 2.9258
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: FROM FIFTEEN TO TWENTY MINUTES WILL BE REQUIRED TO BAKE THEM NICELY
Prediction: 00000000000000000000
Loss: 16.1061
outputs.loss tensor(16.1061, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0002.flac
Waveform stats - mean: -0.0001, std: 0.0667, min: -0.4828, max: 0.5173
Resampled waveform stats - mean: -0.0001, std: 0.0667, min: -0.4828, max: 0.5173
Raw mel spectrogram stats - mean: 1.6712, std: 12.2240, min: 0.0000, max: 548.5284
Log mel spectrogram stats - mean: -5.5093, std: 4.0643, min: -13.7117, max: 6.3072
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0181, max: 2.9074
Mel spec shape: torch.Size([1, 80, 1012])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0176, max: 2.9082
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS SILENT BECAUSE ALL SPEECH IS OF NECESSITY FEEBLE AND IMPERFECT THEREFORE THE SOULS OF MY ANCESTORS ASCENDED TO GOD IN WORDLESS ADORATION
Prediction: 0000000000000000000000000000000000000000000000
Loss: 16.0178
outputs.loss tensor(16.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0042.flac
Waveform stats - mean: -0.0000, std: 0.0639, min: -0.4063, max: 0.3685
Resampled waveform stats - mean: -0.0000, std: 0.0639, min: -0.4063, max: 0.3685
Raw mel spectrogram stats - mean: 1.5317, std: 10.2535, min: 0.0000, max: 654.7586
Log mel spectrogram stats - mean: -6.3932, std: 4.5982, min: -13.7913, max: 6.4843
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6089, max: 2.8005
Mel spec shape: torch.Size([1, 80, 1116])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6094, max: 2.8008
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 35840
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 35840
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 250880
Inf count: 0
audio_emb.shape torch.Size([1, 70, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AS FOR THE EUCHARIST TRANSUBSTANTIATION THE REAL PRESENCE IT WAS ALL NO MYSTERY TO THEM BUT PALPABLE EVIDENCE AND YET THEY WERE NOT JESUITS
Prediction: 0000000000000000000000000000000000000000000
Loss: 16.1866
outputs.loss tensor(16.1866, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0018.flac
Waveform stats - mean: -0.0000, std: 0.0521, min: -0.4213, max: 0.2999
Resampled waveform stats - mean: -0.0000, std: 0.0521, min: -0.4213, max: 0.2999
Raw mel spectrogram stats - mean: 1.0145, std: 5.2895, min: 0.0000, max: 170.7786
Log mel spectrogram stats - mean: -4.7355, std: 3.3226, min: -13.7466, max: 5.1404
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.7120, max: 2.9723
Mel spec shape: torch.Size([1, 80, 707])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.7129, max: 2.9727
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS HARDIER THAN THE ORANGE AND AS ONE OF THE CITRON TRIBE WAS BROUGHT INTO EUROPE BY THE ARABIANS
Prediction: 0000000000000000000000000000
Loss: 16.5984
outputs.loss tensor(16.5984, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0026.flac
Waveform stats - mean: 0.0000, std: 0.0682, min: -0.7360, max: 0.7827
Resampled waveform stats - mean: 0.0000, std: 0.0682, min: -0.7360, max: 0.7827
Raw mel spectrogram stats - mean: 1.7381, std: 14.0286, min: 0.0000, max: 699.1981
Log mel spectrogram stats - mean: -6.9186, std: 4.8802, min: -13.8126, max: 6.5499
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4127, max: 2.7598
Mel spec shape: torch.Size([1, 80, 482])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4131, max: 2.7598
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YET THE ORIGINAL WEARS TO COMMON EYES A VERY DIFFERENT EXPRESSION
Prediction: 00000000000000000
Loss: 16.0808
outputs.loss tensor(16.0808, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0070.flac
Waveform stats - mean: 0.0008, std: 0.0138, min: -0.1898, max: 0.1751
Resampled waveform stats - mean: 0.0008, std: 0.0138, min: -0.1898, max: 0.1751
Raw mel spectrogram stats - mean: 0.0682, std: 0.5115, min: 0.0000, max: 91.0425
Log mel spectrogram stats - mean: -6.2952, std: 2.9185, min: -13.6149, max: 4.5113
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5080, max: 3.7028
Mel spec shape: torch.Size([1, 80, 1174])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5078, max: 3.7031
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL WAIT HERE TILL YOU'RE READY EXPLAIN YOURSELF TO THE LADY TELL HER I'M AN OLD AND RHEUMATIC INVALID WHO HAS BEEN USED TO ASKING HIS OWN QUESTIONS
Prediction: 00000000000000000000000000000000000000000000
Loss: 16.0304
outputs.loss tensor(16.0304, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0011.flac
Waveform stats - mean: -0.0000, std: 0.0620, min: -0.3160, max: 0.3880
Resampled waveform stats - mean: -0.0000, std: 0.0620, min: -0.3160, max: 0.3880
Raw mel spectrogram stats - mean: 1.4391, std: 8.0102, min: 0.0000, max: 235.0202
Log mel spectrogram stats - mean: -6.4140, std: 4.5357, min: -13.8107, max: 5.4597
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6308, max: 2.6178
Mel spec shape: torch.Size([1, 80, 748])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6309, max: 2.6172
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALL DISCARDED PORTIONS OF THE HUMAN BODY STILL REMAIN PART OF IT ATTACHED TO IT BY INTANGIBLE CONNECTIONS
Prediction: 000000000000000000000000000
Loss: 16.4011
outputs.loss tensor(16.4011, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0040.flac
Waveform stats - mean: 0.0000, std: 0.0576, min: -0.5091, max: 0.3383
Resampled waveform stats - mean: 0.0000, std: 0.0576, min: -0.5091, max: 0.3383
Raw mel spectrogram stats - mean: 1.2264, std: 8.3986, min: 0.0000, max: 605.1581
Log mel spectrogram stats - mean: -5.1896, std: 3.6146, min: -12.6644, max: 6.4055
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0680, max: 3.2078
Mel spec shape: torch.Size([1, 80, 670])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0684, max: 3.2070
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY WERE ALL TAKEN ABACK AND NOT ONE OF THEM DARED TO UTTER A WORD SUCH DEFERENCE DID THEY PAY HIM
Prediction: 00000000000000000000000000000
Loss: 16.2206
outputs.loss tensor(16.2206, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0041.flac
Waveform stats - mean: -0.0000, std: 0.0698, min: -0.4329, max: 0.3983
Resampled waveform stats - mean: -0.0000, std: 0.0698, min: -0.4329, max: 0.3983
Raw mel spectrogram stats - mean: 1.8265, std: 15.7946, min: 0.0000, max: 1058.7787
Log mel spectrogram stats - mean: -6.1233, std: 4.3620, min: -13.8079, max: 6.9649
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7617, max: 3.0005
Mel spec shape: torch.Size([1, 80, 703])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7617, max: 3.0000
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY WERE NOT ONLY GOOD CHRISTIANS AND FAITHFUL TO THE CHURCH BUT EVEN REAL DEVOTEES AND FULL OF SCRUPLES
Prediction: 0000000000000000000000000000000
Loss: 16.1761
outputs.loss tensor(16.1761, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0011.flac
Waveform stats - mean: -0.0001, std: 0.0669, min: -0.5368, max: 0.5670
Resampled waveform stats - mean: -0.0001, std: 0.0669, min: -0.5368, max: 0.5670
Raw mel spectrogram stats - mean: 1.6670, std: 10.4494, min: 0.0000, max: 337.8802
Log mel spectrogram stats - mean: -4.7935, std: 3.4394, min: -12.3135, max: 5.8227
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1864, max: 3.0866
Mel spec shape: torch.Size([1, 80, 319])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1855, max: 3.0859
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NOT EXACTLY RETURNED KALIKO
Prediction: 0000000000
Loss: 14.9971
outputs.loss tensor(14.9971, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0038.flac
Waveform stats - mean: 0.0000, std: 0.1176, min: -0.9550, max: 0.8860
Resampled waveform stats - mean: 0.0000, std: 0.1176, min: -0.9550, max: 0.8860
Raw mel spectrogram stats - mean: 5.1740, std: 39.0155, min: 0.0000, max: 1548.7380
Log mel spectrogram stats - mean: -5.2633, std: 4.5667, min: -13.6813, max: 7.3452
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8434, max: 2.7610
Mel spec shape: torch.Size([1, 80, 287])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8438, max: 2.7617
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HAS THE STRANGEST FITS AT TIMES
Prediction: 0000000000
Loss: 16.0484
outputs.loss tensor(16.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0021.flac
Waveform stats - mean: -0.0000, std: 0.0740, min: -0.4244, max: 0.4644
Resampled waveform stats - mean: -0.0000, std: 0.0740, min: -0.4244, max: 0.4644
Raw mel spectrogram stats - mean: 2.0431, std: 13.5077, min: 0.0000, max: 496.0974
Log mel spectrogram stats - mean: -8.2019, std: 5.4896, min: -13.8153, max: 6.2068
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.0226, max: 2.6247
Mel spec shape: torch.Size([1, 80, 334])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.0225, max: 2.6250
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU SEE JIM KNOWED THE LAW
Prediction: 00000000
Loss: 16.1080
outputs.loss tensor(16.1080, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143396/1673-143396-0009.flac
Waveform stats - mean: -0.0000, std: 0.1071, min: -0.7951, max: 0.8190
Resampled waveform stats - mean: -0.0000, std: 0.1071, min: -0.7951, max: 0.8190
Raw mel spectrogram stats - mean: 4.2970, std: 55.4655, min: 0.0000, max: 5334.8931
Log mel spectrogram stats - mean: -4.2744, std: 3.5673, min: -13.3058, max: 8.5820
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5317, max: 3.6039
Mel spec shape: torch.Size([1, 80, 1647])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5312, max: 3.6035
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 52736
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 52736
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 369152
Inf count: 0
audio_emb.shape torch.Size([1, 103, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE FIRST APPEARED ON THE BANKS OF THE JORDAN IN THE FORM OF PERFECT MANHOOD BUT IT WAS A FORM ONLY AND NOT A SUBSTANCE A HUMAN FIGURE CREATED BY THE HAND OF OMNIPOTENCE TO IMITATE THE FACULTIES AND ACTIONS OF A MAN AND TO IMPOSE A PERPETUAL ILLUSION ON THE SENSES OF HIS FRIENDS AND ENEMIES
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.8977
outputs.loss tensor(15.8977, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153948/2412-153948-0002.flac
Waveform stats - mean: -0.0001, std: 0.0436, min: -0.4128, max: 0.3097
Resampled waveform stats - mean: -0.0001, std: 0.0436, min: -0.4128, max: 0.3097
Raw mel spectrogram stats - mean: 0.7075, std: 5.5682, min: 0.0000, max: 219.1663
Log mel spectrogram stats - mean: -6.6865, std: 4.1732, min: -13.7155, max: 5.3898
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6843, max: 2.8938
Mel spec shape: torch.Size([1, 80, 373])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6846, max: 2.8945
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NO ONE WHO IS HIMSELF HONEST WILL DOUBT MY BEING SO
Prediction: 00000000000000000
Loss: 15.8934
outputs.loss tensor(15.8934, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/148538/1988-148538-0011.flac
Waveform stats - mean: -0.0000, std: 0.0307, min: -0.2767, max: 0.3742
Resampled waveform stats - mean: -0.0000, std: 0.0307, min: -0.2767, max: 0.3742
Raw mel spectrogram stats - mean: 0.3533, std: 3.2323, min: 0.0000, max: 348.0035
Log mel spectrogram stats - mean: -6.6984, std: 3.9081, min: -13.8148, max: 5.8522
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8209, max: 3.2114
Mel spec shape: torch.Size([1, 80, 956])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8213, max: 3.2109
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30720
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30720
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 215040
Inf count: 0
audio_emb.shape torch.Size([1, 60, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN ARISTOCRATIC COUNTRIES THE GREAT POSSESS IMMENSE PRIVILEGES UPON WHICH THEIR PRIDE RESTS WITHOUT SEEKING TO RELY UPON THE LESSER ADVANTAGES WHICH ACCRUE TO THEM
Prediction: 0000000000000000000000000000000000000000000000
Loss: 16.7487
outputs.loss tensor(16.7487, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/64257/6345-64257-0019.flac
Waveform stats - mean: 0.0000, std: 0.0759, min: -0.3827, max: 0.6172
Resampled waveform stats - mean: 0.0000, std: 0.0759, min: -0.3827, max: 0.6172
Raw mel spectrogram stats - mean: 2.1579, std: 17.9121, min: 0.0000, max: 1054.8927
Log mel spectrogram stats - mean: -4.8817, std: 3.5480, min: -13.6967, max: 6.9612
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4845, max: 3.3379
Mel spec shape: torch.Size([1, 80, 773])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4844, max: 3.3379
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25088
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25088
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 175616
Inf count: 0
audio_emb.shape torch.Size([1, 49, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THUS WAS SHE BORNE AWAY CAPTIVE OF HER DEAD NEITHER WILLING NOR UNWILLING OF LIFE AND DEATH EQUALLY CARELESS
Prediction: 0000000000000000000000000000000000
Loss: 16.4779
outputs.loss tensor(16.4779, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0031.flac
Waveform stats - mean: -0.0000, std: 0.0703, min: -0.5044, max: 0.6293
Resampled waveform stats - mean: -0.0000, std: 0.0703, min: -0.5044, max: 0.6293
Raw mel spectrogram stats - mean: 1.8442, std: 11.4009, min: 0.0000, max: 387.2627
Log mel spectrogram stats - mean: -6.0630, std: 4.9512, min: -13.7952, max: 5.9591
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5617, max: 2.4281
Mel spec shape: torch.Size([1, 80, 332])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5615, max: 2.4277
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BETTER RING AGAIN SUGGESTED THE DRIVER HARD
Prediction: 0000000000000
Loss: 16.4164
outputs.loss tensor(16.4164, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0015.flac
Waveform stats - mean: 0.0009, std: 0.0191, min: -0.2130, max: 0.1644
Resampled waveform stats - mean: 0.0009, std: 0.0191, min: -0.2130, max: 0.1644
Raw mel spectrogram stats - mean: 0.1239, std: 1.5190, min: 0.0000, max: 164.0967
Log mel spectrogram stats - mean: -6.4547, std: 3.2534, min: -13.4090, max: 5.1005
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1376, max: 3.5518
Mel spec shape: torch.Size([1, 80, 273])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1367, max: 3.5527
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I AM LOOKING AT HIM NOW
Prediction: 0000000
Loss: 16.7892
outputs.loss tensor(16.7892, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0004.flac
Waveform stats - mean: -0.0000, std: 0.0821, min: -0.3886, max: 0.6823
Resampled waveform stats - mean: -0.0000, std: 0.0821, min: -0.3886, max: 0.6823
Raw mel spectrogram stats - mean: 2.5282, std: 12.4044, min: 0.0000, max: 313.5810
Log mel spectrogram stats - mean: -5.7605, std: 4.8549, min: -13.8141, max: 5.7481
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6589, max: 2.3705
Mel spec shape: torch.Size([1, 80, 666])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6592, max: 2.3711
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE IS STAYING AT THIS HOTEL TO TRY THE AIR OF SYDENHAM AND HE FINDS THAT IT AGREES WITH HIM
Prediction: 000000000000000000000000000
Loss: 16.2265
outputs.loss tensor(16.2265, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/64257/6345-64257-0017.flac
Waveform stats - mean: -0.0000, std: 0.0842, min: -0.4042, max: 0.4152
Resampled waveform stats - mean: -0.0000, std: 0.0842, min: -0.4042, max: 0.4152
Raw mel spectrogram stats - mean: 2.6535, std: 23.0053, min: 0.0000, max: 679.3542
Log mel spectrogram stats - mean: -4.8123, std: 3.3895, min: -13.6167, max: 6.5211
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5976, max: 3.3437
Mel spec shape: torch.Size([1, 80, 284])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5977, max: 3.3438
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HER ONLY LIFE WAS THAT SHE WAS LOST
Prediction: 000000000
Loss: 16.7715
outputs.loss tensor(16.7715, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302201/8842-302201-0007.flac
Waveform stats - mean: 0.0000, std: 0.0800, min: -0.7501, max: 0.8300
Resampled waveform stats - mean: 0.0000, std: 0.0800, min: -0.7501, max: 0.8300
Raw mel spectrogram stats - mean: 2.3956, std: 18.6201, min: 0.0000, max: 904.6334
Log mel spectrogram stats - mean: -5.4794, std: 4.3293, min: -13.7947, max: 6.8075
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9207, max: 2.8381
Mel spec shape: torch.Size([1, 80, 486])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9209, max: 2.8379
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TO HER I WEND ALONG IN WHOSE MUCH STRENGTH MY WEAKNESS IS MADE STRONG
Prediction: 0000000000000000000000
Loss: 16.3348
outputs.loss tensor(16.3348, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0018.flac
Waveform stats - mean: -0.0007, std: 0.0711, min: -0.2867, max: 0.4152
Resampled waveform stats - mean: -0.0007, std: 0.0711, min: -0.2867, max: 0.4152
Raw mel spectrogram stats - mean: 1.8909, std: 13.5619, min: 0.0000, max: 332.5027
Log mel spectrogram stats - mean: -6.3031, std: 3.9037, min: -13.7557, max: 5.8066
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9091, max: 3.1021
Mel spec shape: torch.Size([1, 80, 327])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9092, max: 3.1016
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE IS THIS DECEPTION BETWEEN ME AND EVERYTHING
Prediction: 00000000000
Loss: 16.1751
outputs.loss tensor(16.1751, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0016.flac
Waveform stats - mean: -0.0000, std: 0.0753, min: -0.4137, max: 0.5650
Resampled waveform stats - mean: -0.0000, std: 0.0753, min: -0.4137, max: 0.5650
Raw mel spectrogram stats - mean: 2.1209, std: 11.4328, min: 0.0000, max: 413.9249
Log mel spectrogram stats - mean: -5.4561, std: 4.7681, min: -13.8125, max: 6.0257
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7526, max: 2.4081
Mel spec shape: torch.Size([1, 80, 678])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7529, max: 2.4082
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND THE ONLY TRUCK WE HAD AVAILABLE WAS IN THAT BURNING SHED THE SUPERINTENDENT ADDED BITTERLY
Prediction: 000000000000000000000000000
Loss: 16.0133
outputs.loss tensor(16.0133, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
