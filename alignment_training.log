Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0014.flac
Waveform stats - mean: -0.0000, std: 0.0715, min: -0.3002, max: 0.6292
Resampled waveform stats - mean: -0.0000, std: 0.0715, min: -0.3002, max: 0.6292
Raw mel spectrogram stats - mean: 1.9178, std: 9.5006, min: 0.0000, max: 365.0411
Log mel spectrogram stats - mean: -6.1885, std: 4.8252, min: -13.8133, max: 5.9000
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5802, max: 2.5053
Mel spec shape: torch.Size([1, 80, 1809])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5801, max: 2.5059
CNN output shape: torch.Size([1, 512, 114])
CNN output stats - mean: 0.2820, std: 0.5884, min: -0.1699, max: 5.0469
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.3789, max: 3.3926
Final output stats - mean: 0.0058, std: 0.1993, min: -0.7607, max: 0.7402
audio_emb.shape torch.Size([1, 114, 3584])
input_embeds.shape torch.Size([1, 169, 3584])
labels.shape torch.Size([1, 169])
outputs.logits.shape torch.Size([1, 169, 152064])

Sample prediction:
Target: YOU DON'T KNOW WHAT IT IS TO BE USED TO SEEING A PRETTY CREATURE ALWAYS NICELY DRESSED ALWAYS ABOUT THE ROOM THINKING SO MUCH OF YOU AND SO LITTLE OF HERSELF AND THEN TO BE LEFT ALONE AS I AM LEFT OUT IN THE DARK
Prediction: ll the1ll111111ll1111ll11111IIll11llll113111II1ll101111101ll111111ll1101111llll111ll1111ll1111111111ll3ll11ll11ll1ll1101ll1ll1y1ll110ll1111111
'T KNOW WHAT YOU ME, BE A FORGET THE THE MANTTY GIRATURE LIKE SMELY DRESSED AND LOOK THE HOUSE.ING ABOUT MUCH ABOUT YOU. HOW MUCHITTLE OF HERSELF. YOU YOU SEE AB WITHONE WITH IF AM NOW NOW HERE THE C WITH
Loss: 9.2540
outputs.loss tensor(9.2540, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0019.flac
Waveform stats - mean: 0.0000, std: 0.0663, min: -0.3324, max: 0.4274
Resampled waveform stats - mean: 0.0000, std: 0.0663, min: -0.3324, max: 0.4274
Raw mel spectrogram stats - mean: 1.6474, std: 9.7947, min: 0.0000, max: 480.5544
Log mel spectrogram stats - mean: -5.9911, std: 4.3404, min: -13.8145, max: 6.1749
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8025, max: 2.8030
Mel spec shape: torch.Size([1, 80, 1148])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8027, max: 2.8027
CNN output shape: torch.Size([1, 512, 72])
CNN output stats - mean: 0.2815, std: 0.5884, min: -0.1699, max: 4.2656
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.4023, max: 3.5039
Final output stats - mean: 0.0056, std: 0.1984, min: -0.7676, max: 0.7358
audio_emb.shape torch.Size([1, 72, 3584])
input_embeds.shape torch.Size([1, 116, 3584])
labels.shape torch.Size([1, 116])
outputs.logits.shape torch.Size([1, 116, 152064])

Sample prediction:
Target: THERE THEY STROVE TO BREAK THE SILVER CORD OF LIFE AND THRUST MY SOUL INTO THE BODY OF A FOUL NIGHT WEIRD THEIR SORCERY SUMMONED UP FROM HELL AH
Prediction: ll111lllly11111ll1ll1111ll11ll01y100111100111111111011111101ll111ll11111111111ll1llal IS AREANDVE TO MAKE THE LAWVER BAGES THE. TOUST THEIR SOUL OUT THE H OF A BEE,MATHERNESSSONGROWERY ISMONED THE THE THEELL TO!
Loss: 11.1762
outputs.loss tensor(11.1762, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0009.flac
Waveform stats - mean: -0.0000, std: 0.0622, min: -0.4271, max: 0.2989
Resampled waveform stats - mean: -0.0000, std: 0.0622, min: -0.4271, max: 0.2989
Raw mel spectrogram stats - mean: 1.3535, std: 8.8445, min: 0.0000, max: 657.6238
Log mel spectrogram stats - mean: -6.2554, std: 4.5305, min: -13.7745, max: 6.4886
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6597, max: 2.8129
Mel spec shape: torch.Size([1, 80, 301])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6592, max: 2.8125
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: 0.2864, std: 0.5815, min: -0.1699, max: 3.5508
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.4141, max: 3.4570
Final output stats - mean: 0.0059, std: 0.1973, min: -0.7505, max: 0.6938
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 30, 3584])
labels.shape torch.Size([1, 30])
outputs.logits.shape torch.Size([1, 30, 152064])

Sample prediction:
Target: HE WAS IN A MOOD FOR MUSIC WAS HE NOT
Prediction: llllllllII111IIll1ll1II11llllllALTH A THE HOD TO FUN.N IN?

Loss: 9.2503
outputs.loss tensor(9.2503, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0036.flac
Waveform stats - mean: 0.0001, std: 0.0940, min: -0.8556, max: 0.6372
Resampled waveform stats - mean: 0.0001, std: 0.0940, min: -0.8556, max: 0.6372
Raw mel spectrogram stats - mean: 2.8778, std: 22.6069, min: 0.0000, max: 3160.1025
Log mel spectrogram stats - mean: -5.1284, std: 4.8352, min: -13.8025, max: 8.0584
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7940, max: 2.7272
Mel spec shape: torch.Size([1, 80, 722])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7939, max: 2.7266
CNN output shape: torch.Size([1, 512, 46])
CNN output stats - mean: 0.2815, std: 0.5898, min: -0.1699, max: 3.9863
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.5312, max: 3.5586
Final output stats - mean: 0.0060, std: 0.1975, min: -0.7339, max: 0.7441
audio_emb.shape torch.Size([1, 46, 3584])
input_embeds.shape torch.Size([1, 80, 3584])
labels.shape torch.Size([1, 80])
outputs.logits.shape torch.Size([1, 80, 152064])

Sample prediction:
Target: HE DID NOT SAY THE LAST AS A BOAST BUT MERELY AS AN ASSURANCE TO THE LIVERYMAN WHO HE SAW WAS ANXIOUS ON HIS ACCOUNT
Prediction: llll11IIll
ll11111111llllllII11II1111IIllllII1IIll11ll111111ll11ALTHN KNOW A WORD WORDK JY, ASELY AS A OBSURANCE THAT HIM PEOPLEAD MEN THAT WAS THPO WAIT A HIOUS TO ACCOUNT ACCOUNT.
Loss: 10.5572
outputs.loss tensor(10.5572, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0035.flac
Waveform stats - mean: -0.0000, std: 0.0670, min: -0.4291, max: 0.6053
Resampled waveform stats - mean: -0.0000, std: 0.0670, min: -0.4291, max: 0.6053
Raw mel spectrogram stats - mean: 1.6792, std: 12.2351, min: 0.0000, max: 1255.1699
Log mel spectrogram stats - mean: -6.8066, std: 4.9682, min: -13.8153, max: 7.1350
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4107, max: 2.8062
Mel spec shape: torch.Size([1, 80, 1969])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4111, max: 2.8066
CNN output shape: torch.Size([1, 512, 124])
CNN output stats - mean: 0.2817, std: 0.5879, min: -0.1699, max: 4.2344
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.4629, max: 3.3906
Final output stats - mean: 0.0060, std: 0.1993, min: -0.7437, max: 0.7671
audio_emb.shape torch.Size([1, 124, 3584])
input_embeds.shape torch.Size([1, 200, 3584])
labels.shape torch.Size([1, 200])
outputs.logits.shape torch.Size([1, 200, 152064])

Sample prediction:
Target: SHOULD THE VOLUME OF THE STREAM WHERE YOU STRIKE IT SEEM SMALL THEN YOU WILL KNOW THAT YOU ARE ABOVE THE SPRING IF LARGE NEARLY EQUAL TO ITS VOLUME AT ITS CONFLUENCE WITH THE PITT RIVER THEN YOU ARE BELOW IT AND IN EITHER CASE HAVE ONLY TO FOLLOW THE RIVER UP OR DOWN UNTIL YOU COME TO IT
Prediction: llllll1ll11II11111IIllll1311ll11ll1II1llll1113ll111111ll1111llll11011110ll110310111ll111101ll11llllll11111111II11ll111011111111ll11011IIll1111ll11ll1111
1R NOT UNITEDOTE OF A C BE THE AREAND THE OFFEMSED, YOU ARE BE THAT YOU ARE IN THE WATERING OF YOU ENARBY  TO SMALL VOLUME
 THE SOURCEFLUENCE WITH THE MAINONDIVERIVER. YOU ARE IN THE IF IF THE CASE YOU A A  THE STREAMIVER TOSTREAM DOWN TONTIL YOU FINDE TO THE'S
Loss: 10.1124
outputs.loss tensor(10.1124, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0032.flac
Waveform stats - mean: -0.0001, std: 0.0626, min: -0.5573, max: 0.6211
Resampled waveform stats - mean: -0.0001, std: 0.0626, min: -0.5573, max: 0.6211
Raw mel spectrogram stats - mean: 1.4675, std: 11.2692, min: 0.0000, max: 591.8090
Log mel spectrogram stats - mean: -6.8574, std: 4.8890, min: -13.7996, max: 6.3832
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4200, max: 2.7083
Mel spec shape: torch.Size([1, 80, 651])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4199, max: 2.7090
CNN output shape: torch.Size([1, 512, 41])
CNN output stats - mean: 0.2827, std: 0.5908, min: -0.1699, max: 3.8242
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.2832, max: 3.2461
Final output stats - mean: 0.0059, std: 0.2006, min: -0.7383, max: 0.7495
audio_emb.shape torch.Size([1, 41, 3584])
input_embeds.shape torch.Size([1, 62, 3584])
labels.shape torch.Size([1, 62])
outputs.logits.shape torch.Size([1, 62, 152064])

Sample prediction:
Target: BUT WHY ON THAT ACCOUNT THEY SHOULD PITY ME I ALTOGETHER FAIL TO UNDERSTAND
Prediction: 
ll西侧1llII110ll11111
111111llII1llll1llll11ll110ll11ll1
TER NOT E DAY, SHOULD BEUN US? DONREADYGETHER. TO UNDERSTAND THE
Loss: 10.0555
outputs.loss tensor(10.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0004.flac
Waveform stats - mean: -0.0001, std: 0.0567, min: -0.5422, max: 0.4881
Resampled waveform stats - mean: -0.0001, std: 0.0567, min: -0.5422, max: 0.4881
Raw mel spectrogram stats - mean: 1.2020, std: 6.2435, min: 0.0000, max: 210.8438
Log mel spectrogram stats - mean: -4.6835, std: 3.8201, min: -13.4405, max: 5.3511
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2924, max: 2.6268
Mel spec shape: torch.Size([1, 80, 552])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2930, max: 2.6270
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: 0.2834, std: 0.5879, min: -0.1699, max: 3.4414
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.4297, max: 3.4688
Final output stats - mean: 0.0060, std: 0.1957, min: -0.6548, max: 0.7080
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 65, 3584])
labels.shape torch.Size([1, 65])
outputs.logits.shape torch.Size([1, 65, 152064])

Sample prediction:
Target: WE TOOK OUR WAY THROUGH POOR AND SPARSE MEADOWS WHICH MADE A DESPERATE EFFORT EVERY YEAR TO SHOW A LITTLE GREEN
Prediction: 1
 pérdida all11 the11111111011y11ll011111111111ATHEROK A FIRST TO THEVERT C RENTLYADOWS, W THE STRERTATELYORT TO DAY TO G US LITTLE GREEN.
Loss: 11.3768
outputs.loss tensor(11.3768, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0000.flac
Waveform stats - mean: -0.0007, std: 0.0787, min: -0.4717, max: 0.5628
Resampled waveform stats - mean: -0.0007, std: 0.0787, min: -0.4717, max: 0.5628
Raw mel spectrogram stats - mean: 2.3209, std: 13.4589, min: 0.0000, max: 379.8323
Log mel spectrogram stats - mean: -5.6418, std: 4.1901, min: -13.7768, max: 5.9397
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9415, max: 2.7640
Mel spec shape: torch.Size([1, 80, 472])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9414, max: 2.7637
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: 0.2825, std: 0.5898, min: -0.1699, max: 4.2500
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.3379, max: 3.3145
Final output stats - mean: 0.0058, std: 0.1997, min: -0.7373, max: 0.7671
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 48, 3584])
labels.shape torch.Size([1, 48])
outputs.logits.shape torch.Size([1, 48, 152064])

Sample prediction:
Target: THE LAST TWO DAYS OF THE VOYAGE BARTLEY FOUND ALMOST INTOLERABLE
Prediction: ll1ll1 allllll11ll
1ll1ll1llll1llll1ll111111Y OF YEARS OF THE WORLDYAGE OFeganHO: THEONE THEOLERABLE.
Loss: 10.3368
outputs.loss tensor(10.3368, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=0.0001, std=0.0085
cnn_layers.0.bias: mean=0.0000, std=0.0000
cnn_layers.1.weight: mean=-0.0000, std=0.0036
cnn_layers.1.bias: mean=-0.0001, std=0.0026
cnn_layers.3.weight: mean=-0.0000, std=0.0055
cnn_layers.3.bias: mean=0.0000, std=0.0000
cnn_layers.4.weight: mean=0.0000, std=0.0028
cnn_layers.4.bias: mean=-0.0001, std=0.0021
cnn_layers.6.weight: mean=0.0000, std=0.0045
cnn_layers.6.bias: mean=-0.0000, std=0.0000
cnn_layers.7.weight: mean=-0.0000, std=0.0024
cnn_layers.7.bias: mean=-0.0001, std=0.0018
cnn_layers.9.weight: mean=0.0000, std=0.0039
cnn_layers.9.bias: mean=0.0000, std=0.0000
cnn_layers.10.weight: mean=0.0001, std=0.0161
cnn_layers.10.bias: mean=0.0001, std=0.0191
transformer.layers.0.self_attn.in_proj_weight: mean=-0.0001, std=0.0026
transformer.layers.0.self_attn.in_proj_bias: mean=-0.0003, std=0.0091
transformer.layers.0.self_attn.out_proj.weight: mean=0.0000, std=0.0059
transformer.layers.0.self_attn.out_proj.bias: mean=-0.0000, std=0.0283
transformer.layers.0.linear1.weight: mean=0.0000, std=0.0007
transformer.layers.0.linear1.bias: mean=-0.0000, std=0.0022
transformer.layers.0.linear2.weight: mean=0.0000, std=0.0021
transformer.layers.0.linear2.bias: mean=0.0001, std=0.0154
transformer.layers.0.norm1.weight: mean=-0.0000, std=0.0037
transformer.layers.0.norm1.bias: mean=0.0000, std=0.0165
transformer.layers.0.norm2.weight: mean=0.0000, std=0.0036
transformer.layers.0.norm2.bias: mean=0.0003, std=0.0158
transformer.layers.1.self_attn.in_proj_weight: mean=-0.0000, std=0.0009
transformer.layers.1.self_attn.in_proj_bias: mean=-0.0002, std=0.0038
transformer.layers.1.self_attn.out_proj.weight: mean=-0.0000, std=0.0018
transformer.layers.1.self_attn.out_proj.bias: mean=-0.0000, std=0.0118
transformer.layers.1.linear1.weight: mean=0.0000, std=0.0005
transformer.layers.1.linear1.bias: mean=-0.0000, std=0.0016
transformer.layers.1.linear2.weight: mean=-0.0000, std=0.0016
transformer.layers.1.linear2.bias: mean=-0.0000, std=0.0110
transformer.layers.1.norm1.weight: mean=-0.0000, std=0.0030
transformer.layers.1.norm1.bias: mean=0.0000, std=0.0118
transformer.layers.1.norm2.weight: mean=-0.0000, std=0.0031
transformer.layers.1.norm2.bias: mean=0.0002, std=0.0117
transformer.layers.2.self_attn.in_proj_weight: mean=0.0000, std=0.0008
transformer.layers.2.self_attn.in_proj_bias: mean=-0.0001, std=0.0028
transformer.layers.2.self_attn.out_proj.weight: mean=0.0000, std=0.0016
transformer.layers.2.self_attn.out_proj.bias: mean=0.0000, std=0.0087
transformer.layers.2.linear1.weight: mean=-0.0000, std=0.0005
transformer.layers.2.linear1.bias: mean=-0.0000, std=0.0012
transformer.layers.2.linear2.weight: mean=-0.0000, std=0.0013
transformer.layers.2.linear2.bias: mean=-0.0000, std=0.0082
transformer.layers.2.norm1.weight: mean=-0.0000, std=0.0028
transformer.layers.2.norm1.bias: mean=0.0000, std=0.0088
transformer.layers.2.norm2.weight: mean=0.0000, std=0.0030
transformer.layers.2.norm2.bias: mean=0.0001, std=0.0090
transformer.layers.3.self_attn.in_proj_weight: mean=0.0000, std=0.0007
transformer.layers.3.self_attn.in_proj_bias: mean=-0.0001, std=0.0022
transformer.layers.3.self_attn.out_proj.weight: mean=0.0000, std=0.0015
transformer.layers.3.self_attn.out_proj.bias: mean=-0.0000, std=0.0065
transformer.layers.3.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.3.linear1.bias: mean=-0.0000, std=0.0010
transformer.layers.3.linear2.weight: mean=0.0000, std=0.0011
transformer.layers.3.linear2.bias: mean=0.0000, std=0.0064
transformer.layers.3.norm1.weight: mean=-0.0000, std=0.0027
transformer.layers.3.norm1.bias: mean=0.0000, std=0.0068
transformer.layers.3.norm2.weight: mean=-0.0000, std=0.0028
transformer.layers.3.norm2.bias: mean=0.0001, std=0.0071
transformer.layers.4.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.4.self_attn.in_proj_bias: mean=-0.0001, std=0.0017
transformer.layers.4.self_attn.out_proj.weight: mean=-0.0000, std=0.0015
transformer.layers.4.self_attn.out_proj.bias: mean=-0.0000, std=0.0052
transformer.layers.4.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.4.linear1.bias: mean=-0.0000, std=0.0008
transformer.layers.4.linear2.weight: mean=-0.0000, std=0.0010
transformer.layers.4.linear2.bias: mean=-0.0000, std=0.0050
transformer.layers.4.norm1.weight: mean=-0.0000, std=0.0026
transformer.layers.4.norm1.bias: mean=0.0000, std=0.0054
transformer.layers.4.norm2.weight: mean=0.0000, std=0.0027
transformer.layers.4.norm2.bias: mean=0.0001, std=0.0057
transformer.layers.5.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.5.self_attn.in_proj_bias: mean=-0.0001, std=0.0014
transformer.layers.5.self_attn.out_proj.weight: mean=-0.0000, std=0.0014
transformer.layers.5.self_attn.out_proj.bias: mean=-0.0000, std=0.0041
transformer.layers.5.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.5.linear1.bias: mean=-0.0000, std=0.0007
transformer.layers.5.linear2.weight: mean=0.0000, std=0.0009
transformer.layers.5.linear2.bias: mean=0.0000, std=0.0041
transformer.layers.5.norm1.weight: mean=-0.0000, std=0.0026
transformer.layers.5.norm1.bias: mean=0.0000, std=0.0045
transformer.layers.5.norm2.weight: mean=0.0000, std=0.0026
transformer.layers.5.norm2.bias: mean=0.0001, std=0.0047
transformer.layers.6.self_attn.in_proj_weight: mean=0.0000, std=0.0007
transformer.layers.6.self_attn.in_proj_bias: mean=-0.0001, std=0.0012
transformer.layers.6.self_attn.out_proj.weight: mean=0.0000, std=0.0014
transformer.layers.6.self_attn.out_proj.bias: mean=0.0000, std=0.0034
transformer.layers.6.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.6.linear1.bias: mean=-0.0000, std=0.0006
transformer.layers.6.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.6.linear2.bias: mean=-0.0000, std=0.0035
transformer.layers.6.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.6.norm1.bias: mean=0.0000, std=0.0038
transformer.layers.6.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.6.norm2.bias: mean=0.0000, std=0.0039
transformer.layers.7.self_attn.in_proj_weight: mean=0.0000, std=0.0007
transformer.layers.7.self_attn.in_proj_bias: mean=-0.0000, std=0.0010
transformer.layers.7.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.7.self_attn.out_proj.bias: mean=0.0000, std=0.0029
transformer.layers.7.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.7.linear1.bias: mean=-0.0000, std=0.0005
transformer.layers.7.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.7.linear2.bias: mean=-0.0000, std=0.0031
transformer.layers.7.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.7.norm1.bias: mean=0.0000, std=0.0033
transformer.layers.7.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.7.norm2.bias: mean=0.0000, std=0.0034
transformer.layers.8.self_attn.in_proj_weight: mean=-0.0000, std=0.0006
transformer.layers.8.self_attn.in_proj_bias: mean=-0.0000, std=0.0009
transformer.layers.8.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.8.self_attn.out_proj.bias: mean=-0.0000, std=0.0025
transformer.layers.8.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.8.linear1.bias: mean=-0.0000, std=0.0005
transformer.layers.8.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.8.linear2.bias: mean=-0.0000, std=0.0028
transformer.layers.8.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.8.norm1.bias: mean=0.0000, std=0.0030
transformer.layers.8.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.8.norm2.bias: mean=0.0000, std=0.0031
transformer.layers.9.self_attn.in_proj_weight: mean=0.0000, std=0.0007
transformer.layers.9.self_attn.in_proj_bias: mean=-0.0000, std=0.0008
transformer.layers.9.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.9.self_attn.out_proj.bias: mean=-0.0000, std=0.0023
transformer.layers.9.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.9.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.9.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.9.linear2.bias: mean=-0.0000, std=0.0025
transformer.layers.9.norm1.weight: mean=-0.0000, std=0.0024
transformer.layers.9.norm1.bias: mean=0.0000, std=0.0028
transformer.layers.9.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.9.norm2.bias: mean=0.0000, std=0.0029
transformer.layers.10.self_attn.in_proj_weight: mean=0.0000, std=0.0006
transformer.layers.10.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.10.self_attn.out_proj.weight: mean=0.0000, std=0.0013
transformer.layers.10.self_attn.out_proj.bias: mean=0.0000, std=0.0021
transformer.layers.10.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.10.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.10.linear2.weight: mean=0.0000, std=0.0008
transformer.layers.10.linear2.bias: mean=0.0000, std=0.0024
transformer.layers.10.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.10.norm1.bias: mean=0.0000, std=0.0027
transformer.layers.10.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.10.norm2.bias: mean=-0.0000, std=0.0027
transformer.layers.11.self_attn.in_proj_weight: mean=-0.0000, std=0.0006
transformer.layers.11.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.11.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.11.self_attn.out_proj.bias: mean=-0.0000, std=0.0020
transformer.layers.11.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.11.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.11.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.11.linear2.bias: mean=-0.0000, std=0.0023
transformer.layers.11.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.11.norm1.bias: mean=0.0000, std=0.0026
transformer.layers.11.norm2.weight: mean=0.0000, std=0.0025
transformer.layers.11.norm2.bias: mean=-0.0000, std=0.0026
transformer.layers.12.self_attn.in_proj_weight: mean=-0.0000, std=0.0006
transformer.layers.12.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.12.self_attn.out_proj.weight: mean=0.0000, std=0.0013
transformer.layers.12.self_attn.out_proj.bias: mean=0.0000, std=0.0020
transformer.layers.12.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.12.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.12.linear2.weight: mean=-0.0000, std=0.0008
transformer.layers.12.linear2.bias: mean=-0.0000, std=0.0023
transformer.layers.12.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.12.norm1.bias: mean=0.0000, std=0.0026
transformer.layers.12.norm2.weight: mean=0.0000, std=0.0026
transformer.layers.12.norm2.bias: mean=-0.0000, std=0.0026
transformer.layers.13.self_attn.in_proj_weight: mean=-0.0000, std=0.0006
transformer.layers.13.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.13.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.13.self_attn.out_proj.bias: mean=-0.0000, std=0.0020
transformer.layers.13.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.13.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.13.linear2.weight: mean=0.0000, std=0.0008
transformer.layers.13.linear2.bias: mean=0.0000, std=0.0023
transformer.layers.13.norm1.weight: mean=-0.0000, std=0.0025
transformer.layers.13.norm1.bias: mean=0.0000, std=0.0026
transformer.layers.13.norm2.weight: mean=-0.0000, std=0.0026
transformer.layers.13.norm2.bias: mean=-0.0000, std=0.0026
transformer.layers.14.self_attn.in_proj_weight: mean=-0.0000, std=0.0006
transformer.layers.14.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.14.self_attn.out_proj.weight: mean=-0.0000, std=0.0013
transformer.layers.14.self_attn.out_proj.bias: mean=-0.0000, std=0.0020
transformer.layers.14.linear1.weight: mean=-0.0000, std=0.0004
transformer.layers.14.linear1.bias: mean=-0.0000, std=0.0004
transformer.layers.14.linear2.weight: mean=0.0000, std=0.0008
transformer.layers.14.linear2.bias: mean=0.0000, std=0.0024
transformer.layers.14.norm1.weight: mean=-0.0000, std=0.0026
transformer.layers.14.norm1.bias: mean=0.0000, std=0.0026
transformer.layers.14.norm2.weight: mean=-0.0000, std=0.0027
transformer.layers.14.norm2.bias: mean=-0.0000, std=0.0026
transformer.layers.15.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.15.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.15.self_attn.out_proj.weight: mean=0.0000, std=0.0014
transformer.layers.15.self_attn.out_proj.bias: mean=0.0000, std=0.0020
transformer.layers.15.linear1.weight: mean=0.0000, std=0.0004
transformer.layers.15.linear1.bias: mean=0.0000, std=0.0005
transformer.layers.15.linear2.weight: mean=0.0000, std=0.0009
transformer.layers.15.linear2.bias: mean=0.0000, std=0.0025
transformer.layers.15.norm1.weight: mean=-0.0000, std=0.0027
transformer.layers.15.norm1.bias: mean=0.0000, std=0.0027
transformer.layers.15.norm2.weight: mean=0.0000, std=0.0028
transformer.layers.15.norm2.bias: mean=-0.0000, std=0.0027
transformer.layers.16.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.16.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.16.self_attn.out_proj.weight: mean=-0.0000, std=0.0014
transformer.layers.16.self_attn.out_proj.bias: mean=-0.0000, std=0.0021
transformer.layers.16.linear1.weight: mean=0.0000, std=0.0005
transformer.layers.16.linear1.bias: mean=-0.0000, std=0.0005
transformer.layers.16.linear2.weight: mean=-0.0000, std=0.0009
transformer.layers.16.linear2.bias: mean=-0.0000, std=0.0026
transformer.layers.16.norm1.weight: mean=0.0000, std=0.0028
transformer.layers.16.norm1.bias: mean=0.0000, std=0.0028
transformer.layers.16.norm2.weight: mean=-0.0000, std=0.0029
transformer.layers.16.norm2.bias: mean=0.0000, std=0.0028
transformer.layers.17.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.17.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.17.self_attn.out_proj.weight: mean=0.0000, std=0.0015
transformer.layers.17.self_attn.out_proj.bias: mean=0.0000, std=0.0022
transformer.layers.17.linear1.weight: mean=0.0000, std=0.0005
transformer.layers.17.linear1.bias: mean=0.0000, std=0.0005
transformer.layers.17.linear2.weight: mean=-0.0000, std=0.0009
transformer.layers.17.linear2.bias: mean=-0.0000, std=0.0027
transformer.layers.17.norm1.weight: mean=-0.0000, std=0.0029
transformer.layers.17.norm1.bias: mean=0.0000, std=0.0029
transformer.layers.17.norm2.weight: mean=0.0000, std=0.0030
transformer.layers.17.norm2.bias: mean=0.0000, std=0.0030
transformer.layers.18.self_attn.in_proj_weight: mean=-0.0000, std=0.0007
transformer.layers.18.self_attn.in_proj_bias: mean=-0.0000, std=0.0007
transformer.layers.18.self_attn.out_proj.weight: mean=-0.0000, std=0.0016
transformer.layers.18.self_attn.out_proj.bias: mean=-0.0000, std=0.0023
transformer.layers.18.linear1.weight: mean=0.0000, std=0.0005
transformer.layers.18.linear1.bias: mean=0.0000, std=0.0005
transformer.layers.18.linear2.weight: mean=-0.0000, std=0.0010
transformer.layers.18.linear2.bias: mean=-0.0000, std=0.0028
transformer.layers.18.norm1.weight: mean=0.0000, std=0.0030
transformer.layers.18.norm1.bias: mean=0.0000, std=0.0030
transformer.layers.18.norm2.weight: mean=-0.0000, std=0.0031
transformer.layers.18.norm2.bias: mean=0.0000, std=0.0031
transformer.layers.19.self_attn.in_proj_weight: mean=0.0000, std=0.0008
transformer.layers.19.self_attn.in_proj_bias: mean=-0.0000, std=0.0008
transformer.layers.19.self_attn.out_proj.weight: mean=-0.0000, std=0.0017
transformer.layers.19.self_attn.out_proj.bias: mean=-0.0000, std=0.0025
transformer.layers.19.linear1.weight: mean=-0.0000, std=0.0006
transformer.layers.19.linear1.bias: mean=0.0000, std=0.0006
transformer.layers.19.linear2.weight: mean=0.0000, std=0.0011
transformer.layers.19.linear2.bias: mean=0.0000, std=0.0031
transformer.layers.19.norm1.weight: mean=0.0000, std=0.0032
transformer.layers.19.norm1.bias: mean=0.0001, std=0.0032
transformer.layers.19.norm2.weight: mean=-0.0000, std=0.0033
transformer.layers.19.norm2.bias: mean=0.0001, std=0.0033
transformer.layers.20.self_attn.in_proj_weight: mean=0.0000, std=0.0009
transformer.layers.20.self_attn.in_proj_bias: mean=-0.0000, std=0.0009
transformer.layers.20.self_attn.out_proj.weight: mean=0.0000, std=0.0018
transformer.layers.20.self_attn.out_proj.bias: mean=0.0000, std=0.0027
transformer.layers.20.linear1.weight: mean=-0.0000, std=0.0006
transformer.layers.20.linear1.bias: mean=0.0000, std=0.0006
transformer.layers.20.linear2.weight: mean=-0.0000, std=0.0012
transformer.layers.20.linear2.bias: mean=-0.0000, std=0.0034
transformer.layers.20.norm1.weight: mean=0.0000, std=0.0034
transformer.layers.20.norm1.bias: mean=0.0000, std=0.0034
transformer.layers.20.norm2.weight: mean=-0.0000, std=0.0036
transformer.layers.20.norm2.bias: mean=0.0001, std=0.0036
transformer.layers.21.self_attn.in_proj_weight: mean=0.0000, std=0.0010
transformer.layers.21.self_attn.in_proj_bias: mean=-0.0000, std=0.0010
transformer.layers.21.self_attn.out_proj.weight: mean=-0.0000, std=0.0020
transformer.layers.21.self_attn.out_proj.bias: mean=-0.0001, std=0.0030
transformer.layers.21.linear1.weight: mean=-0.0000, std=0.0007
transformer.layers.21.linear1.bias: mean=0.0000, std=0.0007
transformer.layers.21.linear2.weight: mean=-0.0000, std=0.0014
transformer.layers.21.linear2.bias: mean=-0.0000, std=0.0036
transformer.layers.21.norm1.weight: mean=0.0000, std=0.0037
transformer.layers.21.norm1.bias: mean=0.0001, std=0.0037
transformer.layers.21.norm2.weight: mean=-0.0000, std=0.0039
transformer.layers.21.norm2.bias: mean=0.0001, std=0.0039
transformer.layers.22.self_attn.in_proj_weight: mean=0.0000, std=0.0011
transformer.layers.22.self_attn.in_proj_bias: mean=-0.0000, std=0.0011
transformer.layers.22.self_attn.out_proj.weight: mean=-0.0000, std=0.0024
transformer.layers.22.self_attn.out_proj.bias: mean=-0.0000, std=0.0035
transformer.layers.22.linear1.weight: mean=-0.0000, std=0.0009
transformer.layers.22.linear1.bias: mean=0.0000, std=0.0009
transformer.layers.22.linear2.weight: mean=-0.0000, std=0.0018
transformer.layers.22.linear2.bias: mean=-0.0000, std=0.0046
transformer.layers.22.norm1.weight: mean=0.0000, std=0.0042
transformer.layers.22.norm1.bias: mean=0.0001, std=0.0044
transformer.layers.22.norm2.weight: mean=-0.0000, std=0.0044
transformer.layers.22.norm2.bias: mean=0.0001, std=0.0048
transformer.layers.23.self_attn.in_proj_weight: mean=-0.0000, std=0.0015
transformer.layers.23.self_attn.in_proj_bias: mean=-0.0001, std=0.0015
transformer.layers.23.self_attn.out_proj.weight: mean=0.0000, std=0.0033
transformer.layers.23.self_attn.out_proj.bias: mean=0.0001, std=0.0049
transformer.layers.23.linear1.weight: mean=0.0000, std=0.0013
transformer.layers.23.linear1.bias: mean=-0.0000, std=0.0013
transformer.layers.23.linear2.weight: mean=0.0000, std=0.0025
transformer.layers.23.linear2.bias: mean=0.0002, std=0.0065
transformer.layers.23.norm1.weight: mean=0.0000, std=0.0054
transformer.layers.23.norm1.bias: mean=0.0001, std=0.0059
transformer.layers.23.norm2.weight: mean=0.0009, std=0.0060
transformer.layers.23.norm2.bias: mean=-0.0004, std=0.0065
connector.0.weight: mean=0.0000, std=0.0079
connector.0.bias: mean=0.0005, std=0.0078
connector.2.weight: mean=-0.0000, std=0.0039
connector.2.bias: mean=-0.0004, std=0.0114
Gradients clipped from 1.0000 to 1.0
Gradient norm: 1.0000
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/111771/7850-111771-0005.flac
Waveform stats - mean: -0.0000, std: 0.0756, min: -0.5540, max: 0.4475
Resampled waveform stats - mean: -0.0000, std: 0.0756, min: -0.5540, max: 0.4475
Raw mel spectrogram stats - mean: 2.1388, std: 17.4944, min: 0.0000, max: 651.9890
Log mel spectrogram stats - mean: -6.3857, std: 4.6036, min: -13.8138, max: 6.4800
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6135, max: 2.7947
Mel spec shape: torch.Size([1, 80, 330])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6133, max: 2.7949
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 34, 3584])
labels.shape torch.Size([1, 34])
outputs.logits.shape torch.Size([1, 34, 152064])

Sample prediction:
Target: HE WAS NOW COMMANDER OF ALL THE FEDERAL FORCES
Prediction: 0000000000000000000000  ED OF THE THE LEERAL ARCES.
Loss: 8.9306
outputs.loss tensor(8.9306, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147964/1993-147964-0008.flac
Waveform stats - mean: -0.0001, std: 0.0686, min: -0.4978, max: 0.6435
Resampled waveform stats - mean: -0.0001, std: 0.0686, min: -0.4978, max: 0.6435
Raw mel spectrogram stats - mean: 1.7584, std: 9.6982, min: 0.0000, max: 310.9995
Log mel spectrogram stats - mean: -4.3741, std: 3.1756, min: -13.0242, max: 5.7398
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.7239, max: 3.1849
Mel spec shape: torch.Size([1, 80, 705])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.7246, max: 3.1855
CNN output shape: torch.Size([1, 512, 45])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
input_embeds.shape torch.Size([1, 82, 3584])
labels.shape torch.Size([1, 82])
outputs.logits.shape torch.Size([1, 82, 152064])

Sample prediction:
Target: BY THE TIME WE HAD PLACED THE COLD FRESH SMELLING LITTLE TREE IN A CORNER OF THE SITTING ROOM IT WAS ALREADY CHRISTMAS EVE
Prediction: 0000000000000000000000000000000000000000000000 WAY THE GOTAD ANNED 0ABLE STORAGERO WATEREL OF BEMON B IN THE POTNER OF THE ROOMITTING ROOM, WAS ALREADY GMAS..
Loss: 10.4769
outputs.loss tensor(10.4769, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0020.flac
Waveform stats - mean: -0.0000, std: 0.0617, min: -0.6069, max: 0.6334
Resampled waveform stats - mean: -0.0000, std: 0.0617, min: -0.6069, max: 0.6334
Raw mel spectrogram stats - mean: 1.4247, std: 11.4011, min: 0.0000, max: 669.8707
Log mel spectrogram stats - mean: -5.6583, std: 4.1168, min: -13.8118, max: 6.5071
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9805, max: 2.9551
Mel spec shape: torch.Size([1, 80, 1068])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9805, max: 2.9551
CNN output shape: torch.Size([1, 512, 67])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34304
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34304
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 240128
Inf count: 0
audio_emb.shape torch.Size([1, 67, 3584])
input_embeds.shape torch.Size([1, 115, 3584])
labels.shape torch.Size([1, 115])
outputs.logits.shape torch.Size([1, 115, 152064])

Sample prediction:
Target: THEREFORE I AM A MERE AGENT TO DIRECT THE LAWS WHICH ARE THE WILL OF THE PEOPLE AND AM ONLY A PUBLIC SERVANT OBLIGED CONSTANTLY TO GUARD THE WELFARE OF MY SUBJECTS
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000 NOT CUR EXAMPLEENT0 BE YOUR FWS OF GO TO LAW OF THE PEOPLE,0END A M OFFVANT ANDBLIGED TOLY TO PROARD THE INTERELFARE OF THE CS AND
Loss: 10.4639
outputs.loss tensor(10.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0040.flac
Waveform stats - mean: -0.0006, std: 0.0732, min: -0.3854, max: 0.5182
Resampled waveform stats - mean: -0.0006, std: 0.0732, min: -0.3854, max: 0.5182
Raw mel spectrogram stats - mean: 2.0023, std: 15.0471, min: 0.0000, max: 531.2612
Log mel spectrogram stats - mean: -6.3235, std: 3.9790, min: -13.6561, max: 6.2753
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8428, max: 3.1663
Mel spec shape: torch.Size([1, 80, 461])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8428, max: 3.1660
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 46, 3584])
labels.shape torch.Size([1, 46])
outputs.logits.shape torch.Size([1, 46, 152064])

Sample prediction:
Target: AND THEN YOU CAME BACK NOT CARING VERY MUCH BUT IT MADE NO DIFFERENCE
Prediction: 000000000000000000000000000000 0ANNOT TO TO SOING ABOUT MUCH ABOUT YOU WAS ME DIFFERENCE TO
Loss: 9.4456
outputs.loss tensor(9.4456, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0003.flac
Waveform stats - mean: -0.0001, std: 0.0324, min: -0.1971, max: 0.2061
Resampled waveform stats - mean: -0.0001, std: 0.0324, min: -0.1971, max: 0.2061
Raw mel spectrogram stats - mean: 0.3930, std: 3.8225, min: 0.0000, max: 169.9986
Log mel spectrogram stats - mean: -7.1014, std: 3.4848, min: -13.6169, max: 5.1358
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8697, max: 3.5116
Mel spec shape: torch.Size([1, 80, 270])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8701, max: 3.5117
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 26, 3584])
labels.shape torch.Size([1, 26])
outputs.logits.shape torch.Size([1, 26, 152064])

Sample prediction:
Target: WE LAY STILL AND DID NOT TALK
Prediction: 00000000000000000000  WE NOT GELL TO
Loss: 10.1980
outputs.loss tensor(10.1980, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0009.flac
Waveform stats - mean: 0.0000, std: 0.0672, min: -0.3361, max: 0.3327
Resampled waveform stats - mean: 0.0000, std: 0.0672, min: -0.3361, max: 0.3327
Raw mel spectrogram stats - mean: 1.6907, std: 9.7518, min: 0.0000, max: 296.7968
Log mel spectrogram stats - mean: -6.4879, std: 4.7037, min: -13.8145, max: 5.6930
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5576, max: 2.5897
Mel spec shape: torch.Size([1, 80, 896])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5576, max: 2.5898
CNN output shape: torch.Size([1, 512, 56])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28672
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28672
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 200704
Inf count: 0
audio_emb.shape torch.Size([1, 56, 3584])
input_embeds.shape torch.Size([1, 97, 3584])
labels.shape torch.Size([1, 97])
outputs.logits.shape torch.Size([1, 97, 152064])

Sample prediction:
Target: THE SLANT OF THE MOON PRESAGED EVIL FOR THE KING OF VENDHYA THE STARS ARE IN TURMOIL THE SERPENT IN THE HOUSE OF THE ELEPHANT
Prediction: 0000000000000000000000000000000000000000000000000000000000ANT THE SLON00 THEIL0 THE MO OF THEENAE.0AN AND0 THEROMOIL  MOPENT OF THE SK OF THE DRAGPHANT THE
Loss: 9.4638
outputs.loss tensor(9.4638, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34629/5895-34629-0001.flac
Waveform stats - mean: -0.0000, std: 0.0429, min: -0.3503, max: 0.4778
Resampled waveform stats - mean: -0.0000, std: 0.0429, min: -0.3503, max: 0.4778
Raw mel spectrogram stats - mean: 0.6884, std: 4.5364, min: 0.0000, max: 165.6542
Log mel spectrogram stats - mean: -5.4249, std: 3.8315, min: -13.7632, max: 5.1099
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1763, max: 2.7496
Mel spec shape: torch.Size([1, 80, 321])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1758, max: 2.7500
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 38, 3584])
labels.shape torch.Size([1, 38])
outputs.logits.shape torch.Size([1, 38, 152064])

Sample prediction:
Target: THE DOME OF SAINT PAUL'S WAS A DELIGHT TO URSUS
Prediction: 000000000000000000000000 THEINT PUL0 CH  IGHTFUL THE.SU0
Loss: 9.8845
outputs.loss tensor(9.8845, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0001.flac
Waveform stats - mean: -0.0000, std: 0.0422, min: -0.5595, max: 0.5039
Resampled waveform stats - mean: -0.0000, std: 0.0422, min: -0.5595, max: 0.5039
Raw mel spectrogram stats - mean: 0.6679, std: 6.0287, min: 0.0000, max: 404.9078
Log mel spectrogram stats - mean: -5.2880, std: 3.5262, min: -13.8117, max: 6.0037
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4172, max: 3.2022
Mel spec shape: torch.Size([1, 80, 1181])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.4180, max: 3.2031
CNN output shape: torch.Size([1, 512, 74])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
input_embeds.shape torch.Size([1, 138, 3584])
labels.shape torch.Size([1, 138])
outputs.logits.shape torch.Size([1, 138, 152064])

Sample prediction:
Target: WITH A SHOUT THE BOYS DASHED PELL MELL TO MEET THE PACK TRAIN AND FALLING IN BEHIND THE SLOW MOVING BURROS URGED THEM ON WITH DERISIVE SHOUTS AND SUNDRY RESOUNDING SLAPS ON THE ANIMALS FLANKS
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000 OT OF BY00ED OFFASTINGEL M THE D THE BO. 0ING DOWN THEHIND THE PACKACKLYING TRAINAGG.NTENT THE TO TO AANGEDIVE LAOUTS.0NYY OTHERPECTING THAPS. THE HEADKS BACKANKS.
Loss: 9.6972
outputs.loss tensor(9.6972, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0009.flac
Waveform stats - mean: 0.0000, std: 0.0897, min: -0.6823, max: 0.5706
Resampled waveform stats - mean: 0.0000, std: 0.0897, min: -0.6823, max: 0.5706
Raw mel spectrogram stats - mean: 2.8376, std: 15.0060, min: 0.0000, max: 928.3052
Log mel spectrogram stats - mean: -4.9745, std: 4.8307, min: -13.7956, max: 6.8334
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8261, max: 2.4443
Mel spec shape: torch.Size([1, 80, 827])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8262, max: 2.4453
CNN output shape: torch.Size([1, 512, 52])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26624
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26624
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 186368
Inf count: 0
audio_emb.shape torch.Size([1, 52, 3584])
input_embeds.shape torch.Size([1, 94, 3584])
labels.shape torch.Size([1, 94])
outputs.logits.shape torch.Size([1, 94, 152064])

Sample prediction:
Target: LINCOLN HAD CALLED FOR VOLUNTEERS TO PUT DOWN A REBELLION BUT HARRY HEARD EVERYWHERE IN CHARLESTON THAT THE CONFEDERACY WAS NOW SECURE
Prediction: 00000000000000000000000000000000000000000000000000000N00 LED  UN.ER TO HELP  THE0CENT0ION IN WASAD HARD FORTHING THAT THELESTON THAT THE REEDERATE WAS GO CALURING.
Loss: 10.3761
outputs.loss tensor(10.3761, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0010.flac
Waveform stats - mean: 0.0000, std: 0.0711, min: -0.4725, max: 0.5048
Resampled waveform stats - mean: 0.0000, std: 0.0711, min: -0.4725, max: 0.5048
Raw mel spectrogram stats - mean: 1.8894, std: 10.4289, min: 0.0000, max: 343.8516
Log mel spectrogram stats - mean: -5.4978, std: 4.4960, min: -13.8138, max: 5.8402
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8497, max: 2.5218
Mel spec shape: torch.Size([1, 80, 386])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8496, max: 2.5215
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
input_embeds.shape torch.Size([1, 40, 3584])
labels.shape torch.Size([1, 40])
outputs.logits.shape torch.Size([1, 40, 152064])

Sample prediction:
Target: HE DIVINED MY MOTIVE AND FIRED THE BALL MISSED ITS AIM
Prediction: 0000000000000000000000000000 ION TO MYRED MY0 ATSED THE TARGET AND
Loss: 11.1978
outputs.loss tensor(11.1978, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0031.flac
Waveform stats - mean: 0.0000, std: 0.0579, min: -0.4400, max: 0.3918
Resampled waveform stats - mean: 0.0000, std: 0.0579, min: -0.4400, max: 0.3918
Raw mel spectrogram stats - mean: 1.2564, std: 9.9364, min: 0.0000, max: 337.1015
Log mel spectrogram stats - mean: -5.4342, std: 3.9395, min: -13.8079, max: 5.8204
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1256, max: 2.8568
Mel spec shape: torch.Size([1, 80, 221])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1250, max: 2.8574
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
input_embeds.shape torch.Size([1, 25, 3584])
labels.shape torch.Size([1, 25])
outputs.logits.shape torch.Size([1, 25, 152064])

Sample prediction:
Target: WE HAD BETTER START THE DRIVE THIS MORNING
Prediction: 0000000000000000 TER NOT WITH   ISNING.
Loss: 10.6517
outputs.loss tensor(10.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147964/1993-147964-0001.flac
Waveform stats - mean: -0.0001, std: 0.0617, min: -0.3619, max: 0.3191
Resampled waveform stats - mean: -0.0001, std: 0.0617, min: -0.3619, max: 0.3191
Raw mel spectrogram stats - mean: 1.4259, std: 10.5718, min: 0.0000, max: 286.3548
Log mel spectrogram stats - mean: -4.8817, std: 3.3390, min: -13.3799, max: 5.6572
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5451, max: 3.1563
Mel spec shape: torch.Size([1, 80, 476])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5449, max: 3.1562
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 50, 3584])
labels.shape torch.Size([1, 50])
outputs.logits.shape torch.Size([1, 50, 152064])

Sample prediction:
Target: ANYWAY HE WOULD NEVER ALLOW ONE OF HIS HORSES TO BE PUT TO SUCH A STRAIN
Prediction: 00000000000000000000000000000000AROULD BE BE YOU TO THEM OWNSES TO BE  IN THE A USEANGE THAT
Loss: 9.4702
outputs.loss tensor(9.4702, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0004.flac
Waveform stats - mean: -0.0000, std: 0.0437, min: -0.4421, max: 0.4626
Resampled waveform stats - mean: -0.0000, std: 0.0437, min: -0.4421, max: 0.4626
Raw mel spectrogram stats - mean: 0.7152, std: 5.4027, min: 0.0000, max: 301.6186
Log mel spectrogram stats - mean: -5.5094, std: 3.7031, min: -13.8113, max: 5.7092
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2419, max: 3.0295
Mel spec shape: torch.Size([1, 80, 557])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.0293
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 63, 3584])
labels.shape torch.Size([1, 63])
outputs.logits.shape torch.Size([1, 63, 152064])

Sample prediction:
Target: THE PONY DID MOST OF IT ADMITTED THE LAD I JUST GAVE HIM HIS HEAD AND THAT'S ALL THERE WAS TO IT
Prediction: 00000000000000000000000000000000000000 NOTLY THESELFITTEDLY0ATESTY DID SAVE YOU A MED ON HE WAS WHY I IS TO IT.
Loss: 9.0978
outputs.loss tensor(9.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0023.flac
Waveform stats - mean: 0.0138, std: 0.0551, min: -0.4210, max: 0.4331
Resampled waveform stats - mean: 0.0138, std: 0.0551, min: -0.4210, max: 0.4331
Raw mel spectrogram stats - mean: 1.1564, std: 8.5747, min: 0.0000, max: 308.7828
Log mel spectrogram stats - mean: -5.9771, std: 4.2609, min: -13.6873, max: 5.7326
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8095, max: 2.7482
Mel spec shape: torch.Size([1, 80, 415])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8096, max: 2.7480
CNN output shape: torch.Size([1, 512, 26])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
input_embeds.shape torch.Size([1, 44, 3584])
labels.shape torch.Size([1, 44])
outputs.logits.shape torch.Size([1, 44, 152064])

Sample prediction:
Target: ANTONIA POINTED UP TO THE SKY AND QUESTIONED ME WITH HER GLANCE
Prediction: 00000000000000000000000000000S OUT0  Y00 MARK THE. A EASS.
Loss: 9.1246
outputs.loss tensor(9.1246, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0036.flac
Waveform stats - mean: 0.0015, std: 0.2397, min: -0.8414, max: 0.8487
Resampled waveform stats - mean: 0.0015, std: 0.2397, min: -0.8414, max: 0.8487
Raw mel spectrogram stats - mean: 21.5378, std: 159.0368, min: 0.0000, max: 7179.9922
Log mel spectrogram stats - mean: -1.9169, std: 3.2817, min: -12.9995, max: 8.8791
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -3.3771, max: 3.2898
Mel spec shape: torch.Size([1, 80, 2413])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -3.3770, max: 3.2891
CNN output shape: torch.Size([1, 512, 151])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 77312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 77312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 541184
Inf count: 0
audio_emb.shape torch.Size([1, 151, 3584])
input_embeds.shape torch.Size([1, 267, 3584])
labels.shape torch.Size([1, 267])
outputs.logits.shape torch.Size([1, 267, 152064])

Sample prediction:
Target: HIS WIFE FED HIM WITH THE FAT OF THE LAND REGARDLESS OF CONSEQUENCES HIS CHILDREN REVOLVED ABOUT HIM WITH TIRELESS CURIOSITY AND WONDER HIS NEIGHBORS FLOCKED IN TO APPLAUD ADVISE AND ADMIRE EVERY ONE TREATED HIM WITH A RESPECT MOST GRATEFUL TO HIS FEELINGS HE WAS AN OBJECT OF INTEREST AND WITH EVERY HOUR HIS IMPORTANCE INCREASED SO THAT BY NIGHT HE FELT LIKE A COMMANDER IN CHIEF AND BORE HIMSELF ACCORDINGLY
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 0ERSELF HIS   THE F ING OF THESIDENCE. FREN WOLVED A THE  THEREATLESS F0OSITY.0DER. CHILDIGHBORS WEDED TO TO HELPPEAUD HIMIS HIM HELPIRE HISTHING OFOLD HIM WITH RES HIGHPECTFUL OFATEFUL FOR HIS WATINGS AND WAS A EX OF CONEST AND0 THEONE HE POPANCE GCREASED AND MUCH HE THE HE WASELT LIKE A KINGER IN CHIEF OF ARI THESELF AORDINGLY.
Loss: 10.2129
outputs.loss tensor(10.2129, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0008.flac
Waveform stats - mean: -0.0000, std: 0.0371, min: -0.3434, max: 0.4953
Resampled waveform stats - mean: -0.0000, std: 0.0371, min: -0.3434, max: 0.4953
Raw mel spectrogram stats - mean: 0.5128, std: 2.2412, min: 0.0000, max: 66.2038
Log mel spectrogram stats - mean: -5.6725, std: 3.9685, min: -13.7281, max: 4.1927
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0299, max: 2.4859
Mel spec shape: torch.Size([1, 80, 1039])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0293, max: 2.4863
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
input_embeds.shape torch.Size([1, 110, 3584])
labels.shape torch.Size([1, 110])
outputs.logits.shape torch.Size([1, 110, 152064])

Sample prediction:
Target: UNDER SUCH CIRCUMSTANCES SHE WAS HARDLY LIKELY TO BE LIVELY HERSELF BUT HER NAME WAS MADGE AND IT WAS THE ACCIDENT OF HER CHRISTIAN NAME WHICH DECIDED ME TO GO
Prediction: 000000000000000000000000000000000000000000000000000000000000000000 CONDITIONSIRCUMSTANCES, W NOTLY KELY TO BE AUCKED.SELF. SHESELF WAS LY. SHE WAS A0OMAL THE LIFEIANITY THAT MADEIDED HER TO WRITE TO
Loss: 9.7586
outputs.loss tensor(9.7586, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0011.flac
Waveform stats - mean: -0.0001, std: 0.0395, min: -0.3542, max: 0.3333
Resampled waveform stats - mean: -0.0001, std: 0.0395, min: -0.3542, max: 0.3333
Raw mel spectrogram stats - mean: 0.5616, std: 5.6367, min: 0.0000, max: 288.0907
Log mel spectrogram stats - mean: -6.4094, std: 3.8036, min: -13.7791, max: 5.6633
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9375, max: 3.1740
Mel spec shape: torch.Size([1, 80, 874])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9375, max: 3.1738
CNN output shape: torch.Size([1, 512, 55])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
input_embeds.shape torch.Size([1, 100, 3584])
labels.shape torch.Size([1, 100])
outputs.logits.shape torch.Size([1, 100, 152064])

Sample prediction:
Target: I'LL TELL YOU WHAT IT IS B EXCLAIMED MISSUS BOZZLE IT'S MY BELIEF AS HE AIN'T QUITE RIGHT UP HERE AND MISSUS BOZZLE TOUCHED HER FOREHEAD
Prediction: 00000000000000000000000000000000000000000000000000000000000 YOU THAT I IS.0ACTED ISS OO  IS A BOLOF THAT A ISDO'T AITE A. THERE.0US BOZZLE ITED ME NOHEAD AND
Loss: 9.0224
outputs.loss tensor(9.0224, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0023.flac
Waveform stats - mean: 0.0003, std: 0.1403, min: -0.6441, max: 0.6488
Resampled waveform stats - mean: 0.0003, std: 0.1403, min: -0.6441, max: 0.6488
Raw mel spectrogram stats - mean: 7.3655, std: 58.9716, min: 0.0000, max: 2399.0203
Log mel spectrogram stats - mean: -4.9308, std: 5.3304, min: -13.8155, max: 7.7828
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6668, max: 2.3851
Mel spec shape: torch.Size([1, 80, 499])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6670, max: 2.3848
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: YOU'VE SOMETHING TO TELL ME I SEE IT IN YOUR FACE DEAR I MUST GO
Prediction: 0000000000000000000000000000000002 GOTETHING TO SAYELL ME",'M YOU IN YOUR E0AR I SEE BE NOW
Loss: 9.7579
outputs.loss tensor(9.7579, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43363/5536-43363-0002.flac
Waveform stats - mean: -0.0001, std: 0.0743, min: -0.3287, max: 0.4828
Resampled waveform stats - mean: -0.0001, std: 0.0743, min: -0.3287, max: 0.4828
Raw mel spectrogram stats - mean: 2.0683, std: 10.0197, min: 0.0000, max: 264.4271
Log mel spectrogram stats - mean: -5.2917, std: 4.1250, min: -13.3503, max: 5.5776
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9536, max: 2.6350
Mel spec shape: torch.Size([1, 80, 703])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9541, max: 2.6348
CNN output shape: torch.Size([1, 512, 44])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
input_embeds.shape torch.Size([1, 84, 3584])
labels.shape torch.Size([1, 84])
outputs.logits.shape torch.Size([1, 84, 152064])

Sample prediction:
Target: THE MEN BLACKEN THEIR FACES AND WIDOWS OR BEREAVED PARENTS SOMETIMES GASH THEIR ARMS AND LEGS TILL THEY ARE COVERED WITH BLOOD
Prediction: 0000000000000000000000000000000000000000000000 ANDED EACES00EREENS OF0URNAVED WARENTS.UFFERIMES HAVEIVEED HEADMS AND LEGS WITHILL THEY BLE BED WITH BLOOD AND
Loss: 11.4446
outputs.loss tensor(11.4446, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0012.flac
Waveform stats - mean: -0.0000, std: 0.0538, min: -0.5219, max: 0.4296
Resampled waveform stats - mean: -0.0000, std: 0.0538, min: -0.5219, max: 0.4296
Raw mel spectrogram stats - mean: 1.0352, std: 7.2156, min: 0.0000, max: 404.9312
Log mel spectrogram stats - mean: -5.2730, std: 4.0397, min: -13.7456, max: 6.0037
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0974, max: 2.7915
Mel spec shape: torch.Size([1, 80, 581])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0977, max: 2.7910
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
input_embeds.shape torch.Size([1, 65, 3584])
labels.shape torch.Size([1, 65])
outputs.logits.shape torch.Size([1, 65, 152064])

Sample prediction:
Target: IN EITHER OF THESE RESTAURANTS YOU WILL BE SERVED WITH THE BEST THE MARKET AFFORDS COOKED THE RIGHT WAY
Prediction: 00000000000000000000000000000000000000   TWOAURANTS,0 FIND ABVED BY A BEST FOOD BESTET HASORDS.FFED FOOD BEST WAY.
Loss: 9.6428
outputs.loss tensor(9.6428, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0012.flac
Waveform stats - mean: -0.0000, std: 0.0796, min: -0.5012, max: 0.5099
Resampled waveform stats - mean: -0.0000, std: 0.0796, min: -0.5012, max: 0.5099
Raw mel spectrogram stats - mean: 2.3260, std: 20.3614, min: 0.0000, max: 833.7675
Log mel spectrogram stats - mean: -4.9173, std: 3.6143, min: -13.4017, max: 6.7260
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3475, max: 3.2215
Mel spec shape: torch.Size([1, 80, 549])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3477, max: 3.2207
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 62, 3584])
labels.shape torch.Size([1, 62])
outputs.logits.shape torch.Size([1, 62, 152064])

Sample prediction:
Target: DON QUIXOTE DID SO AND ASKED HIM WHAT HAD HAPPENED TO HIM AND WHAT HE WAS AFRAID OF
Prediction: 000000000000000000000000000000000000XOT0 NOT0  AED THE TO HEAD HEENED TO THE.0 HE H DORAID OF.
Loss: 9.2040
outputs.loss tensor(9.2040, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0026.flac
Waveform stats - mean: 0.0000, std: 0.0682, min: -0.7360, max: 0.7827
Resampled waveform stats - mean: 0.0000, std: 0.0682, min: -0.7360, max: 0.7827
Raw mel spectrogram stats - mean: 1.7381, std: 14.0286, min: 0.0000, max: 699.1981
Log mel spectrogram stats - mean: -6.9186, std: 4.8802, min: -13.8126, max: 6.5499
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4127, max: 2.7598
Mel spec shape: torch.Size([1, 80, 482])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4131, max: 2.7598
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 48, 3584])
labels.shape torch.Size([1, 48])
outputs.logits.shape torch.Size([1, 48, 152064])

Sample prediction:
Target: YET THE ORIGINAL WEARS TO COMMON EYES A VERY DIFFERENT EXPRESSION
Prediction: 000000000000000000000000000000000  ATHER  BE 0 0 GOODERENT WERESSION OF
Loss: 10.6964
outputs.loss tensor(10.6964, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0009.flac
Waveform stats - mean: 0.0000, std: 0.1268, min: -0.8604, max: 0.6904
Resampled waveform stats - mean: 0.0000, std: 0.1268, min: -0.8604, max: 0.6904
Raw mel spectrogram stats - mean: 6.0110, std: 32.6122, min: 0.0000, max: 1007.8614
Log mel spectrogram stats - mean: -4.5198, std: 4.6666, min: -13.6455, max: 6.9156
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9556, max: 2.4505
Mel spec shape: torch.Size([1, 80, 262])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9551, max: 2.4512
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 32, 3584])
labels.shape torch.Size([1, 32])
outputs.logits.shape torch.Size([1, 32, 152064])

Sample prediction:
Target: I HAD THE PLEASURE OF MEETING HIM IN SOCIETY
Prediction: 00000000000000000000 0ASURE OF BEETING YOU. THE MANYETY.
Loss: 10.7724
outputs.loss tensor(10.7724, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0009.flac
Waveform stats - mean: 0.0000, std: 0.0444, min: -0.5961, max: 0.4986
Resampled waveform stats - mean: 0.0000, std: 0.0444, min: -0.5961, max: 0.4986
Raw mel spectrogram stats - mean: 0.7364, std: 8.9305, min: 0.0000, max: 476.8108
Log mel spectrogram stats - mean: -5.6789, std: 3.5288, min: -13.5365, max: 6.1671
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2267, max: 3.3570
Mel spec shape: torch.Size([1, 80, 279])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2266, max: 3.3574
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
input_embeds.shape torch.Size([1, 29, 3584])
labels.shape torch.Size([1, 29])
outputs.logits.shape torch.Size([1, 29, 152064])

Sample prediction:
Target: NO ONE COULD ESCAPE FROM THIS RICTUS
Prediction: 00000000000000000000ULD DOAPE THE THE PR0OR0
Loss: 10.8742
outputs.loss tensor(10.8742, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9006/2902-9006-0017.flac
Waveform stats - mean: -0.0000, std: 0.0441, min: -0.4455, max: 0.4314
Resampled waveform stats - mean: -0.0000, std: 0.0441, min: -0.4455, max: 0.4314
Raw mel spectrogram stats - mean: 0.7295, std: 8.9797, min: 0.0000, max: 1050.3644
Log mel spectrogram stats - mean: -5.3691, std: 3.1223, min: -13.7472, max: 6.9569
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.6833, max: 3.9477
Mel spec shape: torch.Size([1, 80, 2377])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.6836, max: 3.9473
CNN output shape: torch.Size([1, 512, 149])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 76288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 76288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 534016
Inf count: 0
audio_emb.shape torch.Size([1, 149, 3584])
input_embeds.shape torch.Size([1, 259, 3584])
labels.shape torch.Size([1, 259])
outputs.logits.shape torch.Size([1, 259, 152064])

Sample prediction:
Target: CLIMATE BAD EXAMPLE AND THE LUXURY OF POWER DEGRADED THEM IN ONE CENTURY INTO A RACE OF HELPLESS AND DEBAUCHED SLAVE HOLDERS DOOMED TO UTTER EXTERMINATION BEFORE THE SEMI GOTHIC ARMIES OF BELISARIUS AND WITH THEM VANISHED THE LAST CHANCE THAT THE GOTHIC RACES WOULD EXERCISE ON THE EASTERN WORLD THE SAME STERN YET WHOLESOME DISCIPLINE UNDER WHICH THE WESTERN HAD BEEN RESTORED TO LIFE
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000S  ARGURY OF THE GREDED  TO THE WAYURY. THE AP OF POWERIXSSNESS0GRAUCHERY POWERUT TR.INGED TO ATERLYISTMINATION OF THE ENDI-CHOSTIC AGEEN OF THEGIARUS AND THE THE THEISH INTO0 OFANCE OF THE0OTHIC ARACE HOULD HAVEISTISE ANY THE WORLDERN B THE LAST POWERRE ANDET UNSEOME INCIPLINE THAT WHICH THEY GERN WORLDAD BEEN RULEORED BY THE AND
Loss: 10.3136
outputs.loss tensor(10.3136, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0009.flac
Waveform stats - mean: -0.0000, std: 0.0453, min: -0.3103, max: 0.3913
Resampled waveform stats - mean: -0.0000, std: 0.0453, min: -0.3103, max: 0.3913
Raw mel spectrogram stats - mean: 0.7654, std: 4.6543, min: 0.0000, max: 186.7824
Log mel spectrogram stats - mean: -5.6973, std: 3.7596, min: -13.5292, max: 5.2299
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0832, max: 2.9065
Mel spec shape: torch.Size([1, 80, 336])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0840, max: 2.9062
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 38, 3584])
labels.shape torch.Size([1, 38])
outputs.logits.shape torch.Size([1, 38, 152064])

Sample prediction:
Target: ONE COULD SEE THAT HE WAS VERY MUCH WRAPPED UP IN HIS OFFSPRING
Prediction: 00000000000000000000000 BE 000 CO COIT UP IN ONE OWNICERING.
Loss: 10.3400
outputs.loss tensor(10.3400, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0007.flac
Waveform stats - mean: -0.0000, std: 0.0632, min: -0.3552, max: 0.3695
Resampled waveform stats - mean: -0.0000, std: 0.0632, min: -0.3552, max: 0.3695
Raw mel spectrogram stats - mean: 1.4957, std: 9.8134, min: 0.0000, max: 514.4373
Log mel spectrogram stats - mean: -5.8077, std: 4.3605, min: -13.8155, max: 6.2431
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8364, max: 2.7636
Mel spec shape: torch.Size([1, 80, 953])
Audio encoder input stats - mean: -0.0001, std: 1.0000, min: -1.8369, max: 2.7637
CNN output shape: torch.Size([1, 512, 60])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30720
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30720
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 215040
Inf count: 0
audio_emb.shape torch.Size([1, 60, 3584])
input_embeds.shape torch.Size([1, 99, 3584])
labels.shape torch.Size([1, 99])
outputs.logits.shape torch.Size([1, 99, 152064])

Sample prediction:
Target: BY HIS DEFAULT SHORT WHILE HE SOJOURNED HERE BY HIS DEFAULT TO WEEPING AND TO TOIL HE CHANGED HIS INNOCENT LAUGHTER AND SWEET PLAY
Prediction: 00000000000000000000000000000000000000000000000000000000000000 S HIS ISFTOURNED IN IN HIS DEFAULT SHORT SO0. 0 W WING SOANGED HIS NAMEDIATEENT TOWSER TO0WEETNESSING
Loss: 8.9014
outputs.loss tensor(8.9014, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/23268/3536-23268-0025.flac
Waveform stats - mean: -0.0001, std: 0.0582, min: -0.5068, max: 0.5289
Resampled waveform stats - mean: -0.0001, std: 0.0582, min: -0.5068, max: 0.5289
Raw mel spectrogram stats - mean: 1.2695, std: 12.0957, min: 0.0000, max: 707.1653
Log mel spectrogram stats - mean: -5.0798, std: 3.6117, min: -13.5876, max: 6.5613
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3556, max: 3.2231
Mel spec shape: torch.Size([1, 80, 502])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3555, max: 3.2227
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
input_embeds.shape torch.Size([1, 61, 3584])
labels.shape torch.Size([1, 61])
outputs.logits.shape torch.Size([1, 61, 152064])

Sample prediction:
Target: IT IS OFTEN THE UNGRATEFUL TASK OF A FRIEND TO BE TROUBLESOME SOMETIMES UNMANNERLY
Prediction: 000000000000000000000000000000000 NOTTEN THE0.SUFULNESS OF A MAN TO T AOLDUBLEDOME.OMETIMES.PLEORLY AND
Loss: 10.6868
outputs.loss tensor(10.6868, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/168635/174-168635-0002.flac
Waveform stats - mean: -0.0001, std: 0.0457, min: -0.6046, max: 0.5882
Resampled waveform stats - mean: -0.0001, std: 0.0457, min: -0.6046, max: 0.5882
Raw mel spectrogram stats - mean: 0.7822, std: 6.7384, min: 0.0000, max: 658.5579
Log mel spectrogram stats - mean: -5.6746, std: 3.9594, min: -13.7655, max: 6.4901
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0435, max: 3.0724
Mel spec shape: torch.Size([1, 80, 1587])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0430, max: 3.0723
CNN output shape: torch.Size([1, 512, 100])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 51200
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 51200
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 358400
Inf count: 0
audio_emb.shape torch.Size([1, 100, 3584])
input_embeds.shape torch.Size([1, 163, 3584])
labels.shape torch.Size([1, 163])
outputs.logits.shape torch.Size([1, 163, 152064])

Sample prediction:
Target: HIS SISTER AND HIS SISTER'S CHILDREN HAD LEFT HIM ONLY A VAGUE AND FAR OFF MEMORY WHICH HAD FINALLY ALMOST COMPLETELY VANISHED HE HAD MADE EVERY EFFORT TO FIND THEM AND NOT HAVING BEEN ABLE TO FIND THEM HE HAD FORGOTTEN THEM
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000   SISTER AND SREN AD A THE AL ONE CHILDACUE IDEA0- MEMORY OF HEAD LEFTALLY COMMOST COMLY DISISHED FROM HAD A A EFFORT TO0 HIS BUT0 AAVING SUCCESS SUCCESSLE TO DO THEM HE HAD GIVENGOTTEN THEM.
Loss: 10.5353
outputs.loss tensor(10.5353, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0015.flac
Waveform stats - mean: -0.0001, std: 0.0802, min: -0.5105, max: 0.5475
Resampled waveform stats - mean: -0.0001, std: 0.0802, min: -0.5105, max: 0.5475
Raw mel spectrogram stats - mean: 2.4085, std: 16.4435, min: 0.0000, max: 574.3138
Log mel spectrogram stats - mean: -4.4418, std: 3.4882, min: -12.6900, max: 6.3532
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3646, max: 3.0947
Mel spec shape: torch.Size([1, 80, 738])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3652, max: 3.0957
CNN output shape: torch.Size([1, 512, 47])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
input_embeds.shape torch.Size([1, 76, 3584])
labels.shape torch.Size([1, 76])
outputs.logits.shape torch.Size([1, 76, 152064])

Sample prediction:
Target: THE METAL FOREST IS IN THE GREAT DOMED CAVERN THE LARGEST IN ALL OUR DOMINIONS REPLIED KALIKO
Prediction: 000000000000000000000000000000000000000000000000RO ST00 THE FORE NORTHIN CITYVE.0ARGEST IN THE THE LANDAINSIONS.VER THEALIO.
Loss: 9.6784
outputs.loss tensor(9.6784, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0021.flac
Waveform stats - mean: 0.0000, std: 0.0584, min: -0.3177, max: 0.5584
Resampled waveform stats - mean: 0.0000, std: 0.0584, min: -0.3177, max: 0.5584
Raw mel spectrogram stats - mean: 1.2774, std: 11.9755, min: 0.0000, max: 651.5112
Log mel spectrogram stats - mean: -6.9676, std: 4.3796, min: -13.7464, max: 6.4793
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5478, max: 3.0704
Mel spec shape: torch.Size([1, 80, 379])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5479, max: 3.0703
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 42, 3584])
labels.shape torch.Size([1, 42])
outputs.logits.shape torch.Size([1, 42, 152064])

Sample prediction:
Target: ALL THE SAME HE ADDED IRRELEVANTLY YOU SHALL HAVE THE LATCH KEY
Prediction: 0000000000000000000000000 0ARDED 0LEVANT  0 BE A SAMEATESTES TO
Loss: 9.9059
outputs.loss tensor(9.9059, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0012.flac
Waveform stats - mean: -0.0000, std: 0.0585, min: -0.7557, max: 0.6030
Resampled waveform stats - mean: -0.0000, std: 0.0585, min: -0.7557, max: 0.6030
Raw mel spectrogram stats - mean: 1.2423, std: 8.2578, min: 0.0000, max: 546.5047
Log mel spectrogram stats - mean: -5.9868, std: 4.3743, min: -13.7708, max: 6.3035
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7795, max: 2.8097
Mel spec shape: torch.Size([1, 80, 929])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7793, max: 2.8105
CNN output shape: torch.Size([1, 512, 59])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
input_embeds.shape torch.Size([1, 92, 3584])
labels.shape torch.Size([1, 92])
outputs.logits.shape torch.Size([1, 92, 152064])

Sample prediction:
Target: SALAD TWO CUPS OF APPLES CUT INTO SMALL PIECES ONE CUP CELERY CUT INTO SMALL PIECES ONE CUP ENGLISH WALNUTS
Prediction: 0000000000000000000000000000000000000000000000000000000000000000 OF LES  INTO  PIECES. CUP OFERY CH INTO SMALL PIECES  CUP ONZ CNUTS CUT
Loss: 11.0257
outputs.loss tensor(11.0257, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0013.flac
Waveform stats - mean: -0.0000, std: 0.0466, min: -0.3156, max: 0.3167
Resampled waveform stats - mean: -0.0000, std: 0.0466, min: -0.3156, max: 0.3167
Raw mel spectrogram stats - mean: 0.8116, std: 5.2397, min: 0.0000, max: 244.3999
Log mel spectrogram stats - mean: -8.4800, std: 5.0867, min: -13.8137, max: 5.4988
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.0486, max: 2.7481
Mel spec shape: torch.Size([1, 80, 345])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.0488, max: 2.7480
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
input_embeds.shape torch.Size([1, 32, 3584])
labels.shape torch.Size([1, 32])
outputs.logits.shape torch.Size([1, 32, 152064])

Sample prediction:
Target: ON MONDAY THE TIDE WAS REVERSED
Prediction: 00000000000000000000000   UES OF ASONSED.
Loss: 10.2262
outputs.loss tensor(10.2262, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0011.flac
Waveform stats - mean: 0.0001, std: 0.0850, min: -0.7410, max: 0.5723
Resampled waveform stats - mean: 0.0001, std: 0.0850, min: -0.7410, max: 0.5723
Raw mel spectrogram stats - mean: 2.6866, std: 14.9607, min: 0.0000, max: 529.4606
Log mel spectrogram stats - mean: -3.6823, std: 3.6679, min: -11.8703, max: 6.2719
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2323, max: 2.7139
Mel spec shape: torch.Size([1, 80, 227])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2324, max: 2.7129
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 22, 3584])
labels.shape torch.Size([1, 22])
outputs.logits.shape torch.Size([1, 22, 152064])

Sample prediction:
Target: I SHALL WAIT FOR TIME TO SHOW
Prediction: 0000000000000000 BE FOR  TO COM 
Loss: 11.1264
outputs.loss tensor(11.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0081.flac
Waveform stats - mean: 0.0009, std: 0.0117, min: -0.0843, max: 0.0798
Resampled waveform stats - mean: 0.0009, std: 0.0117, min: -0.0843, max: 0.0798
Raw mel spectrogram stats - mean: 0.0505, std: 0.2761, min: 0.0000, max: 10.2244
Log mel spectrogram stats - mean: -6.7117, std: 2.9483, min: -12.9840, max: 2.3248
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1274, max: 3.0650
Mel spec shape: torch.Size([1, 80, 306])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1270, max: 3.0645
CNN output shape: torch.Size([1, 512, 20])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
input_embeds.shape torch.Size([1, 32, 3584])
labels.shape torch.Size([1, 32])
outputs.logits.shape torch.Size([1, 32, 152064])

Sample prediction:
Target: THERE WAS NO PONIARD IN THE WOUND
Prediction: 0000000000000000000000 A WAY0TO00 THE BOOD.
Loss: 10.1552
outputs.loss tensor(10.1552, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0018.flac
Waveform stats - mean: 0.0000, std: 0.0546, min: -0.5815, max: 0.6027
Resampled waveform stats - mean: 0.0000, std: 0.0546, min: -0.5815, max: 0.6027
Raw mel spectrogram stats - mean: 1.1210, std: 11.9651, min: 0.0000, max: 1152.6433
Log mel spectrogram stats - mean: -5.5522, std: 3.8037, min: -13.8093, max: 7.0498
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1708, max: 3.3131
Mel spec shape: torch.Size([1, 80, 620])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1699, max: 3.3125
CNN output shape: torch.Size([1, 512, 39])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
input_embeds.shape torch.Size([1, 69, 3584])
labels.shape torch.Size([1, 69])
outputs.logits.shape torch.Size([1, 69, 152064])

Sample prediction:
Target: HI THERE HISSED LUMPY FILLED WITH INDIGNATION THAT ANYONE SHOULD ATTEMPT TO MOUNT A PONY FROM THE RIGHT SIDE
Prediction: 0000000000000000000000000000000000000000 ARE 00 OF LED L UATION.0ONE WHO BE LEPT TO READEND IT CHUNCH. THE BACK SIDE OF
Loss: 10.5452
outputs.loss tensor(10.5452, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0023.flac
Waveform stats - mean: -0.0000, std: 0.0502, min: -0.2464, max: 0.5405
Resampled waveform stats - mean: -0.0000, std: 0.0502, min: -0.2464, max: 0.5405
Raw mel spectrogram stats - mean: 0.9444, std: 4.6550, min: 0.0000, max: 109.5251
Log mel spectrogram stats - mean: -6.4430, std: 4.6977, min: -13.8138, max: 4.6962
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5690, max: 2.3712
Mel spec shape: torch.Size([1, 80, 329])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5693, max: 2.3711
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 34, 3584])
labels.shape torch.Size([1, 34])
outputs.logits.shape torch.Size([1, 34, 152064])

Sample prediction:
Target: SHE ASKED DIRECTLY IF HER FATHER WAS DEAD
Prediction: 00000000000000000000000 ED LY FOR0 CHILDATHER WAS A OR
Loss: 9.5781
outputs.loss tensor(9.5781, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0006.flac
Waveform stats - mean: -0.0001, std: 0.0574, min: -0.5909, max: 0.5388
Resampled waveform stats - mean: -0.0001, std: 0.0574, min: -0.5909, max: 0.5388
Raw mel spectrogram stats - mean: 1.1672, std: 6.4115, min: 0.0000, max: 523.8611
Log mel spectrogram stats - mean: -5.4320, std: 4.2268, min: -13.6950, max: 6.2612
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9549, max: 2.7664
Mel spec shape: torch.Size([1, 80, 912])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9551, max: 2.7656
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
input_embeds.shape torch.Size([1, 93, 3584])
labels.shape torch.Size([1, 93])
outputs.logits.shape torch.Size([1, 93, 152064])

Sample prediction:
Target: CAULIFLOWER MAYONNAISE TAKE COLD BOILED CAULIFLOWER BREAK INTO BRANCHES ADDING SALT PEPPER AND VINEGAR TO SEASON
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000 CAIL CAULIFLOWER MAY CA ANES CA CA CAALT CAAN CA CAINEGAR CA CAASONED
Loss: 11.4219
outputs.loss tensor(11.4219, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/64726/6319-64726-0009.flac
Waveform stats - mean: -0.0000, std: 0.0762, min: -0.6805, max: 0.8763
Resampled waveform stats - mean: -0.0000, std: 0.0762, min: -0.6805, max: 0.8763
Raw mel spectrogram stats - mean: 2.1688, std: 19.1949, min: 0.0000, max: 1484.0223
Log mel spectrogram stats - mean: -5.6330, std: 4.0919, min: -13.7579, max: 7.3025
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9856, max: 3.1613
Mel spec shape: torch.Size([1, 80, 1025])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9854, max: 3.1621
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
input_embeds.shape torch.Size([1, 115, 3584])
labels.shape torch.Size([1, 115])
outputs.logits.shape torch.Size([1, 115, 152064])

Sample prediction:
Target: SCARCELY HAD HE COME TO THE WOOD WHEN ALL THE TREES AND THORNS WHICH HAD MADE SUCH AN IMPENETRABLE THICKET OPENED ON ONE SIDE AND THE OTHER TO OFFER HIM A PATH
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000 ARE TO THE OODS HE THE WES W0OOUGH W WAD BEEN THE A IMPRESSIONETRABLE BICKNESS OFED UP THE SIDE AND0 W AND LETER A A WAY OF
Loss: 9.4462
outputs.loss tensor(9.4462, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147960/2035-147960-0003.flac
Waveform stats - mean: -0.0001, std: 0.0503, min: -0.3207, max: 0.4095
Resampled waveform stats - mean: -0.0001, std: 0.0503, min: -0.3207, max: 0.4095
Raw mel spectrogram stats - mean: 0.9483, std: 7.6776, min: 0.0000, max: 450.9011
Log mel spectrogram stats - mean: -5.4755, std: 3.5012, min: -13.1585, max: 6.1112
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1944, max: 3.3094
Mel spec shape: torch.Size([1, 80, 585])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1953, max: 3.3086
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
input_embeds.shape torch.Size([1, 62, 3584])
labels.shape torch.Size([1, 62])
outputs.logits.shape torch.Size([1, 62, 152064])

Sample prediction:
Target: THERE HAD BEEN ANOTHER BLACK FROST THE NIGHT BEFORE AND THE AIR WAS CLEAR AND HEADY AS WINE
Prediction: 0000000000000000000000000000000000000000 BEEN OTHER  ORTG.  BEFORE THE0 MO WAS C AND THEY WITH THEINE AND
Loss: 8.9160
outputs.loss tensor(8.9160, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0019.flac
Waveform stats - mean: 0.0000, std: 0.0500, min: -0.3816, max: 0.4119
Resampled waveform stats - mean: 0.0000, std: 0.0500, min: -0.3816, max: 0.4119
Raw mel spectrogram stats - mean: 0.9333, std: 6.4448, min: 0.0000, max: 338.7372
Log mel spectrogram stats - mean: -4.7459, std: 3.0892, min: -13.7315, max: 5.8252
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.9087, max: 3.4219
Mel spec shape: torch.Size([1, 80, 839])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.9082, max: 3.4219
CNN output shape: torch.Size([1, 512, 53])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27136
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27136
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 189952
Inf count: 0
audio_emb.shape torch.Size([1, 53, 3584])
input_embeds.shape torch.Size([1, 93, 3584])
labels.shape torch.Size([1, 93])
outputs.logits.shape torch.Size([1, 93, 152064])

Sample prediction:
Target: THE LEMON WAS FIRST CULTIVATED IN ENGLAND IN THE BEGINNING OF THE SEVENTEENTH CENTURY AND IS NOW OFTEN TO BE FOUND IN OUR GREEN HOUSES
Prediction: 00000000000000000000000000000000000000000000000000000000  USEDULTIVATED IN GLAND IN THE NING OF THE VENTEENTH CENTURY.0 STILL GTEN USED BE FOUND IN THE KHOS AND
Loss: 9.9662
outputs.loss tensor(9.9662, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0027.flac
Waveform stats - mean: -0.0000, std: 0.0687, min: -0.4436, max: 0.3703
Resampled waveform stats - mean: -0.0000, std: 0.0687, min: -0.4436, max: 0.3703
Raw mel spectrogram stats - mean: 1.7221, std: 8.6340, min: 0.0000, max: 155.1669
Log mel spectrogram stats - mean: -6.7836, std: 5.0410, min: -13.8054, max: 5.0445
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3929, max: 2.3464
Mel spec shape: torch.Size([1, 80, 243])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.3926, max: 2.3457
CNN output shape: torch.Size([1, 512, 16])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
input_embeds.shape torch.Size([1, 25, 3584])
labels.shape torch.Size([1, 25])
outputs.logits.shape torch.Size([1, 25, 152064])

Sample prediction:
Target: HE WAS GOING HOME AFTER VICTORY
Prediction: 00000000000000000 ING TO  HEACORY.
Loss: 11.1960
outputs.loss tensor(11.1960, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/136532/251-136532-0023.flac
Waveform stats - mean: -0.0000, std: 0.0930, min: -0.9200, max: 0.9402
Resampled waveform stats - mean: -0.0000, std: 0.0930, min: -0.9200, max: 0.9402
Raw mel spectrogram stats - mean: 3.2438, std: 18.9150, min: 0.0000, max: 765.7662
Log mel spectrogram stats - mean: -4.3285, std: 4.3713, min: -13.7860, max: 6.6409
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1635, max: 2.5094
Mel spec shape: torch.Size([1, 80, 840])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1641, max: 2.5098
CNN output shape: torch.Size([1, 512, 53])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27136
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27136
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 189952
Inf count: 0
audio_emb.shape torch.Size([1, 53, 3584])
input_embeds.shape torch.Size([1, 89, 3584])
labels.shape torch.Size([1, 89])
outputs.logits.shape torch.Size([1, 89, 152064])

Sample prediction:
Target: THE ORGANIZATION OF A SOCIETY OF MARTIAN ARCHAEOLOGY WITH ANTHONY LATTIMER PH D THE LOGICAL CANDIDATE FOR THE CHAIR
Prediction: 000000000000000000000000000000000000000000000000000000IGINIZATION OF THE CIETY OF HE MCHAEOLOG  A ORY .AN .D 0IC ANDULTIDATE FOR THE MIEMAN
Loss: 10.5952
outputs.loss tensor(10.5952, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/152373/2035-152373-0018.flac
Waveform stats - mean: -0.0001, std: 0.0554, min: -0.4581, max: 0.5807
Resampled waveform stats - mean: -0.0001, std: 0.0554, min: -0.4581, max: 0.5807
Raw mel spectrogram stats - mean: 1.1470, std: 10.8221, min: 0.0000, max: 602.3528
Log mel spectrogram stats - mean: -5.5481, std: 3.6057, min: -13.6949, max: 6.4008
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2594, max: 3.3140
Mel spec shape: torch.Size([1, 80, 602])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2598, max: 3.3145
CNN output shape: torch.Size([1, 512, 38])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
input_embeds.shape torch.Size([1, 61, 3584])
labels.shape torch.Size([1, 61])
outputs.logits.shape torch.Size([1, 61, 152064])

Sample prediction:
Target: NOTHING COULD BE MORE NATURAL THAN SUCH AN ASSEMBLY IN SUCH A PLACE AT SUCH A PERIOD
Prediction: 0000000000000000000000000000000000000000ULD BE DONE IMPORTANTURAL THAN THIS A EHLY OF THE A NAT. SUCH A TIMEIOD OF
Loss: 9.8084
outputs.loss tensor(9.8084, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0056.flac
Waveform stats - mean: -0.0000, std: 0.0572, min: -0.5045, max: 0.3451
Resampled waveform stats - mean: -0.0000, std: 0.0572, min: -0.5045, max: 0.3451
Raw mel spectrogram stats - mean: 1.2174, std: 7.4658, min: 0.0000, max: 184.2628
Log mel spectrogram stats - mean: -5.5389, std: 3.2886, min: -12.8205, max: 5.2164
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2142, max: 3.2704
Mel spec shape: torch.Size([1, 80, 211])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2148, max: 3.2695
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
input_embeds.shape torch.Size([1, 22, 3584])
labels.shape torch.Size([1, 22])
outputs.logits.shape torch.Size([1, 22, 152064])

Sample prediction:
Target: FRIED BREAD CRUMBS
Prediction: 000000000000000000 UMBS 
Loss: 10.7381
outputs.loss tensor(10.7381, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302196/8842-302196-0012.flac
Waveform stats - mean: -0.0003, std: 0.0651, min: -0.5326, max: 0.5601
Resampled waveform stats - mean: -0.0003, std: 0.0651, min: -0.5326, max: 0.5601
Raw mel spectrogram stats - mean: 1.5675, std: 12.1824, min: 0.0000, max: 831.9683
Log mel spectrogram stats - mean: -4.1764, std: 3.4577, min: -12.9770, max: 6.7238
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5452, max: 3.1524
Mel spec shape: torch.Size([1, 80, 564])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5449, max: 3.1523
CNN output shape: torch.Size([1, 512, 36])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
input_embeds.shape torch.Size([1, 63, 3584])
labels.shape torch.Size([1, 63])
outputs.logits.shape torch.Size([1, 63, 152064])

Sample prediction:
Target: THIS HAS INDUCED SOME EDITORS OF THE VITA NUOVA TO EXPLAIN THE TITLE AS MEANING EARLY LIFE
Prediction: 000000000000000000000000000000000000 IS BEENICED  ING TO THE ARI00VA TO WRITEAM THE2 OF AANING "LY IN.
Loss: 10.5053
outputs.loss tensor(10.5053, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0023.flac
Waveform stats - mean: -0.0000, std: 0.0687, min: -0.4441, max: 0.4272
Resampled waveform stats - mean: -0.0000, std: 0.0687, min: -0.4441, max: 0.4272
Raw mel spectrogram stats - mean: 1.7661, std: 9.7670, min: 0.0000, max: 388.1075
Log mel spectrogram stats - mean: -5.0449, std: 4.5927, min: -13.8134, max: 5.9613
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9092, max: 2.3964
Mel spec shape: torch.Size([1, 80, 721])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9092, max: 2.3965
CNN output shape: torch.Size([1, 512, 46])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23552
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23552
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 164864
Inf count: 0
audio_emb.shape torch.Size([1, 46, 3584])
input_embeds.shape torch.Size([1, 85, 3584])
labels.shape torch.Size([1, 85])
outputs.logits.shape torch.Size([1, 85, 152064])

Sample prediction:
Target: WE WERE AT THAT TIME AT LEAST A HUNDRED YARDS IN ADVANCE OF THE BRIGADE CHEATHAM ALL THE TIME CALLING UPON THE MEN TO COME ON
Prediction: 000000000000000000000000000000000000000000000000 NOT  PLACE  THATAST  WEUNDRED THARDS AW THEANCE OF THE IDGEADE.EAM  THE BR INING FOR THE THE BR IN THEE TO THE
Loss: 9.3496
outputs.loss tensor(9.3496, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0010.flac
Waveform stats - mean: -0.0001, std: 0.0742, min: -0.5332, max: 0.5040
Resampled waveform stats - mean: -0.0001, std: 0.0742, min: -0.5332, max: 0.5040
Raw mel spectrogram stats - mean: 2.0559, std: 11.5373, min: 0.0000, max: 534.1320
Log mel spectrogram stats - mean: -4.7033, std: 3.7920, min: -13.6162, max: 6.2806
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3505, max: 2.8966
Mel spec shape: torch.Size([1, 80, 1442])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3496, max: 2.8965
CNN output shape: torch.Size([1, 512, 91])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 46592
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 46592
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 326144
Inf count: 0
audio_emb.shape torch.Size([1, 91, 3584])
input_embeds.shape torch.Size([1, 142, 3584])
labels.shape torch.Size([1, 142])
outputs.logits.shape torch.Size([1, 142, 152064])

Sample prediction:
Target: ITALIAN MILLET OR GREAT INDIAN MILLET IS CULTIVATED IN EGYPT AND NUBIA WHERE IT IS CALLED DHOURRA AND IS USED AS HUMAN FOOD AS WELL AS FOR THE FERMENTATION OF BEER
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000IT 0NESSDIAN MILLET
 NOTULTIVATED IN INDGYPT, INDORTIA. IT IS USEDLED KAM..0 USED FOR AUMAN FOOD AND WELL AS FE FE FELOURMENTATION OF BEER.
Loss: 10.8035
outputs.loss tensor(10.8035, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0027.flac
Waveform stats - mean: 0.0009, std: 0.0174, min: -0.1444, max: 0.1721
Resampled waveform stats - mean: 0.0009, std: 0.0174, min: -0.1444, max: 0.1721
Raw mel spectrogram stats - mean: 0.1072, std: 0.5653, min: 0.0000, max: 34.7479
Log mel spectrogram stats - mean: -5.9066, std: 3.0555, min: -12.7611, max: 3.5481
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2433, max: 3.0944
Mel spec shape: torch.Size([1, 80, 398])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2441, max: 3.0938
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
input_embeds.shape torch.Size([1, 42, 3584])
labels.shape torch.Size([1, 42])
outputs.logits.shape torch.Size([1, 42, 152064])

Sample prediction:
Target: HOWEVER A LITTLE LATER WE HAD A COMFORTABLE CHAT
Prediction: 00000000000000000000000000EVER2ITTLE BITITTLE, WILLAD A MEBORTABLE DISAT WITH
Loss: 10.5880
outputs.loss tensor(10.5880, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0003.flac
Waveform stats - mean: 0.0000, std: 0.0547, min: -0.3672, max: 0.5066
Resampled waveform stats - mean: 0.0000, std: 0.0547, min: -0.3672, max: 0.5066
Raw mel spectrogram stats - mean: 1.1235, std: 8.5232, min: 0.0000, max: 359.4055
Log mel spectrogram stats - mean: -8.8309, std: 5.0934, min: -13.8151, max: 5.8845
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -0.9785, max: 2.8891
Mel spec shape: torch.Size([1, 80, 468])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -0.9785, max: 2.8887
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 44, 3584])
labels.shape torch.Size([1, 44])
outputs.logits.shape torch.Size([1, 44, 152064])

Sample prediction:
Target: GO QUIETLY ALONE NO HARM WILL BEFALL YOU
Prediction: 0000000000000000000000000000000ZLY GO ISE2  YOU DONEAR ON 
Loss: 9.7498
outputs.loss tensor(9.7498, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0019.flac
Waveform stats - mean: 0.0000, std: 0.0405, min: -0.2791, max: 0.3686
Resampled waveform stats - mean: 0.0000, std: 0.0405, min: -0.2791, max: 0.3686
Raw mel spectrogram stats - mean: 0.6150, std: 6.1629, min: 0.0000, max: 349.6954
Log mel spectrogram stats - mean: -7.1394, std: 4.0129, min: -13.5953, max: 5.8571
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6088, max: 3.2387
Mel spec shape: torch.Size([1, 80, 272])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6084, max: 3.2383
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 32, 3584])
labels.shape torch.Size([1, 32])
outputs.logits.shape torch.Size([1, 32, 152064])

Sample prediction:
Target: I LOOK AT MY WATCH IT'S A QUARTER TO ELEVEN
Prediction: 000000000000000000ED THE ES IS 0ARTER P TWOIGHTVEN.
Loss: 9.7822
outputs.loss tensor(9.7822, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9006/2902-9006-0014.flac
Waveform stats - mean: -0.0000, std: 0.0491, min: -0.6274, max: 0.5847
Resampled waveform stats - mean: -0.0000, std: 0.0491, min: -0.6274, max: 0.5847
Raw mel spectrogram stats - mean: 0.9045, std: 15.7947, min: 0.0000, max: 2023.8534
Log mel spectrogram stats - mean: -5.6523, std: 3.1896, min: -13.7954, max: 7.6128
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5530, max: 4.1588
Mel spec shape: torch.Size([1, 80, 2042])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5527, max: 4.1602
CNN output shape: torch.Size([1, 512, 128])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 65536
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 65536
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 458752
Inf count: 0
audio_emb.shape torch.Size([1, 128, 3584])
input_embeds.shape torch.Size([1, 230, 3584])
labels.shape torch.Size([1, 230])
outputs.logits.shape torch.Size([1, 230, 152064])

Sample prediction:
Target: HOW INIQUITOUS WAS THE CONDUCT OF THE SONS OF THEODOSIUS IN REFUSING THE USUAL BOUNTY BY WHICH THE GOTHS WERE BRIBED NOT TO ATTACK THE EMPIRE THE WHOLE PENT UP DELUGE BURST OVER THE PLAINS OF ITALY AND THE WESTERN EMPIRE BECAME FROM THAT DAY FORTH A DYING IDIOT WHILE THE NEW INVADERS DIVIDED EUROPE AMONG THEMSELVES
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 THE000 THE JOR THE OUT OF  ORIUS  THEERRING TO0 TO CONURY IN THE THE SHS WERE ABINGED TO TO ATTACK THE ROMPIRE OF USOLE WORLDUNH ARAY OF0G OUT THE USAIN OF THEALY THE0 GERN EMPIRE THEINGAME A THE TIME AORTH A WING WOLOTIC THE EAST EMADERS WIDED THEPE INTOONG THEMSELVES AND
Loss: 10.1919
outputs.loss tensor(10.1919, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143397/1673-143397-0018.flac
Waveform stats - mean: -0.0000, std: 0.1039, min: -0.7340, max: 0.7478
Resampled waveform stats - mean: -0.0000, std: 0.1039, min: -0.7340, max: 0.7478
Raw mel spectrogram stats - mean: 4.0411, std: 39.3015, min: 0.0000, max: 1798.2683
Log mel spectrogram stats - mean: -4.6899, std: 3.6253, min: -13.4847, max: 7.4946
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4259, max: 3.3609
Mel spec shape: torch.Size([1, 80, 427])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4258, max: 3.3613
CNN output shape: torch.Size([1, 512, 27])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
input_embeds.shape torch.Size([1, 46, 3584])
labels.shape torch.Size([1, 46])
outputs.logits.shape torch.Size([1, 46, 152064])

Sample prediction:
Target: AT THE SAME TIME EVERY AVENUE OF THE THRONE WAS ASSAULTED WITH GOLD
Prediction: 0000000000000000000000000000 END TIME,ONEIUE IN THE SAMEIFT  BAILEDED BY THEEN
Loss: 9.5213
outputs.loss tensor(9.5213, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0025.flac
Waveform stats - mean: 0.0000, std: 0.0552, min: -0.4910, max: 0.4702
Resampled waveform stats - mean: 0.0000, std: 0.0552, min: -0.4910, max: 0.4702
Raw mel spectrogram stats - mean: 1.1407, std: 12.8041, min: 0.0000, max: 994.6199
Log mel spectrogram stats - mean: -6.5666, std: 4.5274, min: -13.8155, max: 6.9024
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6011, max: 2.9750
Mel spec shape: torch.Size([1, 80, 524])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6016, max: 2.9746
CNN output shape: torch.Size([1, 512, 33])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
input_embeds.shape torch.Size([1, 57, 3584])
labels.shape torch.Size([1, 57])
outputs.logits.shape torch.Size([1, 57, 152064])

Sample prediction:
Target: INDEED WE OF THE RANK AND FILE HAD LITTLE CONFIDENCE IN GRANT IN THOSE DAYS
Prediction: 000000000000000000000000000000000000 ARET 0ING THE OF0 TOITTLE TOIDENCE IN THEANDING THEOSE DAYS.
Loss: 10.1283
outputs.loss tensor(10.1283, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149897/2277-149897-0031.flac
Waveform stats - mean: -0.0000, std: 0.0664, min: -0.5182, max: 0.4912
Resampled waveform stats - mean: -0.0000, std: 0.0664, min: -0.5182, max: 0.4912
Raw mel spectrogram stats - mean: 1.6547, std: 14.1791, min: 0.0000, max: 854.5742
Log mel spectrogram stats - mean: -5.5632, std: 3.8656, min: -13.7590, max: 6.7506
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1202, max: 3.1855
Mel spec shape: torch.Size([1, 80, 441])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1211, max: 3.1855
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
input_embeds.shape torch.Size([1, 50, 3584])
labels.shape torch.Size([1, 50])
outputs.logits.shape torch.Size([1, 50, 152064])

Sample prediction:
Target: HE TROUBLED OVER MANY LITTLE DETAILS AND TALKED PERFUNCTORILY TO EVERYBODY
Prediction: 0000000000000000000000000000000LES0 0INES  0ROING ABOUTIODCTIY ABOUT THEONE ABOUT
Loss: 10.8711
outputs.loss tensor(10.8711, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0011.flac
Waveform stats - mean: 0.0000, std: 0.0548, min: -0.7114, max: 0.5067
Resampled waveform stats - mean: 0.0000, std: 0.0548, min: -0.7114, max: 0.5067
Raw mel spectrogram stats - mean: 1.1252, std: 11.1844, min: 0.0000, max: 756.7948
Log mel spectrogram stats - mean: -5.2951, std: 3.6569, min: -13.7894, max: 6.6291
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3228, max: 3.2607
Mel spec shape: torch.Size([1, 80, 222])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3223, max: 3.2598
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
input_embeds.shape torch.Size([1, 24, 3584])
labels.shape torch.Size([1, 24])
outputs.logits.shape torch.Size([1, 24, 152064])

Sample prediction:
Target: YES THE COUNTRY IS FULL OF CAVES
Prediction: 000000000000000 0 OF2 OF FOWES AND
Loss: 11.5239
outputs.loss tensor(11.5239, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0028.flac
Waveform stats - mean: -0.0002, std: 0.1003, min: -0.7424, max: 0.6854
Resampled waveform stats - mean: -0.0002, std: 0.1003, min: -0.7424, max: 0.6854
Raw mel spectrogram stats - mean: 3.4625, std: 20.2767, min: 0.0000, max: 1147.6718
Log mel spectrogram stats - mean: -5.8843, std: 5.2445, min: -13.7966, max: 7.0455
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5087, max: 2.4654
Mel spec shape: torch.Size([1, 80, 257])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5088, max: 2.4648
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 29, 3584])
labels.shape torch.Size([1, 29])
outputs.logits.shape torch.Size([1, 29, 152064])

Sample prediction:
Target: HE SOON LEFT CHARLESTON OUT OF SIGHT
Prediction: 00000000000000000000 THEACTSTON. OF THE0.
Loss: 10.0504
outputs.loss tensor(10.0504, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0030.flac
Waveform stats - mean: 0.0000, std: 0.0550, min: -0.5347, max: 0.3321
Resampled waveform stats - mean: 0.0000, std: 0.0550, min: -0.5347, max: 0.3321
Raw mel spectrogram stats - mean: 1.0995, std: 4.7024, min: 0.0000, max: 152.0550
Log mel spectrogram stats - mean: -4.6727, std: 4.0427, min: -13.5998, max: 5.0242
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2082, max: 2.3986
Mel spec shape: torch.Size([1, 80, 285])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2090, max: 2.3984
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
input_embeds.shape torch.Size([1, 36, 3584])
labels.shape torch.Size([1, 36])
outputs.logits.shape torch.Size([1, 36, 152064])

Sample prediction:
Target: SEASON WITH SALT AND PEPPER AND A LITTLE SUGAR TO TASTE
Prediction: 00000000000000000000 0  PEPPER NO S SITTLE SALTAR. MAKEASTE.
Loss: 11.0046
outputs.loss tensor(11.0046, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0034.flac
Waveform stats - mean: -0.0007, std: 0.0693, min: -0.4151, max: 0.3866
Resampled waveform stats - mean: -0.0007, std: 0.0693, min: -0.4151, max: 0.3866
Raw mel spectrogram stats - mean: 1.7957, std: 13.1836, min: 0.0000, max: 332.0038
Log mel spectrogram stats - mean: -6.1694, std: 3.9160, min: -13.5773, max: 5.8051
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8917, max: 3.0578
Mel spec shape: torch.Size([1, 80, 253])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8916, max: 3.0586
CNN output shape: torch.Size([1, 512, 16])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
input_embeds.shape torch.Size([1, 27, 3584])
labels.shape torch.Size([1, 27])
outputs.logits.shape torch.Size([1, 27, 152064])

Sample prediction:
Target: IT'S GOT TO BE A CLEAN BREAK HILDA
Prediction: 00000000000000000 NOT  BE  ER FROMULY 
Loss: 8.8227
outputs.loss tensor(8.8227, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0069.flac
Waveform stats - mean: 0.0008, std: 0.0172, min: -0.1077, max: 0.1374
Resampled waveform stats - mean: 0.0008, std: 0.0172, min: -0.1077, max: 0.1374
Raw mel spectrogram stats - mean: 0.1082, std: 0.8856, min: 0.0000, max: 31.0559
Log mel spectrogram stats - mean: -6.6952, std: 2.9919, min: -13.7774, max: 3.4358
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3672, max: 3.3862
Mel spec shape: torch.Size([1, 80, 372])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3672, max: 3.3867
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 32, 3584])
labels.shape torch.Size([1, 32])
outputs.logits.shape torch.Size([1, 32, 152064])

Sample prediction:
Target: VERY GOOD MANAGE IT AS YOU WILL
Prediction: 00000000000000000000000000000 A W0
Loss: 9.5175
outputs.loss tensor(9.5175, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0006.flac
Waveform stats - mean: -0.0000, std: 0.0444, min: -0.3619, max: 0.3241
Resampled waveform stats - mean: -0.0000, std: 0.0444, min: -0.3619, max: 0.3241
Raw mel spectrogram stats - mean: 0.7121, std: 3.9635, min: 0.0000, max: 127.8060
Log mel spectrogram stats - mean: -6.7611, std: 4.3920, min: -13.7734, max: 4.8505
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5966, max: 2.6438
Mel spec shape: torch.Size([1, 80, 655])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5967, max: 2.6445
CNN output shape: torch.Size([1, 512, 41])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
input_embeds.shape torch.Size([1, 67, 3584])
labels.shape torch.Size([1, 67])
outputs.logits.shape torch.Size([1, 67, 152064])

Sample prediction:
Target: WHEN HE REACHED THE SUBURBS THE LIGHT OF HOMES WAS SHINING THROUGH CURTAINS OF ALL COLORS
Prediction: 0000000000000000000000000000000000000000000 ISACH THE 0B OF0S THEOPEL THE ONINING ON THETAINS OF THE THE.
Loss: 9.5842
outputs.loss tensor(9.5842, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0010.flac
Waveform stats - mean: -0.0000, std: 0.0326, min: -0.2572, max: 0.3165
Resampled waveform stats - mean: -0.0000, std: 0.0326, min: -0.2572, max: 0.3165
Raw mel spectrogram stats - mean: 0.3980, std: 4.6555, min: 0.0000, max: 253.0236
Log mel spectrogram stats - mean: -8.6114, std: 3.6224, min: -13.8043, max: 5.5335
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4335, max: 3.9048
Mel spec shape: torch.Size([1, 80, 304])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4336, max: 3.9043
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 26, 3584])
labels.shape torch.Size([1, 26])
outputs.logits.shape torch.Size([1, 26, 152064])

Sample prediction:
Target: SO HE CARES HUH
Prediction: 00000000000000000000YFUL SO0H H
Loss: 10.5192
outputs.loss tensor(10.5192, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0010.flac
Waveform stats - mean: 0.0138, std: 0.0665, min: -0.3951, max: 0.4048
Resampled waveform stats - mean: 0.0138, std: 0.0665, min: -0.3951, max: 0.4048
Raw mel spectrogram stats - mean: 1.6812, std: 11.1461, min: 0.0000, max: 375.9078
Log mel spectrogram stats - mean: -5.7896, std: 4.4515, min: -13.7797, max: 5.9293
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7949, max: 2.6326
Mel spec shape: torch.Size([1, 80, 386])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7949, max: 2.6328
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
input_embeds.shape torch.Size([1, 42, 3584])
labels.shape torch.Size([1, 42])
outputs.logits.shape torch.Size([1, 42, 152064])

Sample prediction:
Target: I REMEMBERED WHAT THE CONDUCTOR HAD SAID ABOUT HER EYES
Prediction: 000000000000000000000000000ED  I FLICTOR SAAD SAID. THE.IGHT.
Loss: 10.3733
outputs.loss tensor(10.3733, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/24615/5338-24615-0012.flac
Waveform stats - mean: -0.0000, std: 0.0650, min: -0.5542, max: 0.5916
Resampled waveform stats - mean: -0.0000, std: 0.0650, min: -0.5542, max: 0.5916
Raw mel spectrogram stats - mean: 1.5816, std: 13.4528, min: 0.0000, max: 785.0848
Log mel spectrogram stats - mean: -5.3295, std: 3.9608, min: -13.7482, max: 6.6658
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1255, max: 3.0285
Mel spec shape: torch.Size([1, 80, 431])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1250, max: 3.0293
CNN output shape: torch.Size([1, 512, 27])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
input_embeds.shape torch.Size([1, 43, 3584])
labels.shape torch.Size([1, 43])
outputs.logits.shape torch.Size([1, 43, 152064])

Sample prediction:
Target: THIS WORK OF ART WAS THE WONDER OF THE COUNTRY TEN MILES ROUND
Prediction: 0000000000000000000000000000S  IS2 BESTDERFUL THE WORLDIV. YEARSILES FROM IT
Loss: 10.7327
outputs.loss tensor(10.7327, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0019.flac
Waveform stats - mean: -0.0002, std: 0.0769, min: -0.3829, max: 0.4585
Resampled waveform stats - mean: -0.0002, std: 0.0769, min: -0.3829, max: 0.4585
Raw mel spectrogram stats - mean: 2.2113, std: 14.8483, min: 0.0000, max: 464.6429
Log mel spectrogram stats - mean: -4.9221, std: 4.0019, min: -13.6412, max: 6.1413
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1787, max: 2.7645
Mel spec shape: torch.Size([1, 80, 632])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1797, max: 2.7637
CNN output shape: torch.Size([1, 512, 40])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
input_embeds.shape torch.Size([1, 77, 3584])
labels.shape torch.Size([1, 77])
outputs.logits.shape torch.Size([1, 77, 152064])

Sample prediction:
Target: THERE WERE TEARS IN THE BOYS EYES WHEN THEY PARTED AND THE MOTHER CRIED FOR THE INDIAN BOY WHO WAS RETURNING TO HIS PEOPLE
Prediction: 000000000000000000000000000000000000000000ERE A IN THE Y YES. HE WED FROM0 BOOTHERSAMEED OUT HER LJAN BOYS. WAS LED FROM THE HOME.
Loss: 10.2467
outputs.loss tensor(10.2467, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0015.flac
Waveform stats - mean: -0.0001, std: 0.0632, min: -0.5365, max: 0.4511
Resampled waveform stats - mean: -0.0001, std: 0.0632, min: -0.5365, max: 0.4511
Raw mel spectrogram stats - mean: 1.4263, std: 8.9870, min: 0.0000, max: 1214.7031
Log mel spectrogram stats - mean: -5.1914, std: 4.2047, min: -13.6959, max: 7.1023
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0226, max: 2.9238
Mel spec shape: torch.Size([1, 80, 1181])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0234, max: 2.9238
CNN output shape: torch.Size([1, 512, 74])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
input_embeds.shape torch.Size([1, 136, 3584])
labels.shape torch.Size([1, 136])
outputs.logits.shape torch.Size([1, 136, 152064])

Sample prediction:
Target: PUT THE PULP INTO A BASIN WITH TWO OUNCES OF MELTED BUTTER TWO TABLESPOONFULS OF LEMON JUICE HALF A POUND OF CHESTNUTS BOILED AND GRATED AND SEASONING OF SALT AND WHITE PEPPER TO TASTE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000 0P IN THE BIL OF A HILES OF POLT BUTTER. OS OFONS OFS OF BUTEMON JUICE AND A COUND OF SENUTS IL IN MATED 0ASONED WITH SALT AND PE PEPPER GETASTE.
Loss: 11.7267
outputs.loss tensor(11.7267, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0016.flac
Waveform stats - mean: -0.0000, std: 0.0445, min: -0.4662, max: 0.5023
Resampled waveform stats - mean: -0.0000, std: 0.0445, min: -0.4662, max: 0.5023
Raw mel spectrogram stats - mean: 0.7431, std: 7.4834, min: 0.0000, max: 546.6875
Log mel spectrogram stats - mean: -5.3575, std: 3.5256, min: -13.8103, max: 6.3039
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3975, max: 3.3076
Mel spec shape: torch.Size([1, 80, 748])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3984, max: 3.3066
CNN output shape: torch.Size([1, 512, 47])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
input_embeds.shape torch.Size([1, 84, 3584])
labels.shape torch.Size([1, 84])
outputs.logits.shape torch.Size([1, 84, 152064])

Sample prediction:
Target: THE BOYS WERE NOW ALL ANXIETY TO START WHILE THE PONIES AFTER THEIR SUNDAY REST WERE ALMOST AS FULL OF LIFE AS WERE THEIR OWNERS
Prediction: 000000000000000000000000000000000000000000000000AT0ERE   G0  THE THE THE0ARENTIES W THE OWNLEEPAY RUN.ERE ALLREADY ALL HAPP OF ENERGY AS THEYERE THE S S.
Loss: 9.9063
outputs.loss tensor(9.9063, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147964/1993-147964-0009.flac
Waveform stats - mean: -0.0001, std: 0.0654, min: -0.4035, max: 0.3864
Resampled waveform stats - mean: -0.0001, std: 0.0654, min: -0.4035, max: 0.3864
Raw mel spectrogram stats - mean: 1.5986, std: 9.7175, min: 0.0000, max: 322.2408
Log mel spectrogram stats - mean: -4.2585, std: 3.0893, min: -13.0305, max: 5.7753
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.8395, max: 3.2480
Mel spec shape: torch.Size([1, 80, 898])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.8398, max: 3.2480
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
input_embeds.shape torch.Size([1, 100, 3584])
labels.shape torch.Size([1, 100])
outputs.logits.shape torch.Size([1, 100, 152064])

Sample prediction:
Target: FROM UNDER THE LINING HE NOW PRODUCED A COLLECTION OF BRILLIANTLY COLORED PAPER FIGURES SEVERAL INCHES HIGH AND STIFF ENOUGH TO STAND ALONE
Prediction: 0000000000000000000000000000000000000000000000000000000000  UNDERCOL OFAR DUCES A NEW OF ILLIANTLY INOLRED AGES URES.VERAL OF NUMBERES IN AND0ANDINGENOUGH TO2AND UPONE.
Loss: 10.7725
outputs.loss tensor(10.7725, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0009.flac
Waveform stats - mean: -0.0000, std: 0.0679, min: -0.4167, max: 0.4140
Resampled waveform stats - mean: -0.0000, std: 0.0679, min: -0.4167, max: 0.4140
Raw mel spectrogram stats - mean: 1.7236, std: 9.3259, min: 0.0000, max: 342.9941
Log mel spectrogram stats - mean: -5.7234, std: 4.4993, min: -13.8151, max: 5.8377
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7984, max: 2.5695
Mel spec shape: torch.Size([1, 80, 403])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7988, max: 2.5703
CNN output shape: torch.Size([1, 512, 26])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
input_embeds.shape torch.Size([1, 47, 3584])
labels.shape torch.Size([1, 47])
outputs.logits.shape torch.Size([1, 47, 152064])

Sample prediction:
Target: WE HAD TO PASS OVER THE GROUND WHERE TROOPS HAD BEEN FIGHTING ALL DAY
Prediction: 0000000000000000000000000000  BE THE  ATE  WEHEROP WAD TO BEDING. THE.
Loss: 9.7646
outputs.loss tensor(9.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0001.flac
Waveform stats - mean: -0.0001, std: 0.0505, min: -0.7274, max: 0.5343
Resampled waveform stats - mean: -0.0001, std: 0.0505, min: -0.7274, max: 0.5343
Raw mel spectrogram stats - mean: 0.9526, std: 7.7595, min: 0.0000, max: 609.7747
Log mel spectrogram stats - mean: -6.7237, std: 4.9077, min: -13.8155, max: 6.4131
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4450, max: 2.6768
Mel spec shape: torch.Size([1, 80, 976])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4453, max: 2.6777
CNN output shape: torch.Size([1, 512, 61])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31232
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31232
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 218624
Inf count: 0
audio_emb.shape torch.Size([1, 61, 3584])
input_embeds.shape torch.Size([1, 107, 3584])
labels.shape torch.Size([1, 107])
outputs.logits.shape torch.Size([1, 107, 152064])

Sample prediction:
Target: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM
Prediction: 0000000000000000000000000000000000000000000000000000000000000000,ERE NOT OWN TO W URAL GUIDICSESS OF0 PEOPLE WASVENTED ME FROM BETING THE IN DETHINGJVENIENT. I WULD NOT PRE MEASING ME AS CO THEMSEL
Loss: 9.7312
outputs.loss tensor(9.7312, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143397/1673-143397-0014.flac
Waveform stats - mean: -0.0000, std: 0.0913, min: -0.6571, max: 0.7345
Resampled waveform stats - mean: -0.0000, std: 0.0913, min: -0.6571, max: 0.7345
Raw mel spectrogram stats - mean: 3.1251, std: 34.0111, min: 0.0000, max: 2133.0017
Log mel spectrogram stats - mean: -4.8339, std: 3.5950, min: -13.3353, max: 7.6653
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3648, max: 3.4768
Mel spec shape: torch.Size([1, 80, 940])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3652, max: 3.4766
CNN output shape: torch.Size([1, 512, 59])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
input_embeds.shape torch.Size([1, 107, 3584])
labels.shape torch.Size([1, 107])
outputs.logits.shape torch.Size([1, 107, 152064])

Sample prediction:
Target: DURING A BUSY PERIOD OF THREE MONTHS THE EMPEROR TRIED EVERY METHOD EXCEPT THE MOST EFFECTUAL MEANS OF INDIFFERENCE AND CONTEMPT TO RECONCILE THIS THEOLOGICAL QUARREL
Prediction: 0000000000000000000000000000000000000000000000000000000000000 Y IOD OF TIME MONTHS,0LOYEEOR OFED TOTHING TOCEPT THE2 UNIVE TOANS OF GETFLER TO INCEPT TO MAKEPECILE THE CATLOGICAL DISARREL.
Loss: 11.5753
outputs.loss tensor(11.5753, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0013.flac
Waveform stats - mean: -0.0000, std: 0.0453, min: -0.3468, max: 0.3467
Resampled waveform stats - mean: -0.0000, std: 0.0453, min: -0.3468, max: 0.3467
Raw mel spectrogram stats - mean: 0.7677, std: 5.9491, min: 0.0000, max: 212.0373
Log mel spectrogram stats - mean: -6.4712, std: 4.0281, min: -13.6842, max: 5.3568
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7907, max: 2.9364
Mel spec shape: torch.Size([1, 80, 554])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7910, max: 2.9355
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 76, 3584])
labels.shape torch.Size([1, 76])
outputs.logits.shape torch.Size([1, 76, 152064])

Sample prediction:
Target: I GET THE PILLOWS COMFORTABLY ARRANGED ON THE FLOOR WITH A BIG BOTTLE OF SODA AND A BAG OF POPCORN WITHIN EASY REACH
Prediction: 000000000000000000000000000000000000  GRL INGORTABLELY ANGED IN THE TABLELOOR.0 GERANGLE OF COODA.0 BAG OF CHCORN. A IGHT REACH.
Loss: 10.9751
outputs.loss tensor(10.9751, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0007.flac
Waveform stats - mean: -0.0000, std: 0.0389, min: -0.3755, max: 0.4116
Resampled waveform stats - mean: -0.0000, std: 0.0389, min: -0.3755, max: 0.4116
Raw mel spectrogram stats - mean: 0.5664, std: 3.7402, min: 0.0000, max: 182.1033
Log mel spectrogram stats - mean: -5.0866, std: 3.6174, min: -13.3025, max: 5.2046
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2712, max: 2.8449
Mel spec shape: torch.Size([1, 80, 477])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2715, max: 2.8457
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: THE ASTONISHMENT WITH WHICH THE VILLAGERS REGARDED THIS MACHINE WAS OVERWHELMING
Prediction: 00000000000000000000000000000000ISHING OF0 THE WORLDICTLAGES OFARDED THE MAN WAS THATWHELMING.
Loss: 10.6152
outputs.loss tensor(10.6152, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0009.flac
Waveform stats - mean: -0.0000, std: 0.0382, min: -0.3260, max: 0.4088
Resampled waveform stats - mean: -0.0000, std: 0.0382, min: -0.3260, max: 0.4088
Raw mel spectrogram stats - mean: 0.5472, std: 3.5729, min: 0.0000, max: 174.2321
Log mel spectrogram stats - mean: -5.4073, std: 3.7184, min: -13.5807, max: 5.1604
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1981, max: 2.8420
Mel spec shape: torch.Size([1, 80, 1090])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1973, max: 2.8418
CNN output shape: torch.Size([1, 512, 69])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 35328
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 35328
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 247296
Inf count: 0
audio_emb.shape torch.Size([1, 69, 3584])
input_embeds.shape torch.Size([1, 117, 3584])
labels.shape torch.Size([1, 117])
outputs.logits.shape torch.Size([1, 117, 152064])

Sample prediction:
Target: UNKNOWN PEOPLE HAD WORKED UPON HIS FACE HE ON THE OTHER HAND HAD WORKED ON HIS MIND AND BEHIND THIS WELL EXECUTED MASK HE HAD PLACED ALL THAT HE COULD OF THOUGHT
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000AD ED ON TO THE .ARCE FACE HAND HEAD AED UP HIS FACEOUTH 0CAIND HIS M HEUTED   HAD WORKACED ON HIS HE HASULD ON HISOSEUGHTS
Loss: 9.4363
outputs.loss tensor(9.4363, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0001.flac
Waveform stats - mean: -0.0000, std: 0.0894, min: -0.5343, max: 0.6459
Resampled waveform stats - mean: -0.0000, std: 0.0894, min: -0.5343, max: 0.6459
Raw mel spectrogram stats - mean: 2.9888, std: 18.2579, min: 0.0000, max: 555.6494
Log mel spectrogram stats - mean: -5.1716, std: 4.3103, min: -13.6721, max: 6.3201
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9722, max: 2.6661
Mel spec shape: torch.Size([1, 80, 654])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 2.6660
CNN output shape: torch.Size([1, 512, 41])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
input_embeds.shape torch.Size([1, 70, 3584])
labels.shape torch.Size([1, 70])
outputs.logits.shape torch.Size([1, 70, 152064])

Sample prediction:
Target: SWEAT COVERED BRION'S BODY TRICKLING INTO THE TIGHT LOINCLOTH THAT WAS THE ONLY GARMENT HE WORE
Prediction: 00000000000000000000000000000000000000000000  ICK0  IONS  THE RONESSBB0THS0 THE BODY WAYMENT HE WORE.
Loss: 10.3168
outputs.loss tensor(10.3168, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0007.flac
Waveform stats - mean: -0.0001, std: 0.0524, min: -0.5322, max: 0.4338
Resampled waveform stats - mean: -0.0001, std: 0.0524, min: -0.5322, max: 0.4338
Raw mel spectrogram stats - mean: 0.9890, std: 4.4866, min: 0.0000, max: 151.1987
Log mel spectrogram stats - mean: -5.3744, std: 4.1250, min: -13.6678, max: 5.0186
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0105, max: 2.5195
Mel spec shape: torch.Size([1, 80, 489])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0098, max: 2.5195
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: SURROUND WITH A GARNISH OF COOKED AND DICED CARROTS TURNIPS GREEN PEAS
Prediction: 000000000000000000000000000000000  ROUNDET OF 2ING  AED ROTS.ED. BEAS AND
Loss: 10.3124
outputs.loss tensor(10.3124, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0012.flac
Waveform stats - mean: 0.0000, std: 0.0519, min: -0.4670, max: 0.4320
Resampled waveform stats - mean: 0.0000, std: 0.0519, min: -0.4670, max: 0.4320
Raw mel spectrogram stats - mean: 0.9960, std: 7.1753, min: 0.0000, max: 287.4355
Log mel spectrogram stats - mean: -6.1162, std: 3.9273, min: -13.5967, max: 5.6610
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9047, max: 2.9988
Mel spec shape: torch.Size([1, 80, 199])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9043, max: 2.9980
CNN output shape: torch.Size([1, 512, 13])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
input_embeds.shape torch.Size([1, 21, 3584])
labels.shape torch.Size([1, 21])
outputs.logits.shape torch.Size([1, 21, 152064])

Sample prediction:
Target: THERE WAS ALSO ANOTHER CAUSE
Prediction: 000000000000000 A AOTHER USE OF
Loss: 10.9175
outputs.loss tensor(10.9175, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0000.flac
Waveform stats - mean: -0.0000, std: 0.0718, min: -0.4147, max: 0.3811
Resampled waveform stats - mean: -0.0000, std: 0.0718, min: -0.4147, max: 0.3811
Raw mel spectrogram stats - mean: 1.9336, std: 13.7457, min: 0.0000, max: 681.7598
Log mel spectrogram stats - mean: -5.6101, std: 4.4850, min: -13.7823, max: 6.5247
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8221, max: 2.7056
Mel spec shape: torch.Size([1, 80, 2800])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8223, max: 2.7051
CNN output shape: torch.Size([1, 512, 175])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 89600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 89600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 627200
Inf count: 0
audio_emb.shape torch.Size([1, 175, 3584])
input_embeds.shape torch.Size([1, 288, 3584])
labels.shape torch.Size([1, 288])
outputs.logits.shape torch.Size([1, 288, 152064])

Sample prediction:
Target: WITH AN EDUCATION WHICH OUGHT TO HAVE ENSURED ME AN HONOURABLE STANDING IN THE WORLD WITH SOME INTELLIGENCE WIT GOOD LITERARY AND SCIENTIFIC KNOWLEDGE AND ENDOWED WITH THOSE ACCIDENTAL PHYSICAL QUALITIES WHICH ARE SUCH A GOOD PASSPORT INTO SOCIETY I FOUND MYSELF AT THE AGE OF TWENTY THE MEAN FOLLOWER OF A SUBLIME ART IN WHICH IF GREAT TALENT IS RIGHTLY ADMIRED MEDIOCRITY IS AS RIGHTLY DESPISED
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ATION IN ISUGHT TO BE A0URE THEAN EDUCIGORABLE AND IN THE . AN EDUCHLLIG ANDORTHHE UCKATURE ST0IENTIFIC WORKLEDGE AND AOWED WITH AOSE QUALOMAL QUALICAL QUALITIES WHICH MAKE NE AS SOURCE COMPORT TO THECIETY AS0 ITSELF IN THE END OF ENTY-0ANINGING OF THE CEDIME SC, WHICH I I MENALENTS GIVENLY USEDCHRED ANDALSCREITY IS0 ALY DISPISED AND
Loss: 10.6737
outputs.loss tensor(10.6737, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0021.flac
Waveform stats - mean: 0.0000, std: 0.0859, min: -0.7229, max: 0.5403
Resampled waveform stats - mean: 0.0000, std: 0.0859, min: -0.7229, max: 0.5403
Raw mel spectrogram stats - mean: 2.7603, std: 15.0456, min: 0.0000, max: 466.1446
Log mel spectrogram stats - mean: -5.5638, std: 4.5585, min: -13.7702, max: 6.1445
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8003, max: 2.5685
Mel spec shape: torch.Size([1, 80, 452])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7998, max: 2.5684
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: HE HAD HARDLY UTTERED THE WORDS WHEN THE BOY FLUNG HIMSELF BENEATH THE LOG
Prediction: 0000000000000000000000000000000   ANYTERED A S " HE AT WASIPP HIMSELF INTOESATH THE WINDOWIC
Loss: 9.8088
outputs.loss tensor(9.8088, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0019.flac
Waveform stats - mean: 0.0001, std: 0.1203, min: -0.6684, max: 0.6441
Resampled waveform stats - mean: 0.0001, std: 0.1203, min: -0.6684, max: 0.6441
Raw mel spectrogram stats - mean: 5.3541, std: 39.7015, min: 0.0000, max: 2288.4668
Log mel spectrogram stats - mean: -4.3030, std: 3.8185, min: -12.7837, max: 7.7356
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2209, max: 3.1527
Mel spec shape: torch.Size([1, 80, 994])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2207, max: 3.1523
CNN output shape: torch.Size([1, 512, 63])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32256
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32256
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 225792
Inf count: 0
audio_emb.shape torch.Size([1, 63, 3584])
input_embeds.shape torch.Size([1, 107, 3584])
labels.shape torch.Size([1, 107])
outputs.logits.shape torch.Size([1, 107, 152064])

Sample prediction:
Target: GIVE ME MY HORSE AND ARMS AND WAIT FOR ME HERE I WILL GO IN QUEST OF THIS KNIGHT AND DEAD OR ALIVE I WILL MAKE HIM KEEP HIS WORD PLIGHTED TO SO GREAT BEAUTY
Prediction: 00000000000000000000000000000000000000000000000000000000000000000  R  MYMY02ING ME TO 2 BE TO THEIONS THE HORIGHTS0LY ALIVE I WILL GO YOU COM HIS WORDSAIN OR AND MEUL AAUTY AND
Loss: 9.0389
outputs.loss tensor(9.0389, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0007.flac
Waveform stats - mean: -0.0001, std: 0.0768, min: -0.8047, max: 0.5139
Resampled waveform stats - mean: -0.0001, std: 0.0768, min: -0.8047, max: 0.5139
Raw mel spectrogram stats - mean: 2.2018, std: 17.1590, min: 0.0000, max: 650.6193
Log mel spectrogram stats - mean: -4.8223, std: 3.9001, min: -13.3937, max: 6.4779
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1977, max: 2.8974
Mel spec shape: torch.Size([1, 80, 265])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1973, max: 2.8965
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 26, 3584])
labels.shape torch.Size([1, 26])
outputs.logits.shape torch.Size([1, 26, 152064])

Sample prediction:
Target: AT ALL EVENTS WE SHALL GET THERE SOME DAY
Prediction: 000000000000000000   HAVE BE  WHERE 
Loss: 8.9257
outputs.loss tensor(8.9257, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/57405/6319-57405-0009.flac
Waveform stats - mean: 0.0000, std: 0.0601, min: -0.7628, max: 0.5143
Resampled waveform stats - mean: 0.0000, std: 0.0601, min: -0.7628, max: 0.5143
Raw mel spectrogram stats - mean: 1.3545, std: 13.3816, min: 0.0000, max: 1148.6533
Log mel spectrogram stats - mean: -6.1302, std: 3.8699, min: -13.7621, max: 7.0463
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9721, max: 3.4049
Mel spec shape: torch.Size([1, 80, 1544])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 3.4043
CNN output shape: torch.Size([1, 512, 97])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 49664
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 49664
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 347648
Inf count: 0
audio_emb.shape torch.Size([1, 97, 3584])
input_embeds.shape torch.Size([1, 147, 3584])
labels.shape torch.Size([1, 147])
outputs.logits.shape torch.Size([1, 147, 152064])

Sample prediction:
Target: GO ON DOWN THE MOUNTAIN SAID MERCURY AND AS YOU GO CAST THE BONES OF YOUR MOTHER OVER YOUR SHOULDERS BEHIND YOU AND WITH THESE WORDS HE LEAPED INTO THE AIR AND WAS SEEN NO MORE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000  IDDLEAIN INT THE0URY  THEK GO DOWNLE MOW. M MOUNT IN THE MERS ANDCAOLD THE AND AS YOUR BS YOU SAAPED UP THE AIR AND WITH BEN NO MORE.
Loss: 8.6741
outputs.loss tensor(8.6741, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0020.flac
Waveform stats - mean: 0.0000, std: 0.0554, min: -0.4939, max: 0.5660
Resampled waveform stats - mean: 0.0000, std: 0.0554, min: -0.4939, max: 0.5660
Raw mel spectrogram stats - mean: 1.1451, std: 6.2231, min: 0.0000, max: 238.6963
Log mel spectrogram stats - mean: -4.7726, std: 3.5952, min: -13.4325, max: 5.4752
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4087, max: 2.8504
Mel spec shape: torch.Size([1, 80, 739])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.4082, max: 2.8496
CNN output shape: torch.Size([1, 512, 47])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
input_embeds.shape torch.Size([1, 88, 3584])
labels.shape torch.Size([1, 88])
outputs.logits.shape torch.Size([1, 88, 152064])

Sample prediction:
Target: IT GAVE AN IMPOSING APPEARANCE TO MOST OF THE WHOLESALE HOUSES WHOSE OFFICES WERE UPON THE GROUND FLOOR AND IN PLAIN VIEW OF THE STREET
Prediction: 0000000000000000000000000000000000000000000000000  ERRORPO0 PEARANCE OF THE OF THE0OLE WORLDLE MARKH IN W OWNICES WERE INON THE STREETROUND FLOOR OF WHO THEAIN S. THE STREET.
Loss: 10.2157
outputs.loss tensor(10.2157, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0002.flac
Waveform stats - mean: -0.0000, std: 0.0252, min: -0.2687, max: 0.3105
Resampled waveform stats - mean: -0.0000, std: 0.0252, min: -0.2687, max: 0.3105
Raw mel spectrogram stats - mean: 0.2364, std: 2.9344, min: 0.0000, max: 216.6442
Log mel spectrogram stats - mean: -7.6396, std: 3.7141, min: -13.6512, max: 5.3783
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6186, max: 3.5050
Mel spec shape: torch.Size([1, 80, 456])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6182, max: 3.5059
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 56, 3584])
labels.shape torch.Size([1, 56])
outputs.logits.shape torch.Size([1, 56, 152064])

Sample prediction:
Target: SHE DOESN'T PICK THEM UP BUT JUST HAVING THEM IN THE ROOM SURE DOESN'T GIVE HER ASTHMA
Prediction: 0000000000000000000000000000000 NOT'T HAVE UP UP. THEY ANG A IN THE .0LYN'T PICKIVE THEM AHMA.
Loss: 9.6693
outputs.loss tensor(9.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0022.flac
Waveform stats - mean: -0.0000, std: 0.0248, min: -0.1668, max: 0.1752
Resampled waveform stats - mean: -0.0000, std: 0.0248, min: -0.1668, max: 0.1752
Raw mel spectrogram stats - mean: 0.2310, std: 1.5277, min: 0.0000, max: 62.7318
Log mel spectrogram stats - mean: -6.5233, std: 3.7471, min: -13.8155, max: 4.1389
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9461, max: 2.8455
Mel spec shape: torch.Size([1, 80, 303])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9463, max: 2.8457
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 36, 3584])
labels.shape torch.Size([1, 36])
outputs.logits.shape torch.Size([1, 36, 152064])

Sample prediction:
Target: THERE WAS AN EXTRAORDINARY FORCE OF SUGGESTION IN THIS POSTURING
Prediction: 000000000000000000000 A ERROR INARY NUMBER OF ENSEARION IN THE CASE. OF
Loss: 11.1731
outputs.loss tensor(11.1731, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0021.flac
Waveform stats - mean: -0.0000, std: 0.0582, min: -0.4488, max: 0.4551
Resampled waveform stats - mean: -0.0000, std: 0.0582, min: -0.4488, max: 0.4551
Raw mel spectrogram stats - mean: 1.2315, std: 6.6695, min: 0.0000, max: 500.6857
Log mel spectrogram stats - mean: -5.7334, std: 4.4124, min: -13.7708, max: 6.2160
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8216, max: 2.7081
Mel spec shape: torch.Size([1, 80, 818])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8213, max: 2.7090
CNN output shape: torch.Size([1, 512, 52])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26624
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26624
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 186368
Inf count: 0
audio_emb.shape torch.Size([1, 52, 3584])
input_embeds.shape torch.Size([1, 82, 3584])
labels.shape torch.Size([1, 82])
outputs.logits.shape torch.Size([1, 82, 152064])

Sample prediction:
Target: I LOVE THEE I LOVE THEE CRIED THE VIOLIN AND THE WORSHIP WAS ENTREATY THAT KNEW NOT ITSELF
Prediction: 00000000000000000000000000000000000000000000000000000 YOU 00 YOUE 0SS THEEOLINIST0 VIINDSTPER GREATIREINGED THEILLED THE THESELF.
Loss: 9.5444
outputs.loss tensor(9.5444, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0034.flac
Waveform stats - mean: -0.0000, std: 0.0427, min: -0.5695, max: 0.4752
Resampled waveform stats - mean: -0.0000, std: 0.0427, min: -0.5695, max: 0.4752
Raw mel spectrogram stats - mean: 0.6827, std: 5.9614, min: 0.0000, max: 337.1837
Log mel spectrogram stats - mean: -5.3993, std: 3.4691, min: -13.8139, max: 5.8206
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4256, max: 3.2342
Mel spec shape: torch.Size([1, 80, 797])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4258, max: 3.2344
CNN output shape: torch.Size([1, 512, 50])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
input_embeds.shape torch.Size([1, 100, 3584])
labels.shape torch.Size([1, 100])
outputs.logits.shape torch.Size([1, 100, 152064])

Sample prediction:
Target: AT THE MOMENT WHEN HE FREED HIS LEFT FOOT FROM THE STIRRUP HE THREW HIS BODY SHARPLY TO THE RIGHT REACHING FOR THE HAT WITHOUT TAKING THE PRECAUTION TO GRASP THE POMMEL
Prediction: 000000000000000000000000000000000000000000000000000 ENDENT OF I0QU FROM BODY HAND FROM THE GOVEING, WASREW HIS RIGHT INTOOPLY TO THE RIGHT ANDARING FOR THE STOOK HE THEAKING HIS HCAUTIONS LOOKAB THE HINSMEL HE
Loss: 9.9488
outputs.loss tensor(9.9488, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/24615/5338-24615-0000.flac
Waveform stats - mean: -0.0000, std: 0.0508, min: -0.5560, max: 0.5597
Resampled waveform stats - mean: -0.0000, std: 0.0508, min: -0.5560, max: 0.5597
Raw mel spectrogram stats - mean: 0.9666, std: 14.3247, min: 0.0000, max: 1375.1489
Log mel spectrogram stats - mean: -5.7574, std: 3.9982, min: -13.7156, max: 7.2263
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9904, max: 3.2474
Mel spec shape: torch.Size([1, 80, 1003])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9902, max: 3.2480
CNN output shape: torch.Size([1, 512, 63])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32256
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32256
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 225792
Inf count: 0
audio_emb.shape torch.Size([1, 63, 3584])
input_embeds.shape torch.Size([1, 115, 3584])
labels.shape torch.Size([1, 115])
outputs.logits.shape torch.Size([1, 115, 152064])

Sample prediction:
Target: IT WAS ABOUT NOON WHEN CAPTAIN WAVERLEY ENTERED THE STRAGGLING VILLAGE OR RATHER HAMLET OF TULLY VEOLAN CLOSE TO WHICH WAS SITUATED THE MANSION OF THE PROPRIETOR
Prediction: 0000000000000000000000000000000000000000000000000000000000000000 NOT .. ITAIN DELY WASED THE0ANGEGLING SHILLAGE OF0IVER THEUTLET OF THEAKIBVIGHAN. TO THE THE THEITUATED THE HOUSEOUNTION OF THE ORIETOR OF
Loss: 11.0829
outputs.loss tensor(11.0829, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/128104/1272-128104-0012.flac
Waveform stats - mean: 0.0000, std: 0.0892, min: -0.6395, max: 0.5342
Resampled waveform stats - mean: 0.0000, std: 0.0892, min: -0.6395, max: 0.5342
Raw mel spectrogram stats - mean: 2.9768, std: 20.2752, min: 0.0000, max: 700.3299
Log mel spectrogram stats - mean: -5.3317, std: 4.2547, min: -13.7209, max: 6.5516
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 2.7929
Mel spec shape: torch.Size([1, 80, 539])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 2.7930
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
input_embeds.shape torch.Size([1, 48, 3584])
labels.shape torch.Size([1, 48])
outputs.logits.shape torch.Size([1, 48, 152064])

Sample prediction:
Target: ONLY UNFORTUNATELY HIS OWN WORK NEVER DOES GET GOOD
Prediction: 0000000000000000000000000000000000000UNATELY UN0 S DONE ANY DONE RESULTS
Loss: 8.6082
outputs.loss tensor(8.6082, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0017.flac
Waveform stats - mean: -0.0000, std: 0.0562, min: -0.4849, max: 0.4352
Resampled waveform stats - mean: -0.0000, std: 0.0562, min: -0.4849, max: 0.4352
Raw mel spectrogram stats - mean: 1.1837, std: 14.0497, min: 0.0000, max: 851.3514
Log mel spectrogram stats - mean: -6.7036, std: 4.8606, min: -13.8142, max: 6.7468
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4629, max: 2.7673
Mel spec shape: torch.Size([1, 80, 194])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4629, max: 2.7676
CNN output shape: torch.Size([1, 512, 13])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
input_embeds.shape torch.Size([1, 20, 3584])
labels.shape torch.Size([1, 20])
outputs.logits.shape torch.Size([1, 20, 152064])

Sample prediction:
Target: I COULD NOT HELP MY FRIEND
Prediction: 000000000000000 BE BE YOUSELFS
Loss: 10.4179
outputs.loss tensor(10.4179, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0012.flac
Waveform stats - mean: -0.0000, std: 0.0592, min: -0.4688, max: 0.4024
Resampled waveform stats - mean: -0.0000, std: 0.0592, min: -0.4688, max: 0.4024
Raw mel spectrogram stats - mean: 1.3110, std: 15.9060, min: 0.0000, max: 664.2526
Log mel spectrogram stats - mean: -6.1308, std: 4.1577, min: -13.7431, max: 6.4987
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8309, max: 3.0376
Mel spec shape: torch.Size([1, 80, 303])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8311, max: 3.0371
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 36, 3584])
labels.shape torch.Size([1, 36])
outputs.logits.shape torch.Size([1, 36, 152064])

Sample prediction:
Target: THE QUEEN GAZED UPON OUR FRIENDS WITH EVIDENT INTEREST
Prediction: 00000000000000000000EN OF00 ATON THE KINGS WITH AYESENT INTEREST.
Loss: 10.5361
outputs.loss tensor(10.5361, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0018.flac
Waveform stats - mean: -0.0000, std: 0.0608, min: -0.3176, max: 0.5501
Resampled waveform stats - mean: -0.0000, std: 0.0608, min: -0.3176, max: 0.5501
Raw mel spectrogram stats - mean: 1.3854, std: 7.7497, min: 0.0000, max: 243.2537
Log mel spectrogram stats - mean: -6.5267, std: 4.7328, min: -13.8142, max: 5.4941
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5398, max: 2.5399
Mel spec shape: torch.Size([1, 80, 785])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5400, max: 2.5391
CNN output shape: torch.Size([1, 512, 50])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
input_embeds.shape torch.Size([1, 84, 3584])
labels.shape torch.Size([1, 84])
outputs.logits.shape torch.Size([1, 84, 152064])

Sample prediction:
Target: AND THE CAPTAIN OF COURSE CONCLUDED AFTER HAVING BEEN INTRODUCED TO KITTY THAT MISSUS NORMAN WAS A WIDOW
Prediction: 000000000000000000000000000000000000000000000000000 TUREIONS OF THESE SID THAT AVING BEEN RODUCED TO THEORETY AND HEILE.OSAN WAS A GOODORTHELY.
Loss: 10.3378
outputs.loss tensor(10.3378, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0045.flac
Waveform stats - mean: 0.0009, std: 0.0183, min: -0.1893, max: 0.2044
Resampled waveform stats - mean: 0.0009, std: 0.0183, min: -0.1893, max: 0.2044
Raw mel spectrogram stats - mean: 0.1165, std: 0.8987, min: 0.0000, max: 86.7298
Log mel spectrogram stats - mean: -6.0251, std: 2.9456, min: -12.8115, max: 4.4628
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3039, max: 3.5605
Mel spec shape: torch.Size([1, 80, 949])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3047, max: 3.5605
CNN output shape: torch.Size([1, 512, 60])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30720
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30720
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 215040
Inf count: 0
audio_emb.shape torch.Size([1, 60, 3584])
input_embeds.shape torch.Size([1, 95, 3584])
labels.shape torch.Size([1, 95])
outputs.logits.shape torch.Size([1, 95, 152064])

Sample prediction:
Target: THIS SWEETWATER AS THEY CALLED HIM WAS I HAVE SINCE UNDERSTOOD ONE OF HIS PROTEGES AND MORE OR LESS OF A FAVOURITE
Prediction: 00000000000000000000000000000000000000000000000000000000000000ET CORATER IS A SAYLED IT A A  ACE ISTOOD IT TH THE BLEINS TO0 THAN LESS T SORTOURITE OF
Loss: 9.6399
outputs.loss tensor(9.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0023.flac
Waveform stats - mean: -0.0000, std: 0.0650, min: -0.4803, max: 0.3954
Resampled waveform stats - mean: -0.0000, std: 0.0650, min: -0.4803, max: 0.3954
Raw mel spectrogram stats - mean: 1.5804, std: 14.3380, min: 0.0000, max: 609.7508
Log mel spectrogram stats - mean: -6.9101, std: 4.5129, min: -13.8154, max: 6.4131
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5301, max: 2.9522
Mel spec shape: torch.Size([1, 80, 479])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5303, max: 2.9531
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 48, 3584])
labels.shape torch.Size([1, 48])
outputs.logits.shape torch.Size([1, 48, 152064])

Sample prediction:
Target: THEY LAY IN HEAPS OF DOZENS EVEN CLOSE UP TO THE WORKS
Prediction: 000000000000000000000000000000000 IN THEAV OF 0ENS OFLY TO TO  DOING OF
Loss: 8.6765
outputs.loss tensor(8.6765, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0019.flac
Waveform stats - mean: -0.0000, std: 0.0582, min: -0.3971, max: 0.3756
Resampled waveform stats - mean: -0.0000, std: 0.0582, min: -0.3971, max: 0.3756
Raw mel spectrogram stats - mean: 1.2667, std: 8.9139, min: 0.0000, max: 385.0211
Log mel spectrogram stats - mean: -5.7177, std: 4.2027, min: -13.7666, max: 5.9533
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9152, max: 2.7770
Mel spec shape: torch.Size([1, 80, 534])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9150, max: 2.7773
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
input_embeds.shape torch.Size([1, 62, 3584])
labels.shape torch.Size([1, 62])
outputs.logits.shape torch.Size([1, 62, 152064])

Sample prediction:
Target: I PICKED IT UP AND COMING UP TO HIM JUST AS HE WAS GOING DOWN THE STEPS I HANDED IT TO HIM
Prediction: 000000000000000000000000000000000000 UP UP0 IB BACK WITH . LIKE HE WAS COMING TO TO PS.0ADDED IT TO HIM AND
Loss: 8.1506
outputs.loss tensor(8.1506, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0019.flac
Waveform stats - mean: 0.0000, std: 0.0627, min: -0.4637, max: 0.6871
Resampled waveform stats - mean: 0.0000, std: 0.0627, min: -0.4637, max: 0.6871
Raw mel spectrogram stats - mean: 1.4740, std: 10.0717, min: 0.0000, max: 489.8757
Log mel spectrogram stats - mean: -5.7431, std: 4.5183, min: -13.8132, max: 6.1942
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7861, max: 2.6420
Mel spec shape: torch.Size([1, 80, 888])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7861, max: 2.6426
CNN output shape: torch.Size([1, 512, 56])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28672
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28672
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 200704
Inf count: 0
audio_emb.shape torch.Size([1, 56, 3584])
input_embeds.shape torch.Size([1, 105, 3584])
labels.shape torch.Size([1, 105])
outputs.logits.shape torch.Size([1, 105, 152064])

Sample prediction:
Target: I FREQUENTLY THOUGHT IT WOULD BE PLEASANT TO SPLIT THE DIFFERENCE WITH THAT MULE AND I WOULD GLADLY HAVE DONE SO IF I COULD HAVE GOTTEN ONE HALF OF HIS NO
Prediction: 0000000000000000000000000000000000000000000000000000000000ENTLY OUGHT OF0OULD BE AREDASANT TO BEIT THE BER BETWEEN THE OFOTHER.0 WASOULD LIKEADLY HAVE DONE IT. I HULD HAVE DONEOTTEN THE OF OF THE MONEYSE
Loss: 9.9187
outputs.loss tensor(9.9187, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149897/2277-149897-0021.flac
Waveform stats - mean: 0.0000, std: 0.0727, min: -0.6427, max: 0.5952
Resampled waveform stats - mean: 0.0000, std: 0.0727, min: -0.6427, max: 0.5952
Raw mel spectrogram stats - mean: 1.9794, std: 18.2073, min: 0.0000, max: 1124.9679
Log mel spectrogram stats - mean: -5.2817, std: 4.0908, min: -13.7401, max: 7.0255
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0677, max: 3.0085
Mel spec shape: torch.Size([1, 80, 469])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0684, max: 3.0078
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 51, 3584])
labels.shape torch.Size([1, 51])
outputs.logits.shape torch.Size([1, 51, 152064])

Sample prediction:
Target: HE SAW A BUSY SATURDAY USHERED OUT THE SABBATH IN AND NOTHING DONE
Prediction: 0000000000000000000000000000000000Y 0ATED.INGING IN A0AWATH. THERE TO.
Loss: 8.4146
outputs.loss tensor(8.4146, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0005.flac
Waveform stats - mean: 0.0000, std: 0.0672, min: -0.4797, max: 0.6887
Resampled waveform stats - mean: 0.0000, std: 0.0672, min: -0.4797, max: 0.6887
Raw mel spectrogram stats - mean: 1.6896, std: 12.1798, min: 0.0000, max: 526.4161
Log mel spectrogram stats - mean: -6.5158, std: 4.9350, min: -13.8111, max: 6.2661
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4783, max: 2.5901
Mel spec shape: torch.Size([1, 80, 910])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4785, max: 2.5898
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
input_embeds.shape torch.Size([1, 99, 3584])
labels.shape torch.Size([1, 99])
outputs.logits.shape torch.Size([1, 99, 152064])

Sample prediction:
Target: BEES TOO STRANGE TO SAY HAD THOUGHT IT WORTH THEIR WHILE TO COME HITHER POSSIBLY FROM THE RANGE OF HIVES BESIDE SOME FARM HOUSE MILES AWAY
Prediction: 00000000000000000000000000000000000000000000000000000000000 MANYONG. BE 0 AOSEUGHTS WASOULD THE TIME LOOP0E TOAD TOIBLY  THE COM OF 0 TOEE0 THE OFLOWERSSIGHT BEAY FROM
Loss: 9.9598
outputs.loss tensor(9.9598, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0005.flac
Waveform stats - mean: 0.0000, std: 0.0818, min: -0.6386, max: 0.5517
Resampled waveform stats - mean: 0.0000, std: 0.0818, min: -0.6386, max: 0.5517
Raw mel spectrogram stats - mean: 2.4998, std: 16.3766, min: 0.0000, max: 881.6846
Log mel spectrogram stats - mean: -4.3406, std: 3.4189, min: -13.5915, max: 6.7818
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.7058, max: 3.2532
Mel spec shape: torch.Size([1, 80, 1887])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.7051, max: 3.2539
CNN output shape: torch.Size([1, 512, 118])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 60416
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 60416
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 422912
Inf count: 0
audio_emb.shape torch.Size([1, 118, 3584])
input_embeds.shape torch.Size([1, 197, 3584])
labels.shape torch.Size([1, 197])
outputs.logits.shape torch.Size([1, 197, 152064])

Sample prediction:
Target: MODE PARE AND SLICE THE CUCUMBERS AS FOR THE TABLE SPRINKLE WELL WITH SALT AND LET THEM REMAIN FOR TWENTY FOUR HOURS STRAIN OFF THE LIQUOR PACK IN JARS A THICK LAYER OF CUCUMBERS AND SALT ALTERNATELY TIE DOWN CLOSELY AND WHEN WANTED FOR USE TAKE OUT THE QUANTITY REQUIRED
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 A0 UBE0BER0 A  C OFICELES THE  THEALT AND0 THE SAIN IN THEENTY FOUR HOURS.IPING THE SQUID ANDED THEU AND.OUSENAYER OF SUCUMBER AS0ALT ANDONGATELY FORILL THE THELY THE LET THEASH TO THE W OUT THE0ART OF AND
Loss: 10.2791
outputs.loss tensor(10.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147960/2035-147960-0016.flac
Waveform stats - mean: -0.0001, std: 0.0832, min: -0.5559, max: 0.6703
Resampled waveform stats - mean: -0.0001, std: 0.0832, min: -0.5559, max: 0.6703
Raw mel spectrogram stats - mean: 2.5958, std: 20.8004, min: 0.0000, max: 988.7060
Log mel spectrogram stats - mean: -4.8081, std: 3.6756, min: -13.3064, max: 6.8964
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3121, max: 3.1844
Mel spec shape: torch.Size([1, 80, 490])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3125, max: 3.1836
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 55, 3584])
labels.shape torch.Size([1, 55])
outputs.logits.shape torch.Size([1, 55, 152064])

Sample prediction:
Target: A SNAKE OF HIS SIZE IN FIGHTING TRIM WOULD BE MORE THAN ANY BOY COULD HANDLE
Prediction: 000000000000000000000000000000000000 OWN  THEORTING AM0 BE A THAN THINGAT INULD HANDLE.
Loss: 10.2905
outputs.loss tensor(10.2905, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0000.flac
Waveform stats - mean: 0.0000, std: 0.0343, min: -0.5701, max: 0.3462
Resampled waveform stats - mean: 0.0000, std: 0.0343, min: -0.5701, max: 0.3462
Raw mel spectrogram stats - mean: 0.4391, std: 3.1848, min: 0.0000, max: 156.1952
Log mel spectrogram stats - mean: -6.0194, std: 3.6784, min: -13.3990, max: 5.0511
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0062, max: 3.0096
Mel spec shape: torch.Size([1, 80, 334])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0059, max: 3.0098
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 34, 3584])
labels.shape torch.Size([1, 34])
outputs.logits.shape torch.Size([1, 34, 152064])

Sample prediction:
Target: BUT IS LAUGHTER A SYNONYM OF JOY
Prediction: 000000000000000000000000IDER.0NONYM OF FUNY

Loss: 11.4310
outputs.loss tensor(11.4310, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0023.flac
Waveform stats - mean: -0.0000, std: 0.0355, min: -0.3726, max: 0.3769
Resampled waveform stats - mean: -0.0000, std: 0.0355, min: -0.3726, max: 0.3769
Raw mel spectrogram stats - mean: 0.4663, std: 5.7818, min: 0.0000, max: 582.8185
Log mel spectrogram stats - mean: -6.6192, std: 3.7685, min: -13.7869, max: 6.3679
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9020, max: 3.4462
Mel spec shape: torch.Size([1, 80, 1140])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9023, max: 3.4453
CNN output shape: torch.Size([1, 512, 72])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 36864
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 36864
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 258048
Inf count: 0
audio_emb.shape torch.Size([1, 72, 3584])
input_embeds.shape torch.Size([1, 124, 3584])
labels.shape torch.Size([1, 124])
outputs.logits.shape torch.Size([1, 124, 152064])

Sample prediction:
Target: BOZZLE AWAY FROM HIS OWN HOME OUT ON BUSINESS WITH HIS COAT BUTTONED OVER HIS BREAST AND HIS BEST HAT IN HIS HAND WAS AWARE THAT HE COMMANDED RESPECT AND HE COULD CARRY HIMSELF ACCORDINGLY
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000 000000  THE OWN2 AW AW AW HIS COASTS0 HAND FRIENDAT ON HIS HAND AND A B OF HE WASEDED THEPECT FROM THAT WASULD DOARRY OUTSELF WITHORDINGLY.
Loss: 10.4676
outputs.loss tensor(10.4676, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0022.flac
Waveform stats - mean: -0.0000, std: 0.0577, min: -0.2874, max: 0.7291
Resampled waveform stats - mean: -0.0000, std: 0.0577, min: -0.2874, max: 0.7291
Raw mel spectrogram stats - mean: 1.2463, std: 6.4257, min: 0.0000, max: 275.2595
Log mel spectrogram stats - mean: -6.0740, std: 4.6340, min: -13.8154, max: 5.6177
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6706, max: 2.5230
Mel spec shape: torch.Size([1, 80, 670])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6709, max: 2.5234
CNN output shape: torch.Size([1, 512, 42])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
input_embeds.shape torch.Size([1, 71, 3584])
labels.shape torch.Size([1, 71])
outputs.logits.shape torch.Size([1, 71, 152064])

Sample prediction:
Target: AFTER THAT I HAD HER MOTHER'S AUTHORITY FOR TELLING KITTY THAT SHE WOULD NEVER SEE HER FATHER AGAIN
Prediction: 00000000000000000000000000000000000000000000, HAVEAD A INDS ORIZATION TO THATAKING MEIDSTY TO SHE WASOULD NOT HAVE HER AGAINATHER AGAIN.
Loss: 9.6884
outputs.loss tensor(9.6884, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/57405/6319-57405-0006.flac
Waveform stats - mean: -0.0000, std: 0.0645, min: -0.3473, max: 0.4117
Resampled waveform stats - mean: -0.0000, std: 0.0645, min: -0.3473, max: 0.4117
Raw mel spectrogram stats - mean: 1.5572, std: 10.7364, min: 0.0000, max: 451.4763
Log mel spectrogram stats - mean: -5.4070, std: 3.7828, min: -13.6488, max: 6.1125
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1787, max: 3.0452
Mel spec shape: torch.Size([1, 80, 890])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1797, max: 3.0449
CNN output shape: torch.Size([1, 512, 56])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28672
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28672
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 200704
Inf count: 0
audio_emb.shape torch.Size([1, 56, 3584])
input_embeds.shape torch.Size([1, 93, 3584])
labels.shape torch.Size([1, 93])
outputs.logits.shape torch.Size([1, 93, 152064])

Sample prediction:
Target: BUT DEUCALION AND PYRRHA WERE VERY SAD FOR THEY KNEW THAT THEY WERE THE ONLY PERSONS WHO WERE LEFT ALIVE IN ALL THE LAND
Prediction: 00000000000000000000000000000000000000000000000000000000000000 THH00  DIFFLOWLY THE WNEW THAT THEY WERE VERY MOST TWOS WHO COERE THE ALIVE. THE OF WORLDS
Loss: 8.7746
outputs.loss tensor(8.7746, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0055.flac
Waveform stats - mean: 0.0000, std: 0.1018, min: -0.6396, max: 0.4347
Resampled waveform stats - mean: 0.0000, std: 0.1018, min: -0.6396, max: 0.4347
Raw mel spectrogram stats - mean: 3.8826, std: 23.9060, min: 0.0000, max: 592.6193
Log mel spectrogram stats - mean: -5.9247, std: 4.5803, min: -13.6122, max: 6.3846
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6784, max: 2.6874
Mel spec shape: torch.Size([1, 80, 218])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6787, max: 2.6875
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
input_embeds.shape torch.Size([1, 20, 3584])
labels.shape torch.Size([1, 20])
outputs.logits.shape torch.Size([1, 20, 152064])

Sample prediction:
Target: THIS IS MURDEROUS
Prediction: 000000000000000 AINEDER00
Loss: 9.7552
outputs.loss tensor(9.7552, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0059.flac
Waveform stats - mean: 0.0009, std: 0.0191, min: -0.1883, max: 0.2060
Resampled waveform stats - mean: 0.0009, std: 0.0191, min: -0.1883, max: 0.2060
Raw mel spectrogram stats - mean: 0.1310, std: 0.9662, min: 0.0000, max: 72.8981
Log mel spectrogram stats - mean: -6.4090, std: 3.2241, min: -13.7201, max: 4.2891
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2677, max: 3.3182
Mel spec shape: torch.Size([1, 80, 593])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2676, max: 3.3184
CNN output shape: torch.Size([1, 512, 38])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: I SUPPOSE SHE HAS BEEN CAREFULLY QUESTIONED VERY I SHOULD SAY
Prediction: 0000000000000000000000000000000000000000D IS A FULLY STUDED ON CARE0 SAY SHE
Loss: 11.5912
outputs.loss tensor(11.5912, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0005.flac
Waveform stats - mean: -0.0000, std: 0.0382, min: -0.4413, max: 0.4092
Resampled waveform stats - mean: -0.0000, std: 0.0382, min: -0.4413, max: 0.4092
Raw mel spectrogram stats - mean: 0.5456, std: 2.5479, min: 0.0000, max: 91.8729
Log mel spectrogram stats - mean: -5.9119, std: 4.0216, min: -13.6796, max: 4.5204
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9315, max: 2.5941
Mel spec shape: torch.Size([1, 80, 910])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9316, max: 2.5938
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
input_embeds.shape torch.Size([1, 97, 3584])
labels.shape torch.Size([1, 97])
outputs.logits.shape torch.Size([1, 97, 152064])

Sample prediction:
Target: THERE WAS NOTHING SAID ABOUT THE SORT OF ACCOMMODATION WHICH WOULD BE PROVIDED NOTHING ABOUT THE KIND OF ESTABLISHMENT WHICH WAS MAINTAINED OR THE TABLE WHICH WAS KEPT
Prediction: 00000000000000000000000000000000000000000000000000000000000 A TOID ABOUT THE  OF OMMODATION THAT WASOULD BE GIVEN TO WAS THE0 OF ACCABLISHMENT WHICH W TOINTAINED NOTHING0 NUMBERS WAS USEDPT NOTHING
Loss: 10.9862
outputs.loss tensor(10.9862, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0031.flac
Waveform stats - mean: 0.0000, std: 0.0394, min: -0.6429, max: 0.5655
Resampled waveform stats - mean: 0.0000, std: 0.0394, min: -0.6429, max: 0.5655
Raw mel spectrogram stats - mean: 0.5825, std: 7.2483, min: 0.0000, max: 498.9593
Log mel spectrogram stats - mean: -6.0128, std: 3.5699, min: -13.8056, max: 6.2125
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1829, max: 3.4246
Mel spec shape: torch.Size([1, 80, 543])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1836, max: 3.4238
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
input_embeds.shape torch.Size([1, 57, 3584])
labels.shape torch.Size([1, 57])
outputs.logits.shape torch.Size([1, 57, 152064])

Sample prediction:
Target: HAT TOO CLOSE TO ME I COULDN'T GET IT EXPLAINED CHUNKY THE BOYS ROARED
Prediction: 0000000000000000000000000000000000000 TO  0ULDDN'T BEL THIS TOACTAINED TOY CHUNKHY SE AT
Loss: 10.7397
outputs.loss tensor(10.7397, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/154328/2803-154328-0007.flac
Waveform stats - mean: -0.0001, std: 0.0433, min: -0.4900, max: 0.3623
Resampled waveform stats - mean: -0.0001, std: 0.0433, min: -0.4900, max: 0.3623
Raw mel spectrogram stats - mean: 0.6978, std: 4.5610, min: 0.0000, max: 208.6239
Log mel spectrogram stats - mean: -6.8054, std: 4.0848, min: -13.7627, max: 5.3405
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7032, max: 2.9734
Mel spec shape: torch.Size([1, 80, 678])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7031, max: 2.9727
CNN output shape: torch.Size([1, 512, 43])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
input_embeds.shape torch.Size([1, 65, 3584])
labels.shape torch.Size([1, 65])
outputs.logits.shape torch.Size([1, 65, 152064])

Sample prediction:
Target: JOHN YOU HAVE PROMISED MARY WHAT I PROMISED LADY HELENA WHAT IS YOUR PLAN
Prediction: 000000000000000000000000000000000000000000000 YOU ISE TOARY  YOU WILLISE YOUUCY YOU HASNA  I THE PROM FOR
Loss: 11.0856
outputs.loss tensor(11.0856, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0025.flac
Waveform stats - mean: 0.0000, std: 0.1032, min: -0.7505, max: 0.5193
Resampled waveform stats - mean: 0.0000, std: 0.1032, min: -0.7505, max: 0.5193
Raw mel spectrogram stats - mean: 3.9876, std: 22.9546, min: 0.0000, max: 697.2282
Log mel spectrogram stats - mean: -5.3797, std: 4.4269, min: -13.6661, max: 6.5471
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8718, max: 2.6942
Mel spec shape: torch.Size([1, 80, 525])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8721, max: 2.6934
CNN output shape: torch.Size([1, 512, 33])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
input_embeds.shape torch.Size([1, 63, 3584])
labels.shape torch.Size([1, 63])
outputs.logits.shape torch.Size([1, 63, 152064])

Sample prediction:
Target: KIRKLAND JUMPED FOR THE JETTY MISSED HIS FOOTING AND FELL INTO THE ARMS OF THE CHAPLAIN
Prediction: 00000000000000000000000000000000000000    UMPT SED  ..2ELL INTO THE RK OF THE JAMPLAIN.
Loss: 9.9659
outputs.loss tensor(9.9659, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170145/1462-170145-0014.flac
Waveform stats - mean: -0.0005, std: 0.0571, min: -0.2289, max: 0.3805
Resampled waveform stats - mean: -0.0005, std: 0.0571, min: -0.2289, max: 0.3805
Raw mel spectrogram stats - mean: 1.2162, std: 6.8730, min: 0.0000, max: 131.5429
Log mel spectrogram stats - mean: -6.4674, std: 4.2376, min: -13.8061, max: 4.8793
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7318, max: 2.6777
Mel spec shape: torch.Size([1, 80, 298])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7314, max: 2.6777
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 37, 3584])
labels.shape torch.Size([1, 37])
outputs.logits.shape torch.Size([1, 37, 152064])

Sample prediction:
Target: BUT WHEN I CAME I THOUGHT I HAD BEEN MISTAKEN
Prediction: 000000000000000000000 YOU WASAME BACK WASOUGHT I WAD A INISTAKEN FOR
Loss: 9.9740
outputs.loss tensor(9.9740, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0004.flac
Waveform stats - mean: -0.0000, std: 0.0693, min: -0.3972, max: 0.7811
Resampled waveform stats - mean: -0.0000, std: 0.0693, min: -0.3972, max: 0.7811
Raw mel spectrogram stats - mean: 1.7997, std: 9.3241, min: 0.0000, max: 304.8094
Log mel spectrogram stats - mean: -5.9572, std: 4.6876, min: -13.8151, max: 5.7197
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6763, max: 2.4910
Mel spec shape: torch.Size([1, 80, 1166])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6768, max: 2.4902
CNN output shape: torch.Size([1, 512, 73])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37376
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37376
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 261632
Inf count: 0
audio_emb.shape torch.Size([1, 73, 3584])
input_embeds.shape torch.Size([1, 121, 3584])
labels.shape torch.Size([1, 121])
outputs.logits.shape torch.Size([1, 121, 152064])

Sample prediction:
Target: WITH HIS OWN SUSPICIONS STEADILY CONTRADICTING HIM HE ARRIVED AT THE HOTEL OBSTINATELY BELIEVING THAT THE CHARMING WIDOW WOULD PROVE TO BE A STRANGER
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000  PIOUS OFADYY INAICTED HISSELFARIVED AT THE EL WITHEDINATEDLY CONTRIEVING HE0 HOTAIRING GIRIFEOWSENT BEBAB TO BE A CHONG TO
Loss: 10.2397
outputs.loss tensor(10.2397, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/281318/7850-281318-0018.flac
Waveform stats - mean: -0.0000, std: 0.0600, min: -0.5765, max: 0.7058
Resampled waveform stats - mean: -0.0000, std: 0.0600, min: -0.5765, max: 0.7058
Raw mel spectrogram stats - mean: 1.3461, std: 18.7838, min: 0.0000, max: 1356.7101
Log mel spectrogram stats - mean: -6.5690, std: 3.8165, min: -13.8121, max: 7.2128
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8979, max: 3.6111
Mel spec shape: torch.Size([1, 80, 473])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8975, max: 3.6113
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 49, 3584])
labels.shape torch.Size([1, 49])
outputs.logits.shape torch.Size([1, 49, 152064])

Sample prediction:
Target: CRISS CROSS CRISS CROSS SO INTERRUPTED THE WOOD PIGEON
Prediction: 0000000000000000000000000000000000RISS CROSS C0PT SO SO0IREENILESON H
Loss: 10.0290
outputs.loss tensor(10.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0021.flac
Waveform stats - mean: -0.0001, std: 0.0703, min: -0.5189, max: 0.5135
Resampled waveform stats - mean: -0.0001, std: 0.0703, min: -0.5189, max: 0.5135
Raw mel spectrogram stats - mean: 1.8427, std: 10.3930, min: 0.0000, max: 393.8627
Log mel spectrogram stats - mean: -5.2076, std: 3.9750, min: -13.2234, max: 5.9760
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0165, max: 2.8135
Mel spec shape: torch.Size([1, 80, 615])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0156, max: 2.8145
CNN output shape: torch.Size([1, 512, 39])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
input_embeds.shape torch.Size([1, 63, 3584])
labels.shape torch.Size([1, 63])
outputs.logits.shape torch.Size([1, 63, 152064])

Sample prediction:
Target: SOUR MILK OR BUTTERMILK MAY BE USED BUT THEN A LITTLE LESS ACID WILL BE NEEDED
Prediction: 0000000000000000000000000000000000000000CELE00TERILK
BE THE AS NOT YOU ITTLE BIT THANIDIC BE USEDEEDED TO
Loss: 10.6131
outputs.loss tensor(10.6131, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0013.flac
Waveform stats - mean: -0.0001, std: 0.0487, min: -0.2975, max: 0.2896
Resampled waveform stats - mean: -0.0001, std: 0.0487, min: -0.2975, max: 0.2896
Raw mel spectrogram stats - mean: 0.8860, std: 5.2165, min: 0.0000, max: 163.8503
Log mel spectrogram stats - mean: -5.4132, std: 3.2472, min: -12.6937, max: 5.0990
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2421, max: 3.2373
Mel spec shape: torch.Size([1, 80, 371])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.2383
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 35, 3584])
labels.shape torch.Size([1, 35])
outputs.logits.shape torch.Size([1, 35, 152064])

Sample prediction:
Target: INQUIRED SHAGGY IN THE METAL FOREST
Prediction: 000000000000000000000000000000 RO ST 
Loss: 10.6424
outputs.loss tensor(10.6424, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0023.flac
Waveform stats - mean: -0.0001, std: 0.0700, min: -0.4532, max: 0.3282
Resampled waveform stats - mean: -0.0001, std: 0.0700, min: -0.4532, max: 0.3282
Raw mel spectrogram stats - mean: 1.7482, std: 8.9980, min: 0.0000, max: 285.6733
Log mel spectrogram stats - mean: -5.7439, std: 4.7889, min: -13.8020, max: 5.6548
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6827, max: 2.3802
Mel spec shape: torch.Size([1, 80, 342])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6826, max: 2.3809
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
input_embeds.shape torch.Size([1, 39, 3584])
labels.shape torch.Size([1, 39])
outputs.logits.shape torch.Size([1, 39, 152064])

Sample prediction:
Target: HE HAD SEEN GREAT THINGS AND HE HAD DONE HIS SHARE OF THEM
Prediction: 000000000000000000000000 EN  INGS.2 HAD SE GREAT BEST OF THE.
Loss: 9.1435
outputs.loss tensor(9.1435, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0035.flac
Waveform stats - mean: 0.0013, std: 0.0236, min: -0.1490, max: 0.1774
Resampled waveform stats - mean: 0.0013, std: 0.0236, min: -0.1490, max: 0.1774
Raw mel spectrogram stats - mean: 0.1878, std: 1.2920, min: 0.0000, max: 108.5987
Log mel spectrogram stats - mean: -5.7757, std: 3.0432, min: -12.3234, max: 4.6877
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1516, max: 3.4382
Mel spec shape: torch.Size([1, 80, 238])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1523, max: 3.4375
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 24, 3584])
labels.shape torch.Size([1, 24])
outputs.logits.shape torch.Size([1, 24, 152064])

Sample prediction:
Target: AT LEAST THAT IS WHAT WE HOPE
Prediction: 0000000000000000AST 0 WHAT I AREPE TO
Loss: 11.0535
outputs.loss tensor(11.0535, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0019.flac
Waveform stats - mean: -0.0007, std: 0.0615, min: -0.3569, max: 0.4509
Resampled waveform stats - mean: -0.0007, std: 0.0615, min: -0.3569, max: 0.4509
Raw mel spectrogram stats - mean: 1.4173, std: 9.8077, min: 0.0000, max: 472.7876
Log mel spectrogram stats - mean: -6.0247, std: 3.9170, min: -13.8155, max: 6.1586
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9890, max: 3.1104
Mel spec shape: torch.Size([1, 80, 1025])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9893, max: 3.1113
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
input_embeds.shape torch.Size([1, 109, 3584])
labels.shape torch.Size([1, 109])
outputs.logits.shape torch.Size([1, 109, 152064])

Sample prediction:
Target: AT THAT WORD DECEPTION SPOKEN WITH SUCH SELF CONTEMPT THE COLOR FLASHED BACK INTO HILDA'S FACE AS SUDDENLY AS IF SHE HAD BEEN STRUCK BY A WHIPLASH
Prediction: 000000000000000000000000000000000000000000000000000000000000000000 THATS0 0IL  A A-SCPT THAT0FULES IN AND THEUMANARY'S E. SHEDDENLY SHE IT SHE HAD BEEN STRUCK BY A BIPPLASH.
Loss: 10.2519
outputs.loss tensor(10.2519, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0007.flac
Waveform stats - mean: -0.0000, std: 0.0637, min: -0.3165, max: 0.6448
Resampled waveform stats - mean: -0.0000, std: 0.0637, min: -0.3165, max: 0.6448
Raw mel spectrogram stats - mean: 1.5152, std: 7.6342, min: 0.0000, max: 219.8752
Log mel spectrogram stats - mean: -5.8413, std: 4.4515, min: -13.8144, max: 5.3931
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7911, max: 2.5237
Mel spec shape: torch.Size([1, 80, 450])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7910, max: 2.5234
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 46, 3584])
labels.shape torch.Size([1, 46])
outputs.logits.shape torch.Size([1, 46, 152064])

Sample prediction:
Target: AFTER MONTHS OF SEPARATION HE RECEIVED A VISIT FROM HERBERT
Prediction: 0000000000000000000000000000000S0 VERATION FROM WASIVED A IT FROM HIS M 
Loss: 10.8317
outputs.loss tensor(10.8317, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0052.flac
Waveform stats - mean: -0.0000, std: 0.1316, min: -0.9515, max: 0.7676
Resampled waveform stats - mean: -0.0000, std: 0.1316, min: -0.9515, max: 0.7676
Raw mel spectrogram stats - mean: 6.4948, std: 38.9784, min: 0.0000, max: 1749.3149
Log mel spectrogram stats - mean: -4.7839, std: 4.4551, min: -13.6599, max: 7.4670
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9923, max: 2.7498
Mel spec shape: torch.Size([1, 80, 793])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9922, max: 2.7500
CNN output shape: torch.Size([1, 512, 50])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
input_embeds.shape torch.Size([1, 88, 3584])
labels.shape torch.Size([1, 88])
outputs.logits.shape torch.Size([1, 88, 152064])

Sample prediction:
Target: NORTH KNEW WELL THAT HE WOULD NEVER DARE TO ATTEMPT ANY SUCH ACT OF VIOLENCE BUT THE INSULT STUNG HIM LIKE THE CUT OF A WHIP
Prediction: 00000000000000000000000000000000000000000000000000000B  YOU WASOULD BE BEARE TO DOTEMPT TOTHING THIVITY VIOLENCE AGAIN HE0URSAY HIM AND A B OF A KNIP.
Loss: 10.4746
outputs.loss tensor(10.4746, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0002.flac
Waveform stats - mean: -0.0000, std: 0.0499, min: -0.3438, max: 0.4270
Resampled waveform stats - mean: -0.0000, std: 0.0499, min: -0.3438, max: 0.4270
Raw mel spectrogram stats - mean: 0.9340, std: 5.6713, min: 0.0000, max: 475.3623
Log mel spectrogram stats - mean: -6.2235, std: 4.2990, min: -13.8146, max: 6.1641
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7658, max: 2.8815
Mel spec shape: torch.Size([1, 80, 1097])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7656, max: 2.8809
CNN output shape: torch.Size([1, 512, 69])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 35328
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 35328
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 247296
Inf count: 0
audio_emb.shape torch.Size([1, 69, 3584])
input_embeds.shape torch.Size([1, 124, 3584])
labels.shape torch.Size([1, 124])
outputs.logits.shape torch.Size([1, 124, 152064])

Sample prediction:
Target: I DO NOT PRETEND TO TELL OF WHAT COMMAND DISTINGUISHED ITSELF OF HEROES OF BLOOD AND WOUNDS OF SHRIEKS AND GROANS OF BRILLIANT CHARGES OF CANNON CAPTURED ET CETERA
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000 NOT USEPEND TO BEALK YOU YOU ISORTUISH YOU FROM FROM WHATES.00 AND GINE. HEROINEKKS OF0ROANS. THEUTIANCE BICKES OF THEOLDON BALLTS BYHERETERA.
Loss: 10.1797
outputs.loss tensor(10.1797, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0010.flac
Waveform stats - mean: -0.0000, std: 0.0528, min: -0.5164, max: 0.5444
Resampled waveform stats - mean: -0.0000, std: 0.0528, min: -0.5164, max: 0.5444
Raw mel spectrogram stats - mean: 1.0444, std: 10.3806, min: 0.0000, max: 562.7140
Log mel spectrogram stats - mean: -5.0678, std: 3.4738, min: -13.7972, max: 6.3328
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5129, max: 3.2819
Mel spec shape: torch.Size([1, 80, 381])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5137, max: 3.2812
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 45, 3584])
labels.shape torch.Size([1, 45])
outputs.logits.shape torch.Size([1, 45, 152064])

Sample prediction:
Target: YOU'D HAVE BOTH OF US AT THE BOTTOM IF I LEFT IT TO YOU TO TAKE CARE OF THIS END
Prediction: 00000000000000000000000000 TO  THEM  THE SAME OF YOU W YOU AT YOU' DO IT OF IT  OF
Loss: 8.1934
outputs.loss tensor(8.1934, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0029.flac
Waveform stats - mean: -0.0001, std: 0.0628, min: -0.4350, max: 0.3486
Resampled waveform stats - mean: -0.0001, std: 0.0628, min: -0.4350, max: 0.3486
Raw mel spectrogram stats - mean: 1.4656, std: 11.5361, min: 0.0000, max: 542.0304
Log mel spectrogram stats - mean: -5.5007, std: 3.8242, min: -13.4154, max: 6.2953
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0696, max: 3.0846
Mel spec shape: torch.Size([1, 80, 488])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0703, max: 3.0840
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 47, 3584])
labels.shape torch.Size([1, 47])
outputs.logits.shape torch.Size([1, 47, 152064])

Sample prediction:
Target: TO MAKE HOT BUTTERED TOAST SEVENTEEN TWENTY SIX
Prediction: 00000000000000000000000000000000   NOT  BAST.EDYEN TIMESELY- H
Loss: 9.9796
outputs.loss tensor(9.9796, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0006.flac
Waveform stats - mean: -0.0000, std: 0.0700, min: -0.7457, max: 0.7383
Resampled waveform stats - mean: -0.0000, std: 0.0700, min: -0.7457, max: 0.7383
Raw mel spectrogram stats - mean: 1.8333, std: 12.1369, min: 0.0000, max: 395.4905
Log mel spectrogram stats - mean: -5.9557, std: 4.5856, min: -13.7373, max: 5.9801
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6970, max: 2.6029
Mel spec shape: torch.Size([1, 80, 367])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6973, max: 2.6035
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
input_embeds.shape torch.Size([1, 38, 3584])
labels.shape torch.Size([1, 38])
outputs.logits.shape torch.Size([1, 38, 152064])

Sample prediction:
Target: THE ACCIDENT IN QUESTION OCCURRED UPON THE SUNDAY EVENING
Prediction: 000000000000000000000000OMAL THE UR INON THE QUESTIONITUAY OFING OF
Loss: 10.6244
outputs.loss tensor(10.6244, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0017.flac
Waveform stats - mean: -0.0000, std: 0.0536, min: -0.2987, max: 0.3195
Resampled waveform stats - mean: -0.0000, std: 0.0536, min: -0.2987, max: 0.3195
Raw mel spectrogram stats - mean: 1.0755, std: 7.4496, min: 0.0000, max: 270.2794
Log mel spectrogram stats - mean: -5.7566, std: 4.0170, min: -13.7516, max: 5.5995
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9903, max: 2.8270
Mel spec shape: torch.Size([1, 80, 502])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9902, max: 2.8262
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
input_embeds.shape torch.Size([1, 56, 3584])
labels.shape torch.Size([1, 56])
outputs.logits.shape torch.Size([1, 56, 152064])

Sample prediction:
Target: CORALIE DO YOU CONSIDER MAJESTY A PROPER WORD TO USE WHEN ADDRESSING A QUEEN
Prediction: 0000000000000000000000000000000000002SIDER THISINTORITYIC TO GOODBLEM NAME FOR USE IN TING A0EN?
Loss: 10.0908
outputs.loss tensor(10.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0005.flac
Waveform stats - mean: -0.0000, std: 0.0800, min: -0.4349, max: 0.4566
Resampled waveform stats - mean: -0.0000, std: 0.0800, min: -0.4349, max: 0.4566
Raw mel spectrogram stats - mean: 2.3928, std: 14.6177, min: 0.0000, max: 873.6524
Log mel spectrogram stats - mean: -4.9980, std: 4.0204, min: -13.7697, max: 6.7727
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1818, max: 2.9277
Mel spec shape: torch.Size([1, 80, 1276])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1816, max: 2.9277
CNN output shape: torch.Size([1, 512, 80])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 40960
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 40960
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 286720
Inf count: 0
audio_emb.shape torch.Size([1, 80, 3584])
input_embeds.shape torch.Size([1, 136, 3584])
labels.shape torch.Size([1, 136])
outputs.logits.shape torch.Size([1, 136, 152064])

Sample prediction:
Target: STILL THE ROSE TREE STOOD OUT THAT THERE MUST BE SOME GREAT ADVANTAGES IN A GARDENER'S CARE FOR SHE COULD NOT PRETEND TO BE IGNORANT OF HER OWN SUPERIORITY TO ALL HER WILD RELATIONS IN THE WOODS
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000 SAME0 0ILL   IT WAS BE0ONE THANTAGES TO THE ROARDEN'S RO OF THEPULD NOT BEPEND TO BE AORANT OF THE OWN IGNIORITY IN THE OTHER ADVIVES ANDATIONS. A ROILDS.
Loss: 9.4903
outputs.loss tensor(9.4903, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0012.flac
Waveform stats - mean: -0.0000, std: 0.0580, min: -0.3394, max: 0.3745
Resampled waveform stats - mean: -0.0000, std: 0.0580, min: -0.3394, max: 0.3745
Raw mel spectrogram stats - mean: 1.2598, std: 8.6692, min: 0.0000, max: 491.3949
Log mel spectrogram stats - mean: -5.5292, std: 4.0670, min: -13.8155, max: 6.1972
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0375, max: 2.8833
Mel spec shape: torch.Size([1, 80, 925])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0371, max: 2.8828
CNN output shape: torch.Size([1, 512, 58])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
input_embeds.shape torch.Size([1, 99, 3584])
labels.shape torch.Size([1, 99])
outputs.logits.shape torch.Size([1, 99, 152064])

Sample prediction:
Target: THEN BACK I TURNED MY FACE TO THOSE HIGH THINGS WHICH MOVED THEMSELVES TOWARDS US SO SEDATELY THEY HAD BEEN DISTANCED BY NEW WEDDED BRIDES
Prediction: 00000000000000000000000000000000000000000000000000000000000 TO AMED TO HEAD TO THE00 THEN0 I IVED ME TOVES TO0ARDS ME. THATLOWATELY AND COAD NO INANT FROM THE THAYSNES WIDES AND
Loss: 10.8765
outputs.loss tensor(10.8765, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0006.flac
Waveform stats - mean: -0.0000, std: 0.0529, min: -0.3976, max: 0.3607
Resampled waveform stats - mean: -0.0000, std: 0.0529, min: -0.3976, max: 0.3607
Raw mel spectrogram stats - mean: 1.0477, std: 8.0170, min: 0.0000, max: 390.1574
Log mel spectrogram stats - mean: -6.4280, std: 4.4898, min: -13.7762, max: 5.9666
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6366, max: 2.7606
Mel spec shape: torch.Size([1, 80, 715])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6367, max: 2.7598
CNN output shape: torch.Size([1, 512, 45])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
input_embeds.shape torch.Size([1, 79, 3584])
labels.shape torch.Size([1, 79])
outputs.logits.shape torch.Size([1, 79, 152064])

Sample prediction:
Target: WE DID THE SAME WITH PHYSICIANS WHOM WE OFTEN SENT HALF DRESSED TO SOME NOBLEMAN WHO WAS ENJOYING EXCELLENT HEALTH
Prediction: 0000000000000000000000000000000000000000000000 NOT  TH ICSALLY OM WE DIDTEN DID TO OFRESSED TO THEONE. MEN WHO WE ATRYING ACESSLENT HEALTH.
Loss: 10.4040
outputs.loss tensor(10.4040, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0020.flac
Waveform stats - mean: -0.0001, std: 0.0801, min: -0.4324, max: 0.3718
Resampled waveform stats - mean: -0.0001, std: 0.0801, min: -0.4324, max: 0.3718
Raw mel spectrogram stats - mean: 2.3960, std: 15.1226, min: 0.0000, max: 484.7261
Log mel spectrogram stats - mean: -4.6402, std: 3.6001, min: -11.9928, max: 6.1836
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0423, max: 3.0065
Mel spec shape: torch.Size([1, 80, 466])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0430, max: 3.0059
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 49, 3584])
labels.shape torch.Size([1, 49])
outputs.logits.shape torch.Size([1, 49, 152064])

Sample prediction:
Target: I DON'T BELIEVE ANN KNEW ANY MAGIC OR SHE'D HAVE WORKED IT BEFORE
Prediction: 0000000000000000000000000000000'T KNOWIEVE IN0I THING 0 WAS EVER TOED ON OUT.
Loss: 10.2180
outputs.loss tensor(10.2180, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34629/5895-34629-0028.flac
Waveform stats - mean: -0.0000, std: 0.0474, min: -0.3894, max: 0.4859
Resampled waveform stats - mean: -0.0000, std: 0.0474, min: -0.3894, max: 0.4859
Raw mel spectrogram stats - mean: 0.8394, std: 5.7804, min: 0.0000, max: 217.7774
Log mel spectrogram stats - mean: -5.1314, std: 3.8286, min: -13.7985, max: 5.3835
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2638, max: 2.7464
Mel spec shape: torch.Size([1, 80, 541])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2637, max: 2.7461
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
input_embeds.shape torch.Size([1, 60, 3584])
labels.shape torch.Size([1, 60])
outputs.logits.shape torch.Size([1, 60, 152064])

Sample prediction:
Target: HIS ENTHUSIASM CAUSED URSUS TO REMARK THIS MAN AND GWYNPLAINE TO OBSERVE HIM
Prediction: 0000000000000000000000000000000000000USIAM 0NT  EN BEEMBERABLE IS'S0ENETHYNID,0ERV THIS.
Loss: 9.8433
outputs.loss tensor(9.8433, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0017.flac
Waveform stats - mean: -0.0000, std: 0.0788, min: -0.5692, max: 0.6884
Resampled waveform stats - mean: -0.0000, std: 0.0788, min: -0.5692, max: 0.6884
Raw mel spectrogram stats - mean: 2.3171, std: 17.8288, min: 0.0000, max: 587.0830
Log mel spectrogram stats - mean: -6.0922, std: 4.4156, min: -13.6521, max: 6.3752
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7121, max: 2.8235
Mel spec shape: torch.Size([1, 80, 228])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7119, max: 2.8242
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 24, 3584])
labels.shape torch.Size([1, 24])
outputs.logits.shape torch.Size([1, 24, 152064])

Sample prediction:
Target: IT'S HARD FOR SUCH YOUNG UNS
Prediction: 0000000000000000 NOT TO ME AOUNG PEOPLEO
Loss: 11.6991
outputs.loss tensor(11.6991, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/304647/8842-304647-0010.flac
Waveform stats - mean: -0.0000, std: 0.0691, min: -0.4873, max: 0.6607
Resampled waveform stats - mean: -0.0000, std: 0.0691, min: -0.4873, max: 0.6607
Raw mel spectrogram stats - mean: 1.7896, std: 18.1806, min: 0.0000, max: 1009.5877
Log mel spectrogram stats - mean: -5.7134, std: 3.8347, min: -13.8123, max: 6.9173
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1120, max: 3.2938
Mel spec shape: torch.Size([1, 80, 1220])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1113, max: 3.2930
CNN output shape: torch.Size([1, 512, 77])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 39424
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 39424
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 275968
Inf count: 0
audio_emb.shape torch.Size([1, 77, 3584])
input_embeds.shape torch.Size([1, 131, 3584])
labels.shape torch.Size([1, 131])
outputs.logits.shape torch.Size([1, 131, 152064])

Sample prediction:
Target: WELL TOO IF HE LIKE LOVE WOULD FILCH OUR HOARD WITH PLEASURE TO OURSELVES SLUICING OUR VEIN AND VIGOUR TO PERPETUATE THE STRAIN OF LIFE BY SPILTH OF LIFE WITHIN US STORED
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000 LOW YOU IS  ELL YOULE SPE OF0LEAS. BE HOVES.ADEED OUR HOINS TO OURENIL AND OURFORMUATE OUR FUG OF THE AND THEILLLOING B AND THE THE ANDD IN
Loss: 10.1860
outputs.loss tensor(10.1860, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0021.flac
Waveform stats - mean: -0.0000, std: 0.0854, min: -0.5208, max: 0.6275
Resampled waveform stats - mean: -0.0000, std: 0.0854, min: -0.5208, max: 0.6275
Raw mel spectrogram stats - mean: 2.7306, std: 26.8288, min: 0.0000, max: 1457.0822
Log mel spectrogram stats - mean: -6.2825, std: 4.9152, min: -13.8153, max: 7.2842
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5326, max: 2.7602
Mel spec shape: torch.Size([1, 80, 1277])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5322, max: 2.7598
CNN output shape: torch.Size([1, 512, 80])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 40960
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 40960
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 286720
Inf count: 0
audio_emb.shape torch.Size([1, 80, 3584])
input_embeds.shape torch.Size([1, 138, 3584])
labels.shape torch.Size([1, 138])
outputs.logits.shape torch.Size([1, 138, 152064])

Sample prediction:
Target: REGAINING THE LOW GROUND AT THE BASE OF THE MOUNTAIN AND HOLDING ON IN YOUR GRAND ORBIT YOU PASS THROUGH A BELT OF JUNIPER WOODS CALLED THE CEDARS TO SHEEP ROCK AT THE FOOT OF THE SHASTA PASS
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000 ESTAMES   LOW OF THE OUNTAIN 2ING THE TO THE HANDIPMGAN. WILLING THE WT OF THEEWIPER BOODS ANDLED THE0EDAR AND THEP ISS THE BASE OF THE MADOWA MING
Loss: 9.5120
outputs.loss tensor(9.5120, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0034.flac
Waveform stats - mean: -0.0001, std: 0.0439, min: -0.3923, max: 0.4760
Resampled waveform stats - mean: -0.0001, std: 0.0439, min: -0.3923, max: 0.4760
Raw mel spectrogram stats - mean: 0.7198, std: 4.3191, min: 0.0000, max: 150.9607
Log mel spectrogram stats - mean: -7.4608, std: 4.7481, min: -13.7781, max: 5.0170
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3305, max: 2.6280
Mel spec shape: torch.Size([1, 80, 546])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.3301, max: 2.6289
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 53, 3584])
labels.shape torch.Size([1, 53])
outputs.logits.shape torch.Size([1, 53, 152064])

Sample prediction:
Target: THAT WAS WHAT MISSUS MACPHERSON SAID TO ME ONLY THE OTHER DAY
Prediction: 0000000000000000000000000000000000000 A IISS ROHERSON SAID. THE.  OTHER DAY.
Loss: 9.7389
outputs.loss tensor(9.7389, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0013.flac
Waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Resampled waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Raw mel spectrogram stats - mean: 1.2006, std: 7.3066, min: 0.0000, max: 190.1952
Log mel spectrogram stats - mean: -6.7061, std: 4.8185, min: -13.8154, max: 5.2481
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4754, max: 2.4809
Mel spec shape: torch.Size([1, 80, 334])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4756, max: 2.4805
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 31, 3584])
labels.shape torch.Size([1, 31])
outputs.logits.shape torch.Size([1, 31, 152064])

Sample prediction:
Target: SIT DOWN SAID MISSUS PRESTY
Prediction: 00000000000000000000000 0 ISS STON0
Loss: 10.4545
outputs.loss tensor(10.4545, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/64257/6345-64257-0013.flac
Waveform stats - mean: -0.0000, std: 0.0692, min: -0.4910, max: 0.5046
Resampled waveform stats - mean: -0.0000, std: 0.0692, min: -0.4910, max: 0.5046
Raw mel spectrogram stats - mean: 1.7944, std: 17.6194, min: 0.0000, max: 1122.2283
Log mel spectrogram stats - mean: -5.0717, std: 3.4092, min: -13.4205, max: 7.0231
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4489, max: 3.5477
Mel spec shape: torch.Size([1, 80, 630])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4492, max: 3.5469
CNN output shape: torch.Size([1, 512, 40])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
input_embeds.shape torch.Size([1, 65, 3584])
labels.shape torch.Size([1, 65])
outputs.logits.shape torch.Size([1, 65, 152064])

Sample prediction:
Target: SHE HAD LOST HIM YEARS AND YEARS BEFORE AND NOW SHE SAW HIM HE WAS THERE AND SHE KNEW HIM
Prediction: 0000000000000000000000000000000000000000000 OST SELF OF YEARS AND 0 HE ISAYS A AGAIN WAS A HE0 WASNEW HE HE
Loss: 9.0023
outputs.loss tensor(9.0023, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0020.flac
Waveform stats - mean: -0.0000, std: 0.0653, min: -0.4202, max: 0.3714
Resampled waveform stats - mean: -0.0000, std: 0.0653, min: -0.4202, max: 0.3714
Raw mel spectrogram stats - mean: 1.5967, std: 11.4748, min: 0.0000, max: 632.1790
Log mel spectrogram stats - mean: -5.6070, std: 4.4352, min: -13.8146, max: 6.4492
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8505, max: 2.7183
Mel spec shape: torch.Size([1, 80, 736])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8506, max: 2.7188
CNN output shape: torch.Size([1, 512, 46])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23552
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23552
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 164864
Inf count: 0
audio_emb.shape torch.Size([1, 46, 3584])
input_embeds.shape torch.Size([1, 83, 3584])
labels.shape torch.Size([1, 83])
outputs.logits.shape torch.Size([1, 83, 152064])

Sample prediction:
Target: MULE DID NOT DESIRE TO CROSS WHILE I WAS TRYING TO PERSUADE HIM WITH A BIG STICK A ROCK IN HIS EAR AND A TWISTER ON HIS NOSE
Prediction: 000000000000000000000000000000000000000000000000 NOT WORKCR TO BE THE  WAS INING TO CROSSULLPECTDE YOU TO MY  MICK.0 AND THE HEAD AND0 BIGIST IN HIS HEADSE.
Loss: 9.9488
outputs.loss tensor(9.9488, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0010.flac
Waveform stats - mean: -0.0001, std: 0.0373, min: -0.6063, max: 0.3764
Resampled waveform stats - mean: -0.0001, std: 0.0373, min: -0.6063, max: 0.3764
Raw mel spectrogram stats - mean: 0.5193, std: 3.6659, min: 0.0000, max: 252.9689
Log mel spectrogram stats - mean: -6.7964, std: 4.7956, min: -13.8154, max: 5.5333
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4636, max: 2.5710
Mel spec shape: torch.Size([1, 80, 795])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4639, max: 2.5703
CNN output shape: torch.Size([1, 512, 50])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
input_embeds.shape torch.Size([1, 89, 3584])
labels.shape torch.Size([1, 89])
outputs.logits.shape torch.Size([1, 89, 152064])

Sample prediction:
Target: THE CHILDREN WERE INFINITE IN NUMBER AND EXCEEDINGLY MERRY I NEED HARDLY SAY THAT THEY CAME IN FOR THEIR FULL SHARE OF THE PREVAILING BEAUTY
Prediction: 000000000000000000000000000000000000000000000000000REN OFERE  THE. NUMBER0 INACTINGLY INERC IN WAS TOER ANY ANY THE WAME TO THE THE OWN  OF THE SENTIOING AUTY OF
Loss: 9.3050
outputs.loss tensor(9.3050, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0027.flac
Waveform stats - mean: 0.0015, std: 0.2359, min: -0.7857, max: 0.7728
Resampled waveform stats - mean: 0.0015, std: 0.2359, min: -0.7857, max: 0.7728
Raw mel spectrogram stats - mean: 20.8424, std: 146.9720, min: 0.0000, max: 5295.0972
Log mel spectrogram stats - mean: -2.0302, std: 3.3416, min: -12.8277, max: 8.5745
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -3.2312, max: 3.1735
Mel spec shape: torch.Size([1, 80, 859])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -3.2305, max: 3.1738
CNN output shape: torch.Size([1, 512, 54])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27648
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27648
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 193536
Inf count: 0
audio_emb.shape torch.Size([1, 54, 3584])
input_embeds.shape torch.Size([1, 94, 3584])
labels.shape torch.Size([1, 94])
outputs.logits.shape torch.Size([1, 94, 152064])

Sample prediction:
Target: FINDING THAT LISHA SHOWED LITTLE ENTHUSIASM ON THE SUBJECT SHE TRIED TO ROUSE HIM BY PATRIOTIC APPEALS OF VARIOUS SORTS
Prediction: 0000000000000000000000000000000000000000000000000000000002ITTLEA ISS AISH INTERTHUSIASM FOR2 FIRST OF WASED TO AB IN TO TTEROTIC ORPEALS. THEARIOUS KINDS.
Loss: 11.3830
outputs.loss tensor(11.3830, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0005.flac
Waveform stats - mean: -0.0000, std: 0.0706, min: -0.7238, max: 0.6483
Resampled waveform stats - mean: -0.0000, std: 0.0706, min: -0.7238, max: 0.6483
Raw mel spectrogram stats - mean: 1.8643, std: 15.3905, min: 0.0000, max: 1014.3279
Log mel spectrogram stats - mean: -5.0028, std: 3.9691, min: -13.8088, max: 6.9220
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2186, max: 3.0044
Mel spec shape: torch.Size([1, 80, 465])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2188, max: 3.0039
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 54, 3584])
labels.shape torch.Size([1, 54])
outputs.logits.shape torch.Size([1, 54, 152064])

Sample prediction:
Target: I COULD NOT THINK OF ALLOWING ANY OF MY CHARGES TO TAKE SO TERRIBLE A RISK AND
Prediction: 00000000000000000000000000000000 NOT BE OF ANYED THING THE ANGESES TO BE ANY MUCHAKIB A ISK AS0
Loss: 9.6278
outputs.loss tensor(9.6278, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0066.flac
Waveform stats - mean: 0.0000, std: 0.0272, min: -0.1720, max: 0.1979
Resampled waveform stats - mean: 0.0000, std: 0.0272, min: -0.1720, max: 0.1979
Raw mel spectrogram stats - mean: 0.2768, std: 1.5190, min: 0.0000, max: 66.4893
Log mel spectrogram stats - mean: -6.7223, std: 3.9526, min: -13.7967, max: 4.1970
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7898, max: 2.7626
Mel spec shape: torch.Size([1, 80, 1160])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7900, max: 2.7617
CNN output shape: torch.Size([1, 512, 73])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37376
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37376
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 261632
Inf count: 0
audio_emb.shape torch.Size([1, 73, 3584])
input_embeds.shape torch.Size([1, 127, 3584])
labels.shape torch.Size([1, 127])
outputs.logits.shape torch.Size([1, 127, 152064])

Sample prediction:
Target: THERE WAS NO YOUNG MAN OF HIS AGE IN LONDON MORE WILLING AND DOCILE THAN STEPHEN SHE AFFIRMED NONE MORE AFFECTIONATE AND READY TO PLEASE AND EVEN USEFUL AS LONG AS PEOPLE DID NOT UPSET HIS POOR HEAD
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000 A WAY2G MAN0  OWN. THEONDON. THAN COM TO00 THAN THEELEN PEED. OF SOIRMCTIONATE THAN0 TO DO THAN SER MOREFUL THAN STE AS HE ARE NOT KNOWSET HIM EET M AND
Loss: 9.9176
outputs.loss tensor(9.9176, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0010.flac
Waveform stats - mean: -0.0000, std: 0.0684, min: -0.5049, max: 0.5825
Resampled waveform stats - mean: -0.0000, std: 0.0684, min: -0.5049, max: 0.5825
Raw mel spectrogram stats - mean: 1.7512, std: 13.3370, min: 0.0000, max: 492.6931
Log mel spectrogram stats - mean: -5.2454, std: 3.9372, min: -13.7813, max: 6.1999
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1680, max: 2.9070
Mel spec shape: torch.Size([1, 80, 424])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1680, max: 2.9062
CNN output shape: torch.Size([1, 512, 27])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
input_embeds.shape torch.Size([1, 46, 3584])
labels.shape torch.Size([1, 46])
outputs.logits.shape torch.Size([1, 46, 152064])

Sample prediction:
Target: I RECKON THERE ARE SMILED THE GUIDE WE ARE IN THE BEAR COUNTRY NOW
Prediction: 000000000000000000000000000000ING  00  TO ARE SM THE GUIDEAR DENUB .
Loss: 9.4944
outputs.loss tensor(9.4944, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/128104/1272-128104-0011.flac
Waveform stats - mean: -0.0000, std: 0.0672, min: -0.4365, max: 0.5557
Resampled waveform stats - mean: -0.0000, std: 0.0672, min: -0.4365, max: 0.5557
Raw mel spectrogram stats - mean: 1.6860, std: 11.3956, min: 0.0000, max: 661.2581
Log mel spectrogram stats - mean: -5.0695, std: 4.0997, min: -13.7949, max: 6.4941
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1283, max: 2.8206
Mel spec shape: torch.Size([1, 80, 1512])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1289, max: 2.8203
CNN output shape: torch.Size([1, 512, 95])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 48640
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 48640
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 340480
Inf count: 0
audio_emb.shape torch.Size([1, 95, 3584])
input_embeds.shape torch.Size([1, 160, 3584])
labels.shape torch.Size([1, 160])
outputs.logits.shape torch.Size([1, 160, 152064])

Sample prediction:
Target: IN FACT HE IS QUITE SEVERE ON MISTER RUSKIN FOR NOT RECOGNISING THAT A PICTURE SHOULD DENOTE THE FRAILTY OF MAN AND REMARKS WITH PLEASING COURTESY AND FELICITOUS GRACE THAT MANY PHASES OF FEELING
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, IS ITE AVERELY THE0 OTHSELLIN.0 ADINGGNIZ THE HE 0 IS BEOT A0EMALECTIONTY OF THE. THEINDABLE ONINLEASUREANT ACCAGEY ON ININELOITYOUSNESSACE ON THE OFENS OF THEELING AND
Loss: 10.6580
outputs.loss tensor(10.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0027.flac
Waveform stats - mean: 0.0139, std: 0.0563, min: -0.4056, max: 0.4083
Resampled waveform stats - mean: 0.0139, std: 0.0563, min: -0.4056, max: 0.4083
Raw mel spectrogram stats - mean: 1.2064, std: 7.8777, min: 0.0000, max: 337.1236
Log mel spectrogram stats - mean: -5.6158, std: 4.2454, min: -13.6900, max: 5.8204
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9019, max: 2.6938
Mel spec shape: torch.Size([1, 80, 774])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9023, max: 2.6934
CNN output shape: torch.Size([1, 512, 49])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25088
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25088
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 175616
Inf count: 0
audio_emb.shape torch.Size([1, 49, 3584])
input_embeds.shape torch.Size([1, 91, 3584])
labels.shape torch.Size([1, 91])
outputs.logits.shape torch.Size([1, 91, 152064])

Sample prediction:
Target: AFTER ANTONIA HAD SAID THE NEW WORDS OVER AND OVER SHE WANTED TO GIVE ME A LITTLE CHASED SILVER RING SHE WORE ON HER MIDDLE FINGER
Prediction: 000000000000000000000000000000000000000000000000000OTHERIO0AD ID 0 YEARS TO AND OVER AGAIN SAOULD TO LEIVE ME A HITTLE MOREANCE BUT AVER COING BUT SAANTED IT HER LEFTIDDLE FINGER AND
Loss: 10.8340
outputs.loss tensor(10.8340, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0045.flac
Waveform stats - mean: -0.0000, std: 0.0246, min: -0.1806, max: 0.2011
Resampled waveform stats - mean: -0.0000, std: 0.0246, min: -0.1806, max: 0.2011
Raw mel spectrogram stats - mean: 0.2263, std: 1.2739, min: 0.0000, max: 41.1357
Log mel spectrogram stats - mean: -7.0190, std: 4.0029, min: -13.7933, max: 3.7169
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6924, max: 2.6820
Mel spec shape: torch.Size([1, 80, 1198])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6924, max: 2.6816
CNN output shape: torch.Size([1, 512, 75])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 38400
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 38400
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 268800
Inf count: 0
audio_emb.shape torch.Size([1, 75, 3584])
input_embeds.shape torch.Size([1, 135, 3584])
labels.shape torch.Size([1, 135])
outputs.logits.shape torch.Size([1, 135, 152064])

Sample prediction:
Target: THERE ARE NATURES TOO TO WHOSE SENSE OF JUSTICE THE PRICE EXACTED LOOMS UP MONSTROUSLY ENORMOUS ODIOUS OPPRESSIVE WORRYING HUMILIATING EXTORTIONATE INTOLERABLE THOSE ARE THE FANATICS
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000 0ALLY OF0 TO TO00S NICE IS0 OFACTLY TOOSES  TOKEYERSUSLY.ORMOUSLYIOUSLYRESSIVE INICKSTING UMANSATING ORTION.LYOLERABLE INREAT WHO0 WORDUNDATICS OF
Loss: 10.5889
outputs.loss tensor(10.5889, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0017.flac
Waveform stats - mean: -0.0008, std: 0.1137, min: -0.6039, max: 0.7432
Resampled waveform stats - mean: -0.0008, std: 0.1137, min: -0.6039, max: 0.7432
Raw mel spectrogram stats - mean: 4.8398, std: 36.8237, min: 0.0000, max: 1565.9418
Log mel spectrogram stats - mean: -5.4082, std: 4.1090, min: -13.5400, max: 7.3562
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9790, max: 3.1065
Mel spec shape: torch.Size([1, 80, 232])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9795, max: 3.1074
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 24, 3584])
labels.shape torch.Size([1, 24])
outputs.logits.shape torch.Size([1, 24, 152064])

Sample prediction:
Target: I GET NOTHING BUT MISERY OUT OF EITHER
Prediction: 0000000000000000  FROM I. OF IT OF
Loss: 11.6132
outputs.loss tensor(11.6132, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0037.flac
Waveform stats - mean: -0.0001, std: 0.0847, min: -0.7878, max: 0.6761
Resampled waveform stats - mean: -0.0001, std: 0.0847, min: -0.7878, max: 0.6761
Raw mel spectrogram stats - mean: 2.3450, std: 27.5665, min: 0.0000, max: 3215.2783
Log mel spectrogram stats - mean: -6.5606, std: 4.9422, min: -13.7962, max: 8.0757
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4641, max: 2.9615
Mel spec shape: torch.Size([1, 80, 519])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4639, max: 2.9609
CNN output shape: torch.Size([1, 512, 33])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
input_embeds.shape torch.Size([1, 54, 3584])
labels.shape torch.Size([1, 54])
outputs.logits.shape torch.Size([1, 54, 152064])

Sample prediction:
Target: IF YOU'VE GOT PISTOLS JUST YOU THINK ONCE BEFORE YOU SHOOT SAID COLLINS
Prediction: 00000000000000000000000000000000000VE GOT AAINON, USE GOT YOU THE YOU YOU'OOT YOUID THEINS.
Loss: 10.2107
outputs.loss tensor(10.2107, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/154328/2803-154328-0002.flac
Waveform stats - mean: -0.0001, std: 0.0336, min: -0.2419, max: 0.1518
Resampled waveform stats - mean: -0.0001, std: 0.0336, min: -0.2419, max: 0.1518
Raw mel spectrogram stats - mean: 0.4186, std: 2.3491, min: 0.0000, max: 55.3357
Log mel spectrogram stats - mean: -7.9212, std: 3.8270, min: -13.5834, max: 4.0134
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4795, max: 3.1185
Mel spec shape: torch.Size([1, 80, 207])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4795, max: 3.1191
CNN output shape: torch.Size([1, 512, 13])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
input_embeds.shape torch.Size([1, 18, 3584])
labels.shape torch.Size([1, 18])
outputs.logits.shape torch.Size([1, 18, 152064])

Sample prediction:
Target: THE MEAL ENDED
Prediction: 00000000000000AN0TH0
Loss: 10.4768
outputs.loss tensor(10.4768, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0003.flac
Waveform stats - mean: -0.0000, std: 0.0316, min: -0.2472, max: 0.3243
Resampled waveform stats - mean: -0.0000, std: 0.0316, min: -0.2472, max: 0.3243
Raw mel spectrogram stats - mean: 0.3719, std: 1.8312, min: 0.0000, max: 77.0076
Log mel spectrogram stats - mean: -5.3995, std: 3.5600, min: -13.6552, max: 4.3439
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3190, max: 2.7369
Mel spec shape: torch.Size([1, 80, 509])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3184, max: 2.7363
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
input_embeds.shape torch.Size([1, 55, 3584])
labels.shape torch.Size([1, 55])
outputs.logits.shape torch.Size([1, 55, 152064])

Sample prediction:
Target: FROM SIXTEEN EIGHTY TO SEVENTEEN O FOUR A GREAT CHANGE HAD TAKEN PLACE
Prediction: 000000000000000000000000000000000TYEN0IGHTY- VENTEEN HODTH.M  INUND OCCAKEN PLACE IN
Loss: 9.2054
outputs.loss tensor(9.2054, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0017.flac
Waveform stats - mean: -0.0000, std: 0.0707, min: -0.3528, max: 0.6472
Resampled waveform stats - mean: -0.0000, std: 0.0707, min: -0.3528, max: 0.6472
Raw mel spectrogram stats - mean: 1.8718, std: 13.3336, min: 0.0000, max: 497.2420
Log mel spectrogram stats - mean: -5.4794, std: 4.1603, min: -13.7974, max: 6.2091
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9994, max: 2.8095
Mel spec shape: torch.Size([1, 80, 374])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9990, max: 2.8086
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 44, 3584])
labels.shape torch.Size([1, 44])
outputs.logits.shape torch.Size([1, 44, 152064])

Sample prediction:
Target: HE ASKED THE HANDLER WHO WAS KNEADING HIS ACHING MUSCLES
Prediction: 000000000000000000000000000  S TO WAS THENOE THE F0ES FUSCLES.
Loss: 9.4277
outputs.loss tensor(9.4277, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0000.flac
Waveform stats - mean: 0.0000, std: 0.0505, min: -0.5006, max: 0.3490
Resampled waveform stats - mean: 0.0000, std: 0.0505, min: -0.5006, max: 0.3490
Raw mel spectrogram stats - mean: 0.9533, std: 7.1561, min: 0.0000, max: 233.0745
Log mel spectrogram stats - mean: -6.2790, std: 4.2182, min: -13.7988, max: 5.4514
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7827, max: 2.7809
Mel spec shape: torch.Size([1, 80, 456])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7832, max: 2.7812
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 49, 3584])
labels.shape torch.Size([1, 49])
outputs.logits.shape torch.Size([1, 49, 152064])

Sample prediction:
Target: IT HAD NO ORNAMENTATION BEING EXCEEDINGLY PLAIN IN APPEARANCE
Prediction: 0000000000000000000000000000000  EFFECTIGINITAL.CA DONEHINGLY INACLY PEARANCE.
Loss: 9.6750
outputs.loss tensor(9.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/73752/7850-73752-0016.flac
Waveform stats - mean: 0.0000, std: 0.0590, min: -0.6676, max: 0.7754
Resampled waveform stats - mean: 0.0000, std: 0.0590, min: -0.6676, max: 0.7754
Raw mel spectrogram stats - mean: 1.2978, std: 9.4969, min: 0.0000, max: 556.5567
Log mel spectrogram stats - mean: -6.2927, std: 4.1237, min: -13.5938, max: 6.3218
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7705, max: 3.0590
Mel spec shape: torch.Size([1, 80, 360])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7705, max: 3.0586
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
input_embeds.shape torch.Size([1, 38, 3584])
labels.shape torch.Size([1, 38])
outputs.logits.shape torch.Size([1, 38, 152064])

Sample prediction:
Target: HE MIGHT BE ENCHANTED BUT THAT WAS THE TALISMAN
Prediction: 0000000000000000000000000 BE 0ED WITH NOT ISN ENROISMAN OF
Loss: 8.7445
outputs.loss tensor(8.7445, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0055.flac
Waveform stats - mean: 0.0009, std: 0.0205, min: -0.1520, max: 0.1901
Resampled waveform stats - mean: 0.0009, std: 0.0205, min: -0.1520, max: 0.1901
Raw mel spectrogram stats - mean: 0.1516, std: 0.9643, min: 0.0000, max: 36.8798
Log mel spectrogram stats - mean: -5.8393, std: 3.0948, min: -12.7588, max: 3.6077
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2359, max: 3.0526
Mel spec shape: torch.Size([1, 80, 388])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2363, max: 3.0527
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
input_embeds.shape torch.Size([1, 39, 3584])
labels.shape torch.Size([1, 39])
outputs.logits.shape torch.Size([1, 39, 152064])

Sample prediction:
Target: HONEST GERMANS MEN WHO HAVE PLAYED HERE FOR YEARS
Prediction: 00000000000000000000000000000ANY   W ED .0.
Loss: 9.7330
outputs.loss tensor(9.7330, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0009.flac
Waveform stats - mean: -0.0001, std: 0.0627, min: -0.5198, max: 0.6238
Resampled waveform stats - mean: -0.0001, std: 0.0627, min: -0.5198, max: 0.6238
Raw mel spectrogram stats - mean: 1.4660, std: 8.8912, min: 0.0000, max: 261.2542
Log mel spectrogram stats - mean: -7.0861, std: 4.7398, min: -13.7391, max: 5.5655
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4037, max: 2.6692
Mel spec shape: torch.Size([1, 80, 213])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4033, max: 2.6699
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
input_embeds.shape torch.Size([1, 21, 3584])
labels.shape torch.Size([1, 21])
outputs.logits.shape torch.Size([1, 21, 152064])

Sample prediction:
Target: THAT'S IT ON YOUR ACCOUNT
Prediction: 0000000000000000 ! THE COMPUTER.
Loss: 9.0255
outputs.loss tensor(9.0255, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0012.flac
Waveform stats - mean: 0.0000, std: 0.0404, min: -0.3281, max: 0.2596
Resampled waveform stats - mean: 0.0000, std: 0.0404, min: -0.3281, max: 0.2596
Raw mel spectrogram stats - mean: 0.6096, std: 3.8979, min: 0.0000, max: 147.9268
Log mel spectrogram stats - mean: -7.4057, std: 4.3893, min: -13.8152, max: 4.9967
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4603, max: 2.8256
Mel spec shape: torch.Size([1, 80, 652])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4600, max: 2.8262
CNN output shape: torch.Size([1, 512, 41])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
input_embeds.shape torch.Size([1, 61, 3584])
labels.shape torch.Size([1, 61])
outputs.logits.shape torch.Size([1, 61, 152064])

Sample prediction:
Target: THE MUSIC WAS BROKEN AND JOSEPH LEFT ALONE WITH THE DUMB INSTRUMENTS
Prediction: 000000000000000000000000000000000000000000  NOTUGHT INTO THEEPH WAS THEONE IN THE MUSICANCE GIR THEUMENTS.
Loss: 10.0710
outputs.loss tensor(10.0710, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0002.flac
Waveform stats - mean: -0.0001, std: 0.0538, min: -0.5747, max: 0.5417
Resampled waveform stats - mean: -0.0001, std: 0.0538, min: -0.5747, max: 0.5417
Raw mel spectrogram stats - mean: 1.0502, std: 7.3501, min: 0.0000, max: 854.9320
Log mel spectrogram stats - mean: -4.9607, std: 4.0061, min: -13.7844, max: 6.7510
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2026, max: 2.9234
Mel spec shape: torch.Size([1, 80, 1389])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2031, max: 2.9238
CNN output shape: torch.Size([1, 512, 87])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 44544
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 44544
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 311808
Inf count: 0
audio_emb.shape torch.Size([1, 87, 3584])
input_embeds.shape torch.Size([1, 156, 3584])
labels.shape torch.Size([1, 156])
outputs.logits.shape torch.Size([1, 156, 152064])

Sample prediction:
Target: ONE OF HIS WAITERS PHIL TYSON WAS ONE OF THE EARLIER ONES TO GO BACK INTO THE BURNED DISTRICT TO BEGIN BUSINESS AND HE OPENED A RESTAURANT CALLED THE DEL MONTE IN POWELL STREET NEAR MARKET BUT IT WAS TOO EARLY FOR SUCCESS AND CLOSED AFTER A SHORT CAREER
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 THE ERS ASEIPERS0 A OF HIS LYIER WAITES TO USE INTO TO THE ATHING BUILDRICTS GO THE. HE WASED A0AURANT CALLED THE0UXTE REST ELL..AR THEET SQUARE IT FAILED A FARLY FOR BUSINESS AND0 DOWN A FE TIMEER.
Loss: 9.6542
outputs.loss tensor(9.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0047.flac
Waveform stats - mean: 0.0004, std: 0.1866, min: -0.7989, max: 0.7275
Resampled waveform stats - mean: 0.0004, std: 0.1866, min: -0.7989, max: 0.7275
Raw mel spectrogram stats - mean: 13.0552, std: 102.5330, min: 0.0000, max: 4498.8394
Log mel spectrogram stats - mean: -2.6824, std: 3.5008, min: -13.4390, max: 8.4116
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -3.0726, max: 3.1690
Mel spec shape: torch.Size([1, 80, 1194])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -3.0723, max: 3.1699
CNN output shape: torch.Size([1, 512, 75])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 38400
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 38400
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 268800
Inf count: 0
audio_emb.shape torch.Size([1, 75, 3584])
input_embeds.shape torch.Size([1, 134, 3584])
labels.shape torch.Size([1, 134])
outputs.logits.shape torch.Size([1, 134, 152064])

Sample prediction:
Target: DAVID CAUGHT THE EXALTATION AND GAVE NO FURTHER THOUGHT TO ANY THING BUT THE DUTY OF THE HOUR FINDING HIMSELF STRONGER AND BRAVER FOR THAT LONG LOOK INTO THE ILLUMINATED FACE OF THE WOMAN HE LOVED
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000LE0 CATION OF THEAVE IT EXURTHER EXOUGHTS THETHINGO. THE EXANCEY OF THE EX.ING THESELF INANGLY THAN STRVER THAN THE. DAY AT THE0LLNESSINATED F OF THE MANAN HE LOVED.
Loss: 10.1347
outputs.loss tensor(10.1347, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/23268/3536-23268-0013.flac
Waveform stats - mean: -0.0001, std: 0.0503, min: -0.4892, max: 0.6188
Resampled waveform stats - mean: -0.0001, std: 0.0503, min: -0.4892, max: 0.6188
Raw mel spectrogram stats - mean: 0.9403, std: 10.0803, min: 0.0000, max: 826.6030
Log mel spectrogram stats - mean: -5.5227, std: 3.8141, min: -13.6713, max: 6.7173
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1364, max: 3.2091
Mel spec shape: torch.Size([1, 80, 803])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1367, max: 3.2090
CNN output shape: torch.Size([1, 512, 51])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
input_embeds.shape torch.Size([1, 86, 3584])
labels.shape torch.Size([1, 86])
outputs.logits.shape torch.Size([1, 86, 152064])

Sample prediction:
Target: DO YOU THINK I WOULD GO ANSWERED MISS MILNER WITH AN EAGERNESS THAT FOR A TIME SUPPRESSED HER TEARS IN CONTRADICTION TO HIS WILL
Prediction: 0000000000000000000000000000000000000000000000000000 KNOW YOU AMOULD BE TOERED ISSIT 0OTHERYE YES THAT WAS THE MOM WASPRESSED IT REARS AND THEASTCTION TO HER OWNING
Loss: 9.9647
outputs.loss tensor(9.9647, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302201/8842-302201-0015.flac
Waveform stats - mean: -0.0000, std: 0.0629, min: -0.6914, max: 0.7761
Resampled waveform stats - mean: -0.0000, std: 0.0629, min: -0.6914, max: 0.7761
Raw mel spectrogram stats - mean: 1.4811, std: 11.8466, min: 0.0000, max: 949.3973
Log mel spectrogram stats - mean: -5.5334, std: 4.0297, min: -13.7966, max: 6.8558
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0506, max: 3.0744
Mel spec shape: torch.Size([1, 80, 899])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0508, max: 3.0742
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
input_embeds.shape torch.Size([1, 100, 3584])
labels.shape torch.Size([1, 100])
outputs.logits.shape torch.Size([1, 100, 152064])

Sample prediction:
Target: THIS SECOND PART IS DIVIDED INTO TWO FOR IN THE ONE I SPEAK OF THE EYES WHICH ARE THE BEGINNING OF LOVE IN THE SECOND I SPEAK OF THE MOUTH WHICH IS THE END OF LOVE
Prediction: 0000000000000000000000000000000000000000000000000000000000  IS0IDED BY TWO SECESDEPEND SAME YOU HAVEAKED THE OTHERARTH AND SEE THE ENING OF THE AND THE OTHER I SPEAK OF THE EOUTH WHICH IS THE END OF LOVE 
Loss: 8.8556
outputs.loss tensor(8.8556, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0015.flac
Waveform stats - mean: -0.0000, std: 0.0664, min: -0.5290, max: 0.4469
Resampled waveform stats - mean: -0.0000, std: 0.0664, min: -0.5290, max: 0.4469
Raw mel spectrogram stats - mean: 1.6388, std: 10.6060, min: 0.0000, max: 561.4160
Log mel spectrogram stats - mean: -5.2678, std: 3.9551, min: -13.2347, max: 6.3305
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0143, max: 2.9324
Mel spec shape: torch.Size([1, 80, 985])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0137, max: 2.9316
CNN output shape: torch.Size([1, 512, 62])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31744
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31744
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 222208
Inf count: 0
audio_emb.shape torch.Size([1, 62, 3584])
input_embeds.shape torch.Size([1, 98, 3584])
labels.shape torch.Size([1, 98])
outputs.logits.shape torch.Size([1, 98, 152064])

Sample prediction:
Target: MODE BOIL THE RICE IN WATER UNTIL IT IS QUITE TENDER POUR OFF THE WATER AND PUT THE RICE BEFORE IT IS COLD TO THE FLOUR
Prediction: 00000000000000000000000000000000000000000000000000000000000000000 BEOLL  THE IN0IL IT IS ITE CAST.0ING THE ICE U IT IT RICE IN THE IS QUOLD. E WATERLOOR IN
Loss: 9.9180
outputs.loss tensor(9.9180, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93302/6345-93302-0016.flac
Waveform stats - mean: 0.0000, std: 0.0478, min: -0.3879, max: 0.3968
Resampled waveform stats - mean: 0.0000, std: 0.0478, min: -0.3879, max: 0.3968
Raw mel spectrogram stats - mean: 0.8537, std: 12.3885, min: 0.0000, max: 839.0532
Log mel spectrogram stats - mean: -7.6032, std: 4.2531, min: -13.8154, max: 6.7323
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4606, max: 3.3706
Mel spec shape: torch.Size([1, 80, 673])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4609, max: 3.3711
CNN output shape: torch.Size([1, 512, 43])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
input_embeds.shape torch.Size([1, 81, 3584])
labels.shape torch.Size([1, 81])
outputs.logits.shape torch.Size([1, 81, 152064])

Sample prediction:
Target: WHAT OPINION WOULD HE FORM OF THE PURITY OF HER MIND THE INNOCENCE OF HER SOUL IF AN INCIDENT LIKE THIS FAILED TO SHOCK HER DEEPLY
Prediction: 00000000000000000000000000000000000000000000INION ISOULD YOU HAVE   CH OF THE BEIND 0 THEOCENCE OF HER MUL  SHE IN OCC THIS OCC TO OCCOCK HER INTOEPPLY?
Loss: 9.3858
outputs.loss tensor(9.3858, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0038.flac
Waveform stats - mean: -0.0000, std: 0.0623, min: -0.4187, max: 0.2643
Resampled waveform stats - mean: -0.0000, std: 0.0623, min: -0.4187, max: 0.2643
Raw mel spectrogram stats - mean: 1.4360, std: 9.1187, min: 0.0000, max: 350.7572
Log mel spectrogram stats - mean: -5.1382, std: 3.7587, min: -13.5451, max: 5.8601
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2367, max: 2.9261
Mel spec shape: torch.Size([1, 80, 453])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2363, max: 2.9258
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 49, 3584])
labels.shape torch.Size([1, 49])
outputs.logits.shape torch.Size([1, 49, 152064])

Sample prediction:
Target: FROM FIFTEEN TO TWENTY MINUTES WILL BE REQUIRED TO BAKE THEM NICELY
Prediction: 00000000000000000000000000000000EN0 TWENTY-UTES 2  TO COMPLETEURN THE.ELY.
Loss: 10.9859
outputs.loss tensor(10.9859, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0005.flac
Waveform stats - mean: -0.0000, std: 0.0528, min: -0.3836, max: 0.3500
Resampled waveform stats - mean: -0.0000, std: 0.0528, min: -0.3836, max: 0.3500
Raw mel spectrogram stats - mean: 0.9799, std: 4.4179, min: 0.0000, max: 223.6608
Log mel spectrogram stats - mean: -4.8859, std: 4.0002, min: -13.7708, max: 5.4101
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2211, max: 2.5739
Mel spec shape: torch.Size([1, 80, 1045])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2207, max: 2.5742
CNN output shape: torch.Size([1, 512, 66])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33792
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33792
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 236544
Inf count: 0
audio_emb.shape torch.Size([1, 66, 3584])
input_embeds.shape torch.Size([1, 118, 3584])
labels.shape torch.Size([1, 118])
outputs.logits.shape torch.Size([1, 118, 152064])

Sample prediction:
Target: THIS GARISH DISPLAY OF MIRRORS AND ELABORATE DECORATION OF CEILING AND PILLARS GIVES IT THE APPEARANCE OF THE ABODE OF SATURNALIA BUT DECORUM IS THE RULE AMONG THE PATRONS
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000AGEESS USORS0 MIZORATELYORATIONS OF MLE AND2ILLARS ANDIVES A A LOOKPEARANCE OF A PALY OF THEAN.IA. THEORATION.0 WORD HEREONG THE INRONS OF
Loss: 10.0309
outputs.loss tensor(10.0309, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/84280/174-84280-0013.flac
Waveform stats - mean: -0.0001, std: 0.0606, min: -0.7924, max: 0.9333
Resampled waveform stats - mean: -0.0001, std: 0.0606, min: -0.7924, max: 0.9333
Raw mel spectrogram stats - mean: 1.3764, std: 10.3368, min: 0.0000, max: 1263.9769
Log mel spectrogram stats - mean: -5.3272, std: 4.0650, min: -13.8154, max: 7.1420
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0881, max: 3.0675
Mel spec shape: torch.Size([1, 80, 1687])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0879, max: 3.0684
CNN output shape: torch.Size([1, 512, 106])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 54272
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 54272
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 379904
Inf count: 0
audio_emb.shape torch.Size([1, 106, 3584])
input_embeds.shape torch.Size([1, 161, 3584])
labels.shape torch.Size([1, 161])
outputs.logits.shape torch.Size([1, 161, 152064])

Sample prediction:
Target: IN MARY IT SEEMS TO ME I FOUND BOTH WOMANHOOD AND FELLOWSHIP I FOUND WHAT MANY HAVE DREAMT OF LOVE AND FRIENDSHIP FREELY GIVEN AND I COULD DO NOTHING BUT CLUTCH AT HER TO MAKE HER MY POSSESSION
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 ISES TO BEET AM A OFEN ANDOD AND0EMALELOWSHIP TO0 BOTH I PEOPLE FOUNDREAMED OF BUT AND FSHIP ANDED AND AND0 FOUNDULD NOT IT BUT LOVEINGCH MY IT HAND MY HER M WSESSION.
Loss: 9.9288
outputs.loss tensor(9.9288, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0026.flac
Waveform stats - mean: -0.0000, std: 0.0746, min: -0.4731, max: 0.4647
Resampled waveform stats - mean: -0.0000, std: 0.0746, min: -0.4731, max: 0.4647
Raw mel spectrogram stats - mean: 2.0812, std: 14.1605, min: 0.0000, max: 433.9163
Log mel spectrogram stats - mean: -5.6808, std: 4.2029, min: -13.8152, max: 6.0729
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9354, max: 2.7966
Mel spec shape: torch.Size([1, 80, 664])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9355, max: 2.7969
CNN output shape: torch.Size([1, 512, 42])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
input_embeds.shape torch.Size([1, 73, 3584])
labels.shape torch.Size([1, 73])
outputs.logits.shape torch.Size([1, 73, 152064])

Sample prediction:
Target: BREATHING DEEPLY BRION SOFTLY SPOKE THE AUTO HYPNOTIC PHRASES THAT TRIGGERED THE PROCESS
Prediction: 0000000000000000000000000000000000000000000000ATH0 ING0BLY BRE0UT SO WORDBONOTHIC WORDRASE TO TO0IGGERED THE AUT OF
Loss: 10.1427
outputs.loss tensor(10.1427, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0019.flac
Waveform stats - mean: -0.0000, std: 0.0654, min: -0.4510, max: 0.4935
Resampled waveform stats - mean: -0.0000, std: 0.0654, min: -0.4510, max: 0.4935
Raw mel spectrogram stats - mean: 1.6036, std: 25.3186, min: 0.0000, max: 1397.5994
Log mel spectrogram stats - mean: -7.3354, std: 4.0219, min: -13.6946, max: 7.2425
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5811, max: 3.6246
Mel spec shape: torch.Size([1, 80, 768])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5811, max: 3.6250
CNN output shape: torch.Size([1, 512, 48])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24576
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24576
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 172032
Inf count: 0
audio_emb.shape torch.Size([1, 48, 3584])
input_embeds.shape torch.Size([1, 82, 3584])
labels.shape torch.Size([1, 82])
outputs.logits.shape torch.Size([1, 82, 152064])

Sample prediction:
Target: YOU SEE PAPA'S SO VERY RICH AND AT HOME THEY EXPECT ME TO TO GET ACQUAINTED WITH DUKES AND THINGS AND SHE STOPPED
Prediction: 0000000000000000000000000000000000000000000000000 THIS00  YOU BIGAP SO0 THE  AREED TO BE BE RQUAINTED WITH THEADES AND0OSE AND0 SAPED AND
Loss: 9.3235
outputs.loss tensor(9.3235, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0015.flac
Waveform stats - mean: 0.0009, std: 0.0191, min: -0.2130, max: 0.1644
Resampled waveform stats - mean: 0.0009, std: 0.0191, min: -0.2130, max: 0.1644
Raw mel spectrogram stats - mean: 0.1239, std: 1.5190, min: 0.0000, max: 164.0967
Log mel spectrogram stats - mean: -6.4547, std: 3.2534, min: -13.4090, max: 5.1005
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1376, max: 3.5518
Mel spec shape: torch.Size([1, 80, 273])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1367, max: 3.5527
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
input_embeds.shape torch.Size([1, 25, 3584])
labels.shape torch.Size([1, 25])
outputs.logits.shape torch.Size([1, 25, 152064])

Sample prediction:
Target: I AM LOOKING AT HIM NOW
Prediction: 0000000000000000000 NOTING FOR YOU00
Loss: 9.6986
outputs.loss tensor(9.6986, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302196/8842-302196-0001.flac
Waveform stats - mean: -0.0002, std: 0.0532, min: -0.5200, max: 0.5109
Resampled waveform stats - mean: -0.0002, std: 0.0532, min: -0.5200, max: 0.5109
Raw mel spectrogram stats - mean: 1.0406, std: 8.6495, min: 0.0000, max: 554.6061
Log mel spectrogram stats - mean: -4.4511, std: 3.3147, min: -13.2824, max: 6.3183
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.6643, max: 3.2489
Mel spec shape: torch.Size([1, 80, 936])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.6641, max: 3.2480
CNN output shape: torch.Size([1, 512, 59])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
input_embeds.shape torch.Size([1, 105, 3584])
labels.shape torch.Size([1, 105])
outputs.logits.shape torch.Size([1, 105, 152064])

Sample prediction:
Target: THIS BOOK IN ITS ORIGINAL FORM WAS RECEIVED WITH FAVOUR AND SETTLED THE CLAIM OF ROSSETTI TO RANK AS A POETIC TRANSLATOR OR INDEED AS A POET IN HIS OWN RIGHT
Prediction: 000000000000000000000000000000000000000000000000000000000000 IS  WH FORM IS0IVED BY THEAVOROURABLE APPROTLED IN QUESTIONS THEAT'S0USIN A POET OF TSLATOR. NOT ANYED AS A POET IN HIS OWN RIGHT.
Loss: 10.0496
outputs.loss tensor(10.0496, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9008/2902-9008-0006.flac
Waveform stats - mean: -0.0000, std: 0.0338, min: -0.4991, max: 0.5136
Resampled waveform stats - mean: -0.0000, std: 0.0338, min: -0.4991, max: 0.5136
Raw mel spectrogram stats - mean: 0.4277, std: 10.0199, min: 0.0000, max: 1341.5509
Log mel spectrogram stats - mean: -6.4894, std: 3.1553, min: -13.7718, max: 7.2016
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3080, max: 4.3391
Mel spec shape: torch.Size([1, 80, 1835])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3086, max: 4.3398
CNN output shape: torch.Size([1, 512, 115])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 58880
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 58880
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 412160
Inf count: 0
audio_emb.shape torch.Size([1, 115, 3584])
input_embeds.shape torch.Size([1, 188, 3584])
labels.shape torch.Size([1, 188])
outputs.logits.shape torch.Size([1, 188, 152064])

Sample prediction:
Target: TO BE WELCOMED INTO THE CELESTIAL RANKS OF THE HEROIC TO RISE TO THE IMMORTAL GODS TO THE INEFFABLE POWERS ONWARD UPWARD EVER THROUGH AGES AND THROUGH ETERNITIES TILL I FIND MY HOME AT LAST AND VANISH IN THE GLORY OF THE NAMELESS AND THE ABSOLUTE ONE
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000COMED TO THE LEBRIAL 0ING OF THE UNITEDES EANK  THE CEORTAL RS OF R RFINITEICSERS OF THE TO TO UP UP THEES AND0 THETERNITY.ILL THE AM MYSELF IN LAST IN0ISH INTO THE LIGHTORY OF THE LORD OF GOD THE UNKNOWNOLUTE..
Loss: 9.9438
outputs.loss tensor(9.9438, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0040.flac
Waveform stats - mean: -0.0000, std: 0.0649, min: -0.8073, max: 0.6531
Resampled waveform stats - mean: -0.0000, std: 0.0649, min: -0.8073, max: 0.6531
Raw mel spectrogram stats - mean: 1.5674, std: 12.5000, min: 0.0000, max: 1066.0934
Log mel spectrogram stats - mean: -5.1877, std: 3.8883, min: -13.3187, max: 6.9718
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0911, max: 3.1272
Mel spec shape: torch.Size([1, 80, 702])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0918, max: 3.1270
CNN output shape: torch.Size([1, 512, 44])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
input_embeds.shape torch.Size([1, 63, 3584])
labels.shape torch.Size([1, 63])
outputs.logits.shape torch.Size([1, 63, 152064])

Sample prediction:
Target: SUFFICIENT TO MAKE TWELVE BUNS SEASONABLE AT ANY TIME LIGHT BUNS
Prediction: 00000000000000000000000000000000000000000000000  ENTVE0UCK.VENAL.  TIME. BUNS SE
Loss: 11.2852
outputs.loss tensor(11.2852, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0005.flac
Waveform stats - mean: 0.0135, std: 0.0800, min: -0.4048, max: 0.4104
Resampled waveform stats - mean: 0.0135, std: 0.0800, min: -0.4048, max: 0.4104
Raw mel spectrogram stats - mean: 2.4072, std: 17.1022, min: 0.0000, max: 744.4454
Log mel spectrogram stats - mean: -5.7626, std: 4.4928, min: -13.5766, max: 6.6126
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7392, max: 2.7544
Mel spec shape: torch.Size([1, 80, 348])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7393, max: 2.7539
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
input_embeds.shape torch.Size([1, 34, 3584])
labels.shape torch.Size([1, 34])
outputs.logits.shape torch.Size([1, 34, 152064])

Sample prediction:
Target: VERY GLAD VERY GLAD SHE EJACULATED
Prediction: 000000000000000000000000 TO GLAD VERY WASATADULATED 
Loss: 10.1868
outputs.loss tensor(10.1868, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0015.flac
Waveform stats - mean: -0.0000, std: 0.0435, min: -0.3718, max: 0.3438
Resampled waveform stats - mean: -0.0000, std: 0.0435, min: -0.3718, max: 0.3438
Raw mel spectrogram stats - mean: 0.7078, std: 4.3257, min: 0.0000, max: 191.3521
Log mel spectrogram stats - mean: -5.4064, std: 3.6166, min: -13.5681, max: 5.2541
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2567, max: 2.9476
Mel spec shape: torch.Size([1, 80, 474])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2559, max: 2.9473
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
input_embeds.shape torch.Size([1, 52, 3584])
labels.shape torch.Size([1, 52])
outputs.logits.shape torch.Size([1, 52, 152064])

Sample prediction:
Target: SHE WANTED TO MAKE SOME REFERENCE TO THEIR RELATIONS UPON THE TRAIN BUT WAS TOO TIMID
Prediction: 000000000000000000000000000000000 TO BE  MONEYERENCE TO THE ATION.ON THE ING THE NOT AFID TO
Loss: 9.6297
outputs.loss tensor(9.6297, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0031.flac
Waveform stats - mean: -0.0001, std: 0.0412, min: -0.5442, max: 0.5065
Resampled waveform stats - mean: -0.0001, std: 0.0412, min: -0.5442, max: 0.5065
Raw mel spectrogram stats - mean: 0.6361, std: 4.4958, min: 0.0000, max: 141.5134
Log mel spectrogram stats - mean: -7.1242, std: 4.4167, min: -13.8017, max: 4.9524
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5118, max: 2.7343
Mel spec shape: torch.Size([1, 80, 271])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5117, max: 2.7344
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 29, 3584])
labels.shape torch.Size([1, 29])
outputs.logits.shape torch.Size([1, 29, 152064])

Sample prediction:
Target: SUCH IS THE SELFISHNESS OF HUMAN NATURE
Prediction: 00000000000000000000 BEST- GEN OF THEUMANITYATURE.
Loss: 10.8267
outputs.loss tensor(10.8267, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0016.flac
Waveform stats - mean: 0.0009, std: 0.0187, min: -0.1548, max: 0.1813
Resampled waveform stats - mean: 0.0009, std: 0.0187, min: -0.1548, max: 0.1813
Raw mel spectrogram stats - mean: 0.1278, std: 0.7276, min: 0.0000, max: 37.8823
Log mel spectrogram stats - mean: -5.9812, std: 3.0661, min: -12.8598, max: 3.6345
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2434, max: 3.1362
Mel spec shape: torch.Size([1, 80, 769])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2441, max: 3.1367
CNN output shape: torch.Size([1, 512, 49])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25088
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25088
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 175616
Inf count: 0
audio_emb.shape torch.Size([1, 49, 3584])
input_embeds.shape torch.Size([1, 78, 3584])
labels.shape torch.Size([1, 78])
outputs.logits.shape torch.Size([1, 78, 152064])

Sample prediction:
Target: JUST AN EVERYDAY DETECTIVE BUT AMBITIOUS I SUPPOSE AND QUITE ALIVE TO THE IMPORTANCE OF BEING THOROUGH
Prediction: 00000000000000000000000000000000000000000000000000OTHERTHING EIVE0 NOT IIOUS00POSED0OT AMOST00 ENDANCE OF THEING ALOROUGHLY
Loss: 9.9157
outputs.loss tensor(9.9157, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0011.flac
Waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Resampled waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Raw mel spectrogram stats - mean: 1.5914, std: 21.0549, min: 0.0000, max: 1490.1473
Log mel spectrogram stats - mean: -7.2657, std: 4.2950, min: -13.7301, max: 7.3066
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5051, max: 3.3928
Mel spec shape: torch.Size([1, 80, 433])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5049, max: 3.3926
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
input_embeds.shape torch.Size([1, 49, 3584])
labels.shape torch.Size([1, 49])
outputs.logits.shape torch.Size([1, 49, 152064])

Sample prediction:
Target: IS IT ONLY THAT YOU'RE POOR WHY THAT'S NOTHING I'M POOR TOO SHE LAUGHED
Prediction: 00000000000000000000000000000  A YOU CAN NOTORLY YOU YOU THE TO0 NOTOR WHY.'SUGHED AT
Loss: 9.7836
outputs.loss tensor(9.7836, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0016.flac
Waveform stats - mean: -0.0001, std: 0.0538, min: -0.3305, max: 0.5388
Resampled waveform stats - mean: -0.0001, std: 0.0538, min: -0.3305, max: 0.5388
Raw mel spectrogram stats - mean: 1.0816, std: 7.8733, min: 0.0000, max: 262.6759
Log mel spectrogram stats - mean: -5.6139, std: 3.4264, min: -13.5254, max: 5.5709
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3090, max: 3.2643
Mel spec shape: torch.Size([1, 80, 229])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3086, max: 3.2637
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 23, 3584])
labels.shape torch.Size([1, 23])
outputs.logits.shape torch.Size([1, 23, 152064])

Sample prediction:
Target: KALIKO HESITATED
Prediction: 00000000000000000AL000ATION TO
Loss: 9.2468
outputs.loss tensor(9.2468, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0006.flac
Waveform stats - mean: -0.0000, std: 0.0586, min: -0.4041, max: 0.6802
Resampled waveform stats - mean: -0.0000, std: 0.0586, min: -0.4041, max: 0.6802
Raw mel spectrogram stats - mean: 1.2832, std: 8.6502, min: 0.0000, max: 472.0110
Log mel spectrogram stats - mean: -6.7846, std: 4.6535, min: -13.8134, max: 6.1570
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5104, max: 2.7811
Mel spec shape: torch.Size([1, 80, 1036])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5107, max: 2.7812
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
input_embeds.shape torch.Size([1, 116, 3584])
labels.shape torch.Size([1, 116])
outputs.logits.shape torch.Size([1, 116, 152064])

Sample prediction:
Target: THIS WAS A FOUNTAIN SET ROUND WITH A RIM OF OLD MOSSY STONES AND PAVED IN ITS BED WITH WHAT APPEARED TO BE A SORT OF MOSAIC WORK OF VARIOUSLY COLORED PEBBLES
Prediction: 000000000000000000000000000000000000000000000000000000000000000000 A TESTORTAIN OF OF  A FOUNED  FON.0ONES.0APER RO THE CENTER A A WASPEARS TO BE A R OF RAIC OF OF STARIOUS ST SLORED STAVBLES AND
Loss: 10.0020
outputs.loss tensor(10.0020, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0026.flac
Waveform stats - mean: -0.0000, std: 0.0473, min: -0.4906, max: 0.4582
Resampled waveform stats - mean: -0.0000, std: 0.0473, min: -0.4906, max: 0.4582
Raw mel spectrogram stats - mean: 0.8344, std: 8.2421, min: 0.0000, max: 425.0260
Log mel spectrogram stats - mean: -5.3523, std: 3.6403, min: -13.7930, max: 6.0522
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3187, max: 3.1328
Mel spec shape: torch.Size([1, 80, 444])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3184, max: 3.1328
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
input_embeds.shape torch.Size([1, 57, 3584])
labels.shape torch.Size([1, 57])
outputs.logits.shape torch.Size([1, 57, 152064])

Sample prediction:
Target: HE'S A FELLOW WHO'S ALL THE TIME MAKING TROUBLE ISN'T HE ASKED STACY INNOCENTLY
Prediction: 00000000000000000000000000000000 OFSE IN THE TIME TING FUNROUBLE IN0'T HE?KING TOUP TO THEOCENTLY TO
Loss: 10.7020
outputs.loss tensor(10.7020, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0027.flac
Waveform stats - mean: 0.0000, std: 0.0946, min: -0.4301, max: 0.6797
Resampled waveform stats - mean: 0.0000, std: 0.0946, min: -0.4301, max: 0.6797
Raw mel spectrogram stats - mean: 3.3636, std: 19.0712, min: 0.0000, max: 497.9785
Log mel spectrogram stats - mean: -6.0672, std: 4.9944, min: -13.8135, max: 6.2106
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5510, max: 2.4583
Mel spec shape: torch.Size([1, 80, 405])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5508, max: 2.4590
CNN output shape: torch.Size([1, 512, 26])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
input_embeds.shape torch.Size([1, 44, 3584])
labels.shape torch.Size([1, 44])
outputs.logits.shape torch.Size([1, 44, 152064])

Sample prediction:
Target: THEIR LONG MASSIVE EARS GIVE THEM A VERY STRIKING APPEARANCE
Prediction: 0000000000000000000000000000EST OF 00ROW THEM A MASS LONGONGING LOOKPEARANCE.
Loss: 10.9618
outputs.loss tensor(10.9618, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0033.flac
Waveform stats - mean: -0.0000, std: 0.0895, min: -0.9947, max: 0.6573
Resampled waveform stats - mean: -0.0000, std: 0.0895, min: -0.9947, max: 0.6573
Raw mel spectrogram stats - mean: 2.6847, std: 20.8810, min: 0.0000, max: 2642.3950
Log mel spectrogram stats - mean: -5.3417, std: 4.7734, min: -13.7972, max: 7.8794
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7714, max: 2.7698
Mel spec shape: torch.Size([1, 80, 1026])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7715, max: 2.7695
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
input_embeds.shape torch.Size([1, 110, 3584])
labels.shape torch.Size([1, 110])
outputs.logits.shape torch.Size([1, 110, 152064])

Sample prediction:
Target: IT WAS AFTERNOON WHEN HE REACHED THE LITTLE STATION OF WINTON AND LEFT THE TRAIN A TALL STURDY BOY THE SUPERIOR OF MANY A MAN IN SIZE STRENGTH AND AGILITY
Prediction: 000000000000000000000000000000000000000000000000000000000000000000 NOT THEON  I WASACHED THE AD HOUSEATION. INDERS.0 THE TRAIN.HEADOWN MANATIONDY MANY OF AGEINT BO THE BO BO HE THE ANDANDINGNGTH AND INILITY HE
Loss: 10.1923
outputs.loss tensor(10.1923, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0017.flac
Waveform stats - mean: 0.0000, std: 0.0516, min: -0.4706, max: 0.3872
Resampled waveform stats - mean: 0.0000, std: 0.0516, min: -0.4706, max: 0.3872
Raw mel spectrogram stats - mean: 0.9928, std: 5.6093, min: 0.0000, max: 243.7174
Log mel spectrogram stats - mean: -5.0200, std: 3.3432, min: -13.2531, max: 5.4960
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4626, max: 3.1455
Mel spec shape: torch.Size([1, 80, 711])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4629, max: 3.1445
CNN output shape: torch.Size([1, 512, 45])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
input_embeds.shape torch.Size([1, 75, 3584])
labels.shape torch.Size([1, 75])
outputs.logits.shape torch.Size([1, 75, 152064])

Sample prediction:
Target: THE LEMON THIS FRUIT IS A NATIVE OF ASIA AND IS MENTIONED BY VIRGIL AS AN ANTIDOTE TO POISON
Prediction: 000000000000000000000000000000000000000000000000 ISU IS0 LURAL OF THEIA. EURO AATUREED IN THEGIL IN THE EXIDOTE TO POISON.
Loss: 10.5290
outputs.loss tensor(10.5290, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0001.flac
Waveform stats - mean: -0.0000, std: 0.0645, min: -0.4338, max: 0.3743
Resampled waveform stats - mean: -0.0000, std: 0.0645, min: -0.4338, max: 0.3743
Raw mel spectrogram stats - mean: 1.5569, std: 11.7082, min: 0.0000, max: 435.6116
Log mel spectrogram stats - mean: -8.6746, std: 5.4723, min: -13.8143, max: 6.0768
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -0.9392, max: 2.6956
Mel spec shape: torch.Size([1, 80, 366])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -0.9395, max: 2.6953
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
input_embeds.shape torch.Size([1, 35, 3584])
labels.shape torch.Size([1, 35])
outputs.logits.shape torch.Size([1, 35, 152064])

Sample prediction:
Target: YANK SAYS WHAT YOU DOING JOHNNY
Prediction: 00000000000000000000000000:  WANT NOT TOYNSY 
Loss: 9.5048
outputs.loss tensor(9.5048, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0010.flac
Waveform stats - mean: -0.0006, std: 0.0606, min: -0.2736, max: 0.3692
Resampled waveform stats - mean: -0.0006, std: 0.0606, min: -0.2736, max: 0.3692
Raw mel spectrogram stats - mean: 1.3753, std: 12.4763, min: 0.0000, max: 638.4875
Log mel spectrogram stats - mean: -6.9099, std: 3.8709, min: -13.7510, max: 6.4591
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7673, max: 3.4537
Mel spec shape: torch.Size([1, 80, 459])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7676, max: 3.4531
CNN output shape: torch.Size([1, 512, 29])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
input_embeds.shape torch.Size([1, 51, 3584])
labels.shape torch.Size([1, 51])
outputs.logits.shape torch.Size([1, 51, 152064])

Sample prediction:
Target: I'LL DO ANYTHING YOU WISH ME TO BARTLEY SHE SAID TREMULOUSLY
Prediction: 000000000000000000000000000000000THING TO WANTANT TO TO DO0ER WID.MENDOUSLY.
Loss: 10.1327
outputs.loss tensor(10.1327, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0007.flac
Waveform stats - mean: -0.0000, std: 0.0337, min: -0.5840, max: 0.4223
Resampled waveform stats - mean: -0.0000, std: 0.0337, min: -0.5840, max: 0.4223
Raw mel spectrogram stats - mean: 0.4208, std: 3.3627, min: 0.0000, max: 202.5472
Log mel spectrogram stats - mean: -6.2360, std: 3.6255, min: -13.7151, max: 5.3110
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0629, max: 3.1849
Mel spec shape: torch.Size([1, 80, 874])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0625, max: 3.1855
CNN output shape: torch.Size([1, 512, 55])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
input_embeds.shape torch.Size([1, 99, 3584])
labels.shape torch.Size([1, 99])
outputs.logits.shape torch.Size([1, 99, 152064])

Sample prediction:
Target: I MUST NOT CONCLUDE WITHOUT EXPRESSING MY MOST SINCERE THANKS TO MY CRITICS AND TO THE PUBLIC FOR THE LENIENCY AND CONSIDERATION WITH WHICH THEY HAVE TREATED MY ADVENTURES
Prediction: 00000000000000000000000000000000000000000000000000000000 BE BE0  CON CON  OP DEINCER OPS TO0 MOSTUELIC. I THE PEOPLE FOR THEIR GREATIENTENCY AND UNDERSIDERATION THEY WHICH THEY HAVE TREATED ME WORKVENTURES IN
Loss: 10.1503
outputs.loss tensor(10.1503, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275156/8297-275156-0002.flac
Waveform stats - mean: 0.0000, std: 0.0707, min: -0.3702, max: 0.7218
Resampled waveform stats - mean: 0.0000, std: 0.0707, min: -0.3702, max: 0.7218
Raw mel spectrogram stats - mean: 1.8751, std: 10.8603, min: 0.0000, max: 431.4442
Log mel spectrogram stats - mean: -5.9544, std: 4.7676, min: -13.8144, max: 6.0671
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6486, max: 2.5215
Mel spec shape: torch.Size([1, 80, 795])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6484, max: 2.5215
CNN output shape: torch.Size([1, 512, 50])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
input_embeds.shape torch.Size([1, 82, 3584])
labels.shape torch.Size([1, 82])
outputs.logits.shape torch.Size([1, 82, 152064])

Sample prediction:
Target: WE HAVE BOTH SEEN THE SAME NEWSPAPER OF COURSE AND YOU HAVE BEEN THE FIRST TO CLEAR THE THING UP THAT'S IT ISN'T IT
Prediction: 000000000000000000000000000000000000000000000000000  VER THE  NUMBER APER. THESE WE0 KNOW TO T SAME TO SEELY NEWRESH UP. WE GREAT.N'T A'S
Loss: 8.9240
outputs.loss tensor(8.9240, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9006/2902-9006-0005.flac
Waveform stats - mean: -0.0000, std: 0.0565, min: -0.6992, max: 0.6772
Resampled waveform stats - mean: -0.0000, std: 0.0565, min: -0.6992, max: 0.6772
Raw mel spectrogram stats - mean: 1.1970, std: 20.1694, min: 0.0000, max: 2962.8386
Log mel spectrogram stats - mean: -5.5075, std: 3.1994, min: -13.7898, max: 7.9939
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5887, max: 4.2199
Mel spec shape: torch.Size([1, 80, 3232])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5879, max: 4.2188
CNN output shape: torch.Size([1, 512, 202])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 103424
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 103424
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 723968
Inf count: 0
audio_emb.shape torch.Size([1, 202, 3584])
input_embeds.shape torch.Size([1, 360, 3584])
labels.shape torch.Size([1, 360])
outputs.logits.shape torch.Size([1, 360, 152064])

Sample prediction:
Target: JULIAN'S LAST ATTEMPT TO RESTORE PAGANISM BY IMPERIAL INFLUENCE HAD ONLY PROVED THAT THE OLD FAITH HAD LOST ALL HOLD UPON THE HEARTS OF THE MASSES AT HIS DEATH THE GREAT TIDE WAVE OF NEW OPINION ROLLED ON UNCHECKED AND THE RULERS OF EARTH WERE FAIN TO SWIM WITH THE STREAM TO ACCEPT IN WORDS AT LEAST THE CHURCH'S LAWS AS THEIRS TO ACKNOWLEDGE A KING OF KINGS TO WHOM EVEN THEY OWED HOMAGE AND OBEDIENCE AND TO CALL THEIR OWN SLAVES THEIR POORER BRETHREN AND OFTEN TOO THEIR SPIRITUAL SUPERIORS
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 THETEMPT TO SAVEORE THEASTANZ  JROIALISMSTITUTEUENCE.. FAILED ONED THE THE P PCTIONSSAD BEENOST ITS ITS ONON THE PEOPLEART OF OF THE PEOPLEASSES. HOME TIMEATH. OLDNESSERROR OFOULD OF REV THINIONS ISOLLLED OVER ANDTOED AND UN OLDULERS OF THEARTH WERE LEFTEDLY FINDIM AGAIN THE CURRENT OF A THE THES THE LEAST THE OUTANGESCHES PRWS OF TO OWN. BENOWLEDGE THE NEWDOM THEINGS AND RULEOM THEY THE WED NOONAGE AND0EDIENCE AND TO WH HIM OWN DOAVES TO OWNOR AND BRETHREN AND TOTEN THEIR THEIR BETRITUAL SUPERIORS.
Loss: 10.9237
outputs.loss tensor(10.9237, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0013.flac
Waveform stats - mean: -0.0000, std: 0.0774, min: -0.5382, max: 0.3595
Resampled waveform stats - mean: -0.0000, std: 0.0774, min: -0.5382, max: 0.3595
Raw mel spectrogram stats - mean: 2.2387, std: 15.5185, min: 0.0000, max: 482.1914
Log mel spectrogram stats - mean: -5.9761, std: 4.0981, min: -13.5662, max: 6.1783
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8521, max: 2.9659
Mel spec shape: torch.Size([1, 80, 488])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8525, max: 2.9668
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 50, 3584])
labels.shape torch.Size([1, 50])
outputs.logits.shape torch.Size([1, 50, 152064])

Sample prediction:
Target: OH THAT SHE WERE ONCE MORE CLIMBING UP THE FRIENDLY FIR POLE
Prediction: 00000000000000000000000000000000  ISOULD  THE ON THANINATICING UP THE HS CLINGOR.
Loss: 9.1698
outputs.loss tensor(9.1698, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0009.flac
Waveform stats - mean: 0.0001, std: 0.0893, min: -0.7313, max: 0.5630
Resampled waveform stats - mean: 0.0001, std: 0.0893, min: -0.7313, max: 0.5630
Raw mel spectrogram stats - mean: 2.9698, std: 15.7330, min: 0.0000, max: 625.2660
Log mel spectrogram stats - mean: -2.8401, std: 3.3192, min: -12.8497, max: 6.4382
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -3.0157, max: 2.7953
Mel spec shape: torch.Size([1, 80, 580])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -3.0156, max: 2.7949
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
input_embeds.shape torch.Size([1, 64, 3584])
labels.shape torch.Size([1, 64])
outputs.logits.shape torch.Size([1, 64, 152064])

Sample prediction:
Target: IN THAT CASE YOU WILL FIND ME A PROUD IMPETUOUS AMBITIOUS FELLOW CHRISTIE AND HOW WILL THAT SUIT
Prediction: 00000000000000000000000000000000000000 ,2 HAVE THE INROUNDBLEM TOERUOUS ANDERICANIOUS ANDIGHT WHOIAN 0ARD YOU BEIT YOU
Loss: 10.0257
outputs.loss tensor(10.0257, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/84280/174-84280-0011.flac
Waveform stats - mean: -0.0001, std: 0.0792, min: -0.5820, max: 0.9750
Resampled waveform stats - mean: -0.0001, std: 0.0792, min: -0.5820, max: 0.9750
Raw mel spectrogram stats - mean: 2.3521, std: 12.9081, min: 0.0000, max: 611.8659
Log mel spectrogram stats - mean: -4.4135, std: 4.0820, min: -13.7791, max: 6.4165
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2944, max: 2.6531
Mel spec shape: torch.Size([1, 80, 336])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2949, max: 2.6523
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
input_embeds.shape torch.Size([1, 35, 3584])
labels.shape torch.Size([1, 35])
outputs.logits.shape torch.Size([1, 35, 152064])

Sample prediction:
Target: BUT NOW IT DOESN'T SEEM TO MATTER VERY MUCH
Prediction: 00000000000000000000000 YOU ISN'T WORKEM TO BETER. MUCH.
Loss: 10.7658
outputs.loss tensor(10.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/161169/2803-161169-0002.flac
Waveform stats - mean: -0.0000, std: 0.0269, min: -0.1984, max: 0.1871
Resampled waveform stats - mean: -0.0000, std: 0.0269, min: -0.1984, max: 0.1871
Raw mel spectrogram stats - mean: 0.2703, std: 1.5508, min: 0.0000, max: 53.3108
Log mel spectrogram stats - mean: -7.4671, std: 4.0221, min: -13.7762, max: 3.9761
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5686, max: 2.8451
Mel spec shape: torch.Size([1, 80, 492])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5684, max: 2.8457
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
input_embeds.shape torch.Size([1, 51, 3584])
labels.shape torch.Size([1, 51])
outputs.logits.shape torch.Size([1, 51, 152064])

Sample prediction:
Target: WHY DID HE GIVE THAT SO ODD A SHAPE OR SO STRANGE A COVERING
Prediction: 000000000000000000000000000000000 YOU DOIVE ME TO MUCHDD NUMBER NUMBERDE TO0 IANGE A SHAING TO
Loss: 9.2296
outputs.loss tensor(9.2296, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147149/1993-147149-0028.flac
Waveform stats - mean: -0.0000, std: 0.0062, min: -0.0519, max: 0.0402
Resampled waveform stats - mean: -0.0000, std: 0.0062, min: -0.0519, max: 0.0402
Raw mel spectrogram stats - mean: 0.0144, std: 0.1054, min: 0.0000, max: 3.9777
Log mel spectrogram stats - mean: -8.9012, std: 2.9554, min: -13.7838, max: 1.3807
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6521, max: 3.4790
Mel spec shape: torch.Size([1, 80, 1018])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6523, max: 3.4785
CNN output shape: torch.Size([1, 512, 64])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
input_embeds.shape torch.Size([1, 114, 3584])
labels.shape torch.Size([1, 114])
outputs.logits.shape torch.Size([1, 114, 152064])

Sample prediction:
Target: THE OLD LEAVEN INFUSED YEARS AGO BY HER AUNT ESTHER FERMENTED IN HER LITTLE BOSOM AND PERHAPS ALL THE MORE FOR HER FATHER'S AVERSION TO THE RICH AND THE GENTLE
Prediction: 00000000000000000000000000000000000000000000000000000000000000000 ADESED0 WITH0O0 THEMDO HER .REED BY THE AATE GIRROTUS0HAPS IN THE OTHER BE THAT LEAR'S SUNT TO THE OLDAC AND0 POWERENTLEMAN
Loss: 9.7631
outputs.loss tensor(9.7631, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0015.flac
Waveform stats - mean: -0.0000, std: 0.0646, min: -0.3660, max: 0.3829
Resampled waveform stats - mean: -0.0000, std: 0.0646, min: -0.3660, max: 0.3829
Raw mel spectrogram stats - mean: 1.5652, std: 12.1517, min: 0.0000, max: 637.0979
Log mel spectrogram stats - mean: -5.9980, std: 4.2604, min: -13.8155, max: 6.4569
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8349, max: 2.9234
Mel spec shape: torch.Size([1, 80, 816])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8350, max: 2.9238
CNN output shape: torch.Size([1, 512, 51])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
input_embeds.shape torch.Size([1, 87, 3584])
labels.shape torch.Size([1, 87])
outputs.logits.shape torch.Size([1, 87, 152064])

Sample prediction:
Target: THE INTERVAL BETWEEN THESE FOUR CONTAINED A CHARIOT TRIUMPHAL ON TWO WHEELS WHICH BY A GRIFFIN'S NECK CAME DRAWN ALONG
Prediction: 0000000000000000000000000000000000000000000000000000 BETWEEN  TWO NUMINU 0ALLENGOT OFUMPHANT PROCESS THE WHEELS. W ITS WIFTEFIN WAS HEADCK WASARR TON BYONG THE
Loss: 10.4831
outputs.loss tensor(10.4831, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0001.flac
Waveform stats - mean: 0.0000, std: 0.0232, min: -0.1638, max: 0.1595
Resampled waveform stats - mean: 0.0000, std: 0.0232, min: -0.1638, max: 0.1595
Raw mel spectrogram stats - mean: 0.2025, std: 1.2070, min: 0.0000, max: 48.4863
Log mel spectrogram stats - mean: -7.0053, std: 3.9215, min: -13.7517, max: 3.8813
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7204, max: 2.7761
Mel spec shape: torch.Size([1, 80, 231])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7207, max: 2.7754
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
input_embeds.shape torch.Size([1, 29, 3584])
labels.shape torch.Size([1, 29])
outputs.logits.shape torch.Size([1, 29, 152064])

Sample prediction:
Target: THEN WHY INDULGE IN PROPHETIC PHANTASIES
Prediction: 0000000000000000 ICG IN THEFEETICALLYARMOMMS.
Loss: 10.1023
outputs.loss tensor(10.1023, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0014.flac
Waveform stats - mean: -0.0000, std: 0.0430, min: -0.4447, max: 0.3827
Resampled waveform stats - mean: -0.0000, std: 0.0430, min: -0.4447, max: 0.3827
Raw mel spectrogram stats - mean: 0.6921, std: 7.6526, min: 0.0000, max: 516.0972
Log mel spectrogram stats - mean: -8.3689, std: 5.0586, min: -13.8154, max: 6.2463
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.0767, max: 2.8892
Mel spec shape: torch.Size([1, 80, 383])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.0771, max: 2.8887
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 35, 3584])
labels.shape torch.Size([1, 35])
outputs.logits.shape torch.Size([1, 35, 152064])

Sample prediction:
Target: BAD GENERALSHIP I THOUGHT IT WAS CHRISTMAS
Prediction: 0000000000000000000000000000OUGHT I WAS AMAS I
Loss: 10.6528
outputs.loss tensor(10.6528, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/73752/7850-73752-0010.flac
Waveform stats - mean: -0.0000, std: 0.0641, min: -0.7504, max: 0.6490
Resampled waveform stats - mean: -0.0000, std: 0.0641, min: -0.7504, max: 0.6490
Raw mel spectrogram stats - mean: 1.5380, std: 12.9328, min: 0.0000, max: 847.7083
Log mel spectrogram stats - mean: -5.7295, std: 3.9303, min: -13.8018, max: 6.7425
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0538, max: 3.1733
Mel spec shape: torch.Size([1, 80, 1405])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0547, max: 3.1738
CNN output shape: torch.Size([1, 512, 88])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 45056
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 45056
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 315392
Inf count: 0
audio_emb.shape torch.Size([1, 88, 3584])
input_embeds.shape torch.Size([1, 152, 3584])
labels.shape torch.Size([1, 152])
outputs.logits.shape torch.Size([1, 152, 152064])

Sample prediction:
Target: HE COULD NOT FLATTER HIMSELF THAT HE INDEED MERITED SUCH SINGULAR BLESSINGS AND YET WITH ALL HIS FAULTS WHICH WITH HIM WERE BUT THE CONSEQUENCES OF HIS FIERY YOUTH FERDINAND HAD BEEN FAITHFUL TO HENRIETTA
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 BE BE00  WITH HE COFLED COIT THE AUFFULARITYURING.2ET HE THE HIS POWERULTS HE HE ALL WERE SO A RESULTSEQUENCE OF HIS FAERY PASSOUTH ANDELLVENTINANDOAD A AITHFUL TO HISIMRYETTA AND
Loss: 10.3018
outputs.loss tensor(10.3018, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0006.flac
Waveform stats - mean: -0.0001, std: 0.0565, min: -0.5710, max: 0.3773
Resampled waveform stats - mean: -0.0001, std: 0.0565, min: -0.5710, max: 0.3773
Raw mel spectrogram stats - mean: 1.1888, std: 11.0080, min: 0.0000, max: 484.1134
Log mel spectrogram stats - mean: -5.1709, std: 3.6707, min: -13.1044, max: 6.1823
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1613, max: 3.0929
Mel spec shape: torch.Size([1, 80, 384])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1621, max: 3.0938
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
input_embeds.shape torch.Size([1, 42, 3584])
labels.shape torch.Size([1, 42])
outputs.logits.shape torch.Size([1, 42, 152064])

Sample prediction:
Target: NO MISTER HARDWIGG SAID THE CAPTAIN NO FEAR OF THAT
Prediction: 00000000000000000000000000 ER0000 0TAIN. NOEAR NO THE NO
Loss: 10.3737
outputs.loss tensor(10.3737, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0039.flac
Waveform stats - mean: 0.0000, std: 0.1161, min: -0.6551, max: 0.6214
Resampled waveform stats - mean: 0.0000, std: 0.1161, min: -0.6551, max: 0.6214
Raw mel spectrogram stats - mean: 5.0139, std: 39.3570, min: 0.0000, max: 2253.7251
Log mel spectrogram stats - mean: -4.1130, std: 3.7757, min: -13.0885, max: 7.7203
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3772, max: 3.1341
Mel spec shape: torch.Size([1, 80, 1416])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3770, max: 3.1348
CNN output shape: torch.Size([1, 512, 89])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 45568
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 45568
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 318976
Inf count: 0
audio_emb.shape torch.Size([1, 89, 3584])
input_embeds.shape torch.Size([1, 150, 3584])
labels.shape torch.Size([1, 150])
outputs.logits.shape torch.Size([1, 150, 152064])

Sample prediction:
Target: ONE OF THE SQUIRES OBSERVED IN HIS MIXTURE OF GASCON AND CATALAN THIS CAPTAIN OF OURS WOULD MAKE A BETTER FRIAR THAN HIGHWAYMAN IF HE WANTS TO BE SO GENEROUS ANOTHER TIME LET IT BE WITH HIS OWN PROPERTY AND NOT OURS
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 THE MOST0 OFERV IN THE ED OF OLES 0AN0 ISTUREURING OF THES INERE BE A GOODTER MIXORTE THAN THELYMAN  HE WERE TO BE A.OUS.0 F. US BE SO THE MIX MIX AND0 WITHS.
Loss: 9.7441
outputs.loss tensor(9.7441, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0005.flac
Waveform stats - mean: -0.0000, std: 0.0821, min: -0.4814, max: 0.4808
Resampled waveform stats - mean: -0.0000, std: 0.0821, min: -0.4814, max: 0.4808
Raw mel spectrogram stats - mean: 2.5231, std: 27.0958, min: 0.0000, max: 1285.0874
Log mel spectrogram stats - mean: -5.8035, std: 4.4361, min: -13.8151, max: 7.1586
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8060, max: 2.9219
Mel spec shape: torch.Size([1, 80, 557])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8057, max: 2.9219
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
input_embeds.shape torch.Size([1, 55, 3584])
labels.shape torch.Size([1, 55])
outputs.logits.shape torch.Size([1, 55, 152064])

Sample prediction:
Target: NO BATTERY IN THE WHOLE FOUR YEARS WAR LOST SO MANY MEN IN SO SHORT A TIME
Prediction: 00000000000000000000000000000000000000 THE OLE WORLDTH. OST  MUCH L AND THE MANY A TIME THAT
Loss: 9.2958
outputs.loss tensor(9.2958, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0008.flac
Waveform stats - mean: -0.0000, std: 0.0896, min: -0.5831, max: 0.4224
Resampled waveform stats - mean: -0.0000, std: 0.0896, min: -0.5831, max: 0.4224
Raw mel spectrogram stats - mean: 2.9904, std: 21.1026, min: 0.0000, max: 575.0859
Log mel spectrogram stats - mean: -5.8351, std: 4.4910, min: -13.7500, max: 6.3545
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7624, max: 2.7143
Mel spec shape: torch.Size([1, 80, 195])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7627, max: 2.7148
CNN output shape: torch.Size([1, 512, 13])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
input_embeds.shape torch.Size([1, 22, 3584])
labels.shape torch.Size([1, 22])
outputs.logits.shape torch.Size([1, 22, 152064])

Sample prediction:
Target: SO HE'S A FRIEND OF YOURS EH
Prediction: 00000000000000Y   OF MS. 
Loss: 9.5138
outputs.loss tensor(9.5138, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149897/2277-149897-0024.flac
Waveform stats - mean: 0.0000, std: 0.0478, min: -0.4080, max: 0.3272
Resampled waveform stats - mean: 0.0000, std: 0.0478, min: -0.4080, max: 0.3272
Raw mel spectrogram stats - mean: 0.8546, std: 6.8078, min: 0.0000, max: 296.4908
Log mel spectrogram stats - mean: -5.8334, std: 3.6071, min: -13.6826, max: 5.6920
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1760, max: 3.1952
Mel spec shape: torch.Size([1, 80, 294])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1758, max: 3.1953
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
input_embeds.shape torch.Size([1, 36, 3584])
labels.shape torch.Size([1, 36])
outputs.logits.shape torch.Size([1, 36, 152064])

Sample prediction:
Target: IT SEEMED AS IF HIS FAMILY TROUBLES WERE JUST BEGINNING
Prediction: 000000000000000000000ED TO2 THE M WASOLDUBLED WERE NOT BEGINNING.
Loss: 10.1942
outputs.loss tensor(10.1942, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0007.flac
Waveform stats - mean: -0.0000, std: 0.0714, min: -0.6227, max: 0.4343
Resampled waveform stats - mean: -0.0000, std: 0.0714, min: -0.6227, max: 0.4343
Raw mel spectrogram stats - mean: 1.9079, std: 12.1025, min: 0.0000, max: 834.6205
Log mel spectrogram stats - mean: -4.7634, std: 3.8575, min: -13.2653, max: 6.7270
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2040, max: 2.9787
Mel spec shape: torch.Size([1, 80, 2075])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2031, max: 2.9785
CNN output shape: torch.Size([1, 512, 130])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 66560
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 66560
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 465920
Inf count: 0
audio_emb.shape torch.Size([1, 130, 3584])
input_embeds.shape torch.Size([1, 205, 3584])
labels.shape torch.Size([1, 205])
outputs.logits.shape torch.Size([1, 205, 152064])

Sample prediction:
Target: THEN PLACE THE PAN ON A STRONG CHAIR OR DRESSER OR TABLE OF CONVENIENT HEIGHT POUR INTO THE SPONGE THE REMAINDER OF THE WARM MILK AND WATER STIR INTO IT AS MUCH OF THE FLOUR AS YOU CAN WITH THE SPOON THEN WIPE IT OUT CLEAN WITH YOUR FINGERS AND LAY IT ASIDE
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000  ES THE AND MANAIN.0UCKING.0. CREIENCE LOCATION.REFER THE THE PANONGE B SPAININDER OF THE SOAPINE WATERK INTO THE INTOIR IT THE  MUCH AS THE MILLOUR AS YOU CAN ADD THE SPPOON. ADDPE THE ON ON THE THE HANDINGERERS AND PUTICK IT ONIDE TO
Loss: 9.9780
outputs.loss tensor(9.9780, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153948/2412-153948-0005.flac
Waveform stats - mean: -0.0001, std: 0.0537, min: -0.7313, max: 0.5096
Resampled waveform stats - mean: -0.0001, std: 0.0537, min: -0.7313, max: 0.5096
Raw mel spectrogram stats - mean: 1.0744, std: 10.3959, min: 0.0000, max: 526.9546
Log mel spectrogram stats - mean: -5.6663, std: 3.8711, min: -13.6966, max: 6.2671
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0744, max: 3.0827
Mel spec shape: torch.Size([1, 80, 315])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0742, max: 3.0820
CNN output shape: torch.Size([1, 512, 20])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
input_embeds.shape torch.Size([1, 35, 3584])
labels.shape torch.Size([1, 35])
outputs.logits.shape torch.Size([1, 35, 152064])

Sample prediction:
Target: I WAS DELIGHTED WITH THE COUNTRY AND THE MANNER OF LIFE
Prediction: 000000000000000000000 AYED TO THE 0 OF0 PEOPLEOUNT OF THE OF
Loss: 11.3580
outputs.loss tensor(11.3580, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0016.flac
Waveform stats - mean: -0.0000, std: 0.0524, min: -0.3604, max: 0.3165
Resampled waveform stats - mean: -0.0000, std: 0.0524, min: -0.3604, max: 0.3165
Raw mel spectrogram stats - mean: 1.0291, std: 6.1131, min: 0.0000, max: 218.3788
Log mel spectrogram stats - mean: -7.8269, std: 5.4648, min: -13.8136, max: 5.3862
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.0955, max: 2.4179
Mel spec shape: torch.Size([1, 80, 259])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.0957, max: 2.4180
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
input_embeds.shape torch.Size([1, 28, 3584])
labels.shape torch.Size([1, 28])
outputs.logits.shape torch.Size([1, 28, 152064])

Sample prediction:
Target: WE HAD BEEF FOR SUPPER THAT NIGHT
Prediction: 0000000000000000000 000 PER. NIGHT.
Loss: 10.3312
outputs.loss tensor(10.3312, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0002.flac
Waveform stats - mean: 0.0013, std: 0.0212, min: -0.1902, max: 0.1855
Resampled waveform stats - mean: 0.0013, std: 0.0212, min: -0.1902, max: 0.1855
Raw mel spectrogram stats - mean: 0.1622, std: 0.8812, min: 0.0000, max: 37.9381
Log mel spectrogram stats - mean: -5.6717, std: 2.9992, min: -12.5441, max: 3.6360
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2914, max: 3.1034
Mel spec shape: torch.Size([1, 80, 284])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2910, max: 3.1035
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
input_embeds.shape torch.Size([1, 26, 3584])
labels.shape torch.Size([1, 26])
outputs.logits.shape torch.Size([1, 26, 152064])

Sample prediction:
Target: YES AND A VERY RESPECTABLE ONE
Prediction: 0000000000000000000 0 GOODPECTABLE .
Loss: 10.1154
outputs.loss tensor(10.1154, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/84280/174-84280-0006.flac
Waveform stats - mean: -0.0001, std: 0.0706, min: -0.9129, max: 0.9489
Resampled waveform stats - mean: -0.0001, std: 0.0706, min: -0.9129, max: 0.9489
Raw mel spectrogram stats - mean: 1.8671, std: 12.7982, min: 0.0000, max: 739.3179
Log mel spectrogram stats - mean: -4.6967, std: 4.0469, min: -13.4805, max: 6.6057
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1705, max: 2.7929
Mel spec shape: torch.Size([1, 80, 704])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1699, max: 2.7930
CNN output shape: torch.Size([1, 512, 44])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
input_embeds.shape torch.Size([1, 70, 3584])
labels.shape torch.Size([1, 70])
outputs.logits.shape torch.Size([1, 70, 152064])

Sample prediction:
Target: IT WAS THAT IDEA OF WASTE THAT DOMINATED MY MIND IN A STRANGE INTERVIEW I HAD WITH JUSTIN
Prediction: 000000000000000000000000000000000000000000000 A  THAT OR. MADEINATED THE THIND. THE WAYONG WAYEST OF0AD WITH AIN B
Loss: 10.4478
outputs.loss tensor(10.4478, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0048.flac
Waveform stats - mean: -0.0000, std: 0.0676, min: -0.4138, max: 0.3589
Resampled waveform stats - mean: -0.0000, std: 0.0676, min: -0.4138, max: 0.3589
Raw mel spectrogram stats - mean: 1.7108, std: 10.9072, min: 0.0000, max: 419.0660
Log mel spectrogram stats - mean: -6.2507, std: 4.7166, min: -13.8024, max: 6.0380
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6011, max: 2.6054
Mel spec shape: torch.Size([1, 80, 725])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6016, max: 2.6055
CNN output shape: torch.Size([1, 512, 46])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23552
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23552
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 164864
Inf count: 0
audio_emb.shape torch.Size([1, 46, 3584])
input_embeds.shape torch.Size([1, 76, 3584])
labels.shape torch.Size([1, 76])
outputs.logits.shape torch.Size([1, 76, 152064])

Sample prediction:
Target: I THREW MYSELF AT HIS FEET TO ASSURE HIM OF MY GRATITUDE AND EMBRACED HIM CALLING HIM MY FATHER
Prediction: 000000000000000000000000000000000000000000000000   INTO THE OWNET. HISIST HIS THAT MY ITUDE.0TERNRACEED HIM ASING HIM MY SATHER.
Loss: 10.9821
outputs.loss tensor(10.9821, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0015.flac
Waveform stats - mean: 0.0000, std: 0.0647, min: -0.3606, max: 0.3734
Resampled waveform stats - mean: 0.0000, std: 0.0647, min: -0.3606, max: 0.3734
Raw mel spectrogram stats - mean: 1.5684, std: 8.9513, min: 0.0000, max: 329.4942
Log mel spectrogram stats - mean: -6.4953, std: 4.8664, min: -13.8117, max: 5.7976
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5034, max: 2.5260
Mel spec shape: torch.Size([1, 80, 507])
