Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0030.flac
Waveform stats - mean: -0.0000, std: 0.0645, min: -0.5182, max: 0.8201
Resampled waveform stats - mean: -0.0000, std: 0.0645, min: -0.5182, max: 0.8201
Raw mel spectrogram stats - mean: 1.5562, std: 11.2144, min: 0.0000, max: 647.8763
Log mel spectrogram stats - mean: -6.6333, std: 4.8761, min: -13.8107, max: 6.4737
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4720, max: 2.6880
Mel spec shape: torch.Size([1, 80, 811])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4717, max: 2.6875
CNN output shape: torch.Size([1, 512, 51])
CNN output stats - mean: 0.2844, std: 0.5889, min: -0.1699, max: 3.8105
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.8613, max: 4.1289
Final output stats - mean: 0.0075, std: 0.2109, min: -0.7529, max: 0.9995
audio_emb.shape torch.Size([1, 51, 3584])
Audio embedding stats - mean: 0.0076, std: 0.2109

Sample prediction:
Target: THE SUN AS YOU SEE TELLS QUITE ANOTHER STORY AND WILL NOT BE COAXED OUT OF IT AFTER HALF A DOZEN PATIENT ATTEMPTS ON MY PART
Prediction: IIy1II1IIy1,y1IIy,1,II1II11yII1,y1111111IIII1y
Loss: 12.4873
outputs.loss tensor(12.4873, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0013.flac
Waveform stats - mean: -0.0001, std: 0.0484, min: -0.3545, max: 0.4679
Resampled waveform stats - mean: -0.0001, std: 0.0484, min: -0.3545, max: 0.4679
Raw mel spectrogram stats - mean: 0.8761, std: 6.0854, min: 0.0000, max: 214.3375
Log mel spectrogram stats - mean: -6.8495, std: 4.5986, min: -13.7826, max: 5.3676
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5077, max: 2.6567
Mel spec shape: torch.Size([1, 80, 485])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5078, max: 2.6562
CNN output shape: torch.Size([1, 512, 31])
CNN output stats - mean: 0.2844, std: 0.5923, min: -0.1699, max: 3.4277
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.6035, max: 3.5781
Final output stats - mean: 0.0042, std: 0.2120, min: -0.8101, max: 1.0059
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0045, std: 0.2124

Sample prediction:
Target: IT IS FROM HER ACTION IN THAT MATTER THAT MY SUSPICION SPRINGS
Prediction: als 00 or
 orn 或
 或  Dort Dort 或 Dort
Loss: 13.8181
outputs.loss tensor(13.8181, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0009.flac
Waveform stats - mean: 0.0000, std: 0.0700, min: -0.7749, max: 0.7011
Resampled waveform stats - mean: 0.0000, std: 0.0700, min: -0.7749, max: 0.7011
Raw mel spectrogram stats - mean: 1.8296, std: 20.0380, min: 0.0000, max: 1128.2253
Log mel spectrogram stats - mean: -5.1299, std: 3.8227, min: -13.8052, max: 7.0284
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2695, max: 3.1806
Mel spec shape: torch.Size([1, 80, 272])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2695, max: 3.1797
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: 0.2866, std: 0.5874, min: -0.1699, max: 3.2188
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.5703, max: 3.3477
Final output stats - mean: 0.0066, std: 0.2101, min: -0.7607, max: 0.9614
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0065, std: 0.2106

Sample prediction:
Target: I PROTEST SHOUTED THE PROFESSOR
Prediction: ,II  isJJJJJ,
Loss: 12.4395
outputs.loss tensor(12.4395, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/57405/6319-57405-0010.flac
Waveform stats - mean: -0.0000, std: 0.0670, min: -0.4858, max: 0.5674
Resampled waveform stats - mean: -0.0000, std: 0.0670, min: -0.4858, max: 0.5674
Raw mel spectrogram stats - mean: 1.6760, std: 17.3892, min: 0.0000, max: 563.3270
Log mel spectrogram stats - mean: -6.6826, std: 3.7520, min: -13.7965, max: 6.3339
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8960, max: 3.4692
Mel spec shape: torch.Size([1, 80, 335])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8965, max: 3.4688
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: 0.2749, std: 0.5957, min: -0.1699, max: 3.7344
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.4941, max: 3.8691
Final output stats - mean: 0.0033, std: 0.2061, min: -0.7690, max: 1.0977
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0030, std: 0.2064

Sample prediction:
Target: WHAT DID HE MEAN ASKED PYRRHA
Prediction: 再有有有器,有有有,的
Loss: 13.4409
outputs.loss tensor(13.4409, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0001.flac
Waveform stats - mean: -0.0000, std: 0.0624, min: -0.6768, max: 0.6803
Resampled waveform stats - mean: -0.0000, std: 0.0624, min: -0.6768, max: 0.6803
Raw mel spectrogram stats - mean: 1.4615, std: 17.5572, min: 0.0000, max: 1629.5961
Log mel spectrogram stats - mean: -7.0248, std: 4.1771, min: -13.8060, max: 7.3961
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6234, max: 3.4523
Mel spec shape: torch.Size([1, 80, 1447])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6230, max: 3.4531
CNN output shape: torch.Size([1, 512, 91])
CNN output stats - mean: 0.2810, std: 0.5928, min: -0.1699, max: 4.5430
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.6055, max: 3.9141
Final output stats - mean: 0.0057, std: 0.2094, min: -0.8535, max: 1.0713
audio_emb.shape torch.Size([1, 91, 3584])
Audio embedding stats - mean: 0.0056, std: 0.2095

Sample prediction:
Target: THE YOUNG MAN DREW A DEEP BREATH OF RELIEF AND LIGHTED THE WAX CANDLES IN THE SOLID SILVER CANDLESTICKS ON HIS WRITING TABLE FOR NOW THE LATE SUMMER DUSK WAS FALLING AND THAT ORGAN PLEASE HEAVEN MADE FULL THE MEASURE OF THE DAY'S APPOINTED TORTURE
Prediction:  the themy is ()


0
1,
 Dort
 Dort22
1




0212, ,
0

2
1
 �2
 � � is001 � �02


0 is2 Bold � Bold, 



0,0

0 �
Loss: 11.7488
outputs.loss tensor(11.7488, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0040.flac
Waveform stats - mean: 0.0000, std: 0.0576, min: -0.5091, max: 0.3383
Resampled waveform stats - mean: 0.0000, std: 0.0576, min: -0.5091, max: 0.3383
Raw mel spectrogram stats - mean: 1.2264, std: 8.3986, min: 0.0000, max: 605.1581
Log mel spectrogram stats - mean: -5.1896, std: 3.6146, min: -12.6644, max: 6.4055
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0680, max: 3.2078
Mel spec shape: torch.Size([1, 80, 670])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0684, max: 3.2070
CNN output shape: torch.Size([1, 512, 42])
CNN output stats - mean: 0.2800, std: 0.5913, min: -0.1699, max: 3.6191
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.9297, max: 3.8516
Final output stats - mean: 0.0036, std: 0.2080, min: -0.7837, max: 0.9277
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0036, std: 0.2080

Sample prediction:
Target: THEY WERE ALL TAKEN ABACK AND NOT ONE OF THEM DARED TO UTTER A WORD SUCH DEFERENCE DID THEY PAY HIM
Prediction: II11II西侧131111y1111, pérdida11y111111y,
Loss: 12.4310
outputs.loss tensor(12.4310, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/73752/7850-73752-0016.flac
Waveform stats - mean: 0.0000, std: 0.0590, min: -0.6676, max: 0.7754
Resampled waveform stats - mean: 0.0000, std: 0.0590, min: -0.6676, max: 0.7754
Raw mel spectrogram stats - mean: 1.2978, std: 9.4969, min: 0.0000, max: 556.5567
Log mel spectrogram stats - mean: -6.2927, std: 4.1237, min: -13.5938, max: 6.3218
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7705, max: 3.0590
Mel spec shape: torch.Size([1, 80, 360])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7705, max: 3.0586
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: 0.2822, std: 0.5889, min: -0.1699, max: 3.5215
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.1992, max: 3.9102
Final output stats - mean: 0.0068, std: 0.2065, min: -0.7998, max: 0.8760
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0070, std: 0.2069

Sample prediction:
Target: HE MIGHT BE ENCHANTED BUT THAT WAS THE TALISMAN
Prediction: 1.1y.y1y1.y pérdida.y,
Loss: 12.3542
outputs.loss tensor(12.3542, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0016.flac
Waveform stats - mean: -0.0000, std: 0.0698, min: -0.3926, max: 0.5637
Resampled waveform stats - mean: -0.0000, std: 0.0698, min: -0.3926, max: 0.5637
Raw mel spectrogram stats - mean: 1.8259, std: 9.1388, min: 0.0000, max: 320.9494
Log mel spectrogram stats - mean: -6.4460, std: 4.7803, min: -13.8138, max: 5.7713
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5413, max: 2.5558
Mel spec shape: torch.Size([1, 80, 853])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5410, max: 2.5566
CNN output shape: torch.Size([1, 512, 54])
CNN output stats - mean: 0.2805, std: 0.5903, min: -0.1699, max: 4.0117
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.5820, max: 3.8184
Final output stats - mean: 0.0048, std: 0.2109, min: -0.8374, max: 0.9893
audio_emb.shape torch.Size([1, 54, 3584])
Audio embedding stats - mean: 0.0049, std: 0.2106

Sample prediction:
Target: HE WAS INTRODUCED TO MISSUS NORMAN AND TO MISSUS NORMAN'S LITTLE GIRL AND WE WERE ALL CHARMED WITH HIM
Prediction:  in1, Dn,D

DDD
DD
DDD
D is is1D is1,copy Dort �1 000
Loss: 11.1704
outputs.loss tensor(11.1704, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=-0.0004, std=0.2935
cnn_layers.0.bias: mean=-0.0000, std=0.0002
cnn_layers.1.weight: mean=0.0002, std=0.1276
cnn_layers.1.bias: mean=-0.0023, std=0.0917
cnn_layers.3.weight: mean=0.0033, std=0.2025
cnn_layers.3.bias: mean=0.0000, std=0.0002
cnn_layers.4.weight: mean=0.0011, std=0.1032
cnn_layers.4.bias: mean=-0.0026, std=0.0762
cnn_layers.6.weight: mean=0.0009, std=0.1675
cnn_layers.6.bias: mean=-0.0000, std=0.0001
cnn_layers.7.weight: mean=-0.0005, std=0.0903
cnn_layers.7.bias: mean=-0.0006, std=0.0655
cnn_layers.9.weight: mean=0.0000, std=0.1469
cnn_layers.9.bias: mean=-0.0000, std=0.0003
cnn_layers.10.weight: mean=-0.0018, std=0.3855
cnn_layers.10.bias: mean=-0.0010, std=0.4583
transformer.layers.0.self_attn.in_proj_weight: mean=-0.0030, std=0.0627
transformer.layers.0.self_attn.in_proj_bias: mean=-0.0103, std=0.2203
transformer.layers.0.self_attn.out_proj.weight: mean=-0.0000, std=0.1310
transformer.layers.0.self_attn.out_proj.bias: mean=-0.0029, std=0.6602
transformer.layers.0.linear1.weight: mean=0.0000, std=0.0198
transformer.layers.0.linear1.bias: mean=0.0008, std=0.0517
transformer.layers.0.linear2.weight: mean=0.0000, std=0.0536
transformer.layers.0.linear2.bias: mean=0.0004, std=0.3569
transformer.layers.0.norm1.weight: mean=0.0008, std=0.0999
transformer.layers.0.norm1.bias: mean=0.0002, std=0.3816
transformer.layers.0.norm2.weight: mean=0.0004, std=0.0942
transformer.layers.0.norm2.bias: mean=-0.0063, std=0.3645
transformer.layers.1.self_attn.in_proj_weight: mean=-0.0000, std=0.0224
transformer.layers.1.self_attn.in_proj_bias: mean=-0.0045, std=0.0898
transformer.layers.1.self_attn.out_proj.weight: mean=0.0000, std=0.0455
transformer.layers.1.self_attn.out_proj.bias: mean=0.0014, std=0.2693
transformer.layers.1.linear1.weight: mean=-0.0000, std=0.0144
transformer.layers.1.linear1.bias: mean=0.0006, std=0.0366
transformer.layers.1.linear2.weight: mean=0.0001, std=0.0385
transformer.layers.1.linear2.bias: mean=0.0005, std=0.2493
transformer.layers.1.norm1.weight: mean=0.0007, std=0.0752
transformer.layers.1.norm1.bias: mean=0.0004, std=0.2673
transformer.layers.1.norm2.weight: mean=0.0003, std=0.0775
transformer.layers.1.norm2.bias: mean=-0.0042, std=0.2634
transformer.layers.2.self_attn.in_proj_weight: mean=-0.0000, std=0.0182
transformer.layers.2.self_attn.in_proj_bias: mean=-0.0028, std=0.0647
transformer.layers.2.self_attn.out_proj.weight: mean=-0.0000, std=0.0375
transformer.layers.2.self_attn.out_proj.bias: mean=0.0001, std=0.1921
transformer.layers.2.linear1.weight: mean=0.0000, std=0.0117
transformer.layers.2.linear1.bias: mean=0.0005, std=0.0266
transformer.layers.2.linear2.weight: mean=-0.0001, std=0.0300
transformer.layers.2.linear2.bias: mean=-0.0012, std=0.1827
transformer.layers.2.norm1.weight: mean=0.0004, std=0.0679
transformer.layers.2.norm1.bias: mean=0.0006, std=0.1954
transformer.layers.2.norm2.weight: mean=0.0002, std=0.0714
transformer.layers.2.norm2.bias: mean=-0.0031, std=0.1979
transformer.layers.3.self_attn.in_proj_weight: mean=-0.0000, std=0.0167
transformer.layers.3.self_attn.in_proj_bias: mean=-0.0019, std=0.0483
transformer.layers.3.self_attn.out_proj.weight: mean=-0.0000, std=0.0357
transformer.layers.3.self_attn.out_proj.bias: mean=0.0002, std=0.1465
transformer.layers.3.linear1.weight: mean=-0.0000, std=0.0104
transformer.layers.3.linear1.bias: mean=0.0003, std=0.0209
transformer.layers.3.linear2.weight: mean=0.0000, std=0.0256
transformer.layers.3.linear2.bias: mean=0.0002, std=0.1407
transformer.layers.3.norm1.weight: mean=0.0001, std=0.0662
transformer.layers.3.norm1.bias: mean=0.0006, std=0.1498
transformer.layers.3.norm2.weight: mean=0.0001, std=0.0676
transformer.layers.3.norm2.bias: mean=-0.0025, std=0.1537
transformer.layers.4.self_attn.in_proj_weight: mean=0.0000, std=0.0159
transformer.layers.4.self_attn.in_proj_bias: mean=-0.0017, std=0.0371
transformer.layers.4.self_attn.out_proj.weight: mean=-0.0000, std=0.0346
transformer.layers.4.self_attn.out_proj.bias: mean=-0.0001, std=0.1135
transformer.layers.4.linear1.weight: mean=-0.0000, std=0.0095
transformer.layers.4.linear1.bias: mean=0.0003, std=0.0165
transformer.layers.4.linear2.weight: mean=-0.0001, std=0.0224
transformer.layers.4.linear2.bias: mean=-0.0009, std=0.1101
transformer.layers.4.norm1.weight: mean=0.0001, std=0.0645
transformer.layers.4.norm1.bias: mean=0.0008, std=0.1187
transformer.layers.4.norm2.weight: mean=0.0002, std=0.0647
transformer.layers.4.norm2.bias: mean=-0.0022, std=0.1231
transformer.layers.5.self_attn.in_proj_weight: mean=0.0000, std=0.0155
transformer.layers.5.self_attn.in_proj_bias: mean=-0.0013, std=0.0300
transformer.layers.5.self_attn.out_proj.weight: mean=0.0000, std=0.0343
transformer.layers.5.self_attn.out_proj.bias: mean=-0.0000, std=0.0923
transformer.layers.5.linear1.weight: mean=0.0000, std=0.0093
transformer.layers.5.linear1.bias: mean=0.0003, std=0.0141
transformer.layers.5.linear2.weight: mean=-0.0000, std=0.0210
transformer.layers.5.linear2.bias: mean=-0.0002, std=0.0913
transformer.layers.5.norm1.weight: mean=0.0001, std=0.0643
transformer.layers.5.norm1.bias: mean=0.0007, std=0.0980
transformer.layers.5.norm2.weight: mean=-0.0000, std=0.0635
transformer.layers.5.norm2.bias: mean=-0.0020, std=0.1016
transformer.layers.6.self_attn.in_proj_weight: mean=-0.0000, std=0.0151
transformer.layers.6.self_attn.in_proj_bias: mean=-0.0012, std=0.0247
transformer.layers.6.self_attn.out_proj.weight: mean=-0.0000, std=0.0333
transformer.layers.6.self_attn.out_proj.bias: mean=-0.0002, std=0.0757
transformer.layers.6.linear1.weight: mean=0.0000, std=0.0091
transformer.layers.6.linear1.bias: mean=0.0002, std=0.0123
transformer.layers.6.linear2.weight: mean=0.0000, std=0.0198
transformer.layers.6.linear2.bias: mean=0.0003, std=0.0775
transformer.layers.6.norm1.weight: mean=-0.0000, std=0.0631
transformer.layers.6.norm1.bias: mean=0.0007, std=0.0836
transformer.layers.6.norm2.weight: mean=-0.0001, std=0.0617
transformer.layers.6.norm2.bias: mean=-0.0014, std=0.0863
transformer.layers.7.self_attn.in_proj_weight: mean=-0.0000, std=0.0148
transformer.layers.7.self_attn.in_proj_bias: mean=-0.0010, std=0.0209
transformer.layers.7.self_attn.out_proj.weight: mean=-0.0000, std=0.0327
transformer.layers.7.self_attn.out_proj.bias: mean=-0.0003, std=0.0649
transformer.layers.7.linear1.weight: mean=-0.0000, std=0.0092
transformer.layers.7.linear1.bias: mean=0.0003, std=0.0112
transformer.layers.7.linear2.weight: mean=0.0000, std=0.0193
transformer.layers.7.linear2.bias: mean=0.0001, std=0.0681
transformer.layers.7.norm1.weight: mean=-0.0001, std=0.0617
transformer.layers.7.norm1.bias: mean=0.0008, std=0.0735
transformer.layers.7.norm2.weight: mean=-0.0002, std=0.0598
transformer.layers.7.norm2.bias: mean=-0.0013, std=0.0753
transformer.layers.8.self_attn.in_proj_weight: mean=0.0000, std=0.0143
transformer.layers.8.self_attn.in_proj_bias: mean=-0.0009, std=0.0180
transformer.layers.8.self_attn.out_proj.weight: mean=-0.0000, std=0.0317
transformer.layers.8.self_attn.out_proj.bias: mean=-0.0004, std=0.0567
transformer.layers.8.linear1.weight: mean=0.0000, std=0.0092
transformer.layers.8.linear1.bias: mean=0.0002, std=0.0104
transformer.layers.8.linear2.weight: mean=0.0000, std=0.0188
transformer.layers.8.linear2.bias: mean=0.0000, std=0.0615
transformer.layers.8.norm1.weight: mean=-0.0001, std=0.0599
transformer.layers.8.norm1.bias: mean=0.0004, std=0.0663
transformer.layers.8.norm2.weight: mean=-0.0002, std=0.0577
transformer.layers.8.norm2.bias: mean=-0.0011, std=0.0677
transformer.layers.9.self_attn.in_proj_weight: mean=-0.0000, std=0.0141
transformer.layers.9.self_attn.in_proj_bias: mean=-0.0007, std=0.0164
transformer.layers.9.self_attn.out_proj.weight: mean=0.0000, std=0.0306
transformer.layers.9.self_attn.out_proj.bias: mean=0.0000, std=0.0508
transformer.layers.9.linear1.weight: mean=0.0000, std=0.0091
transformer.layers.9.linear1.bias: mean=0.0002, std=0.0098
transformer.layers.9.linear2.weight: mean=0.0000, std=0.0183
transformer.layers.9.linear2.bias: mean=0.0003, std=0.0565
transformer.layers.9.norm1.weight: mean=-0.0001, std=0.0579
transformer.layers.9.norm1.bias: mean=0.0003, std=0.0611
transformer.layers.9.norm2.weight: mean=-0.0001, std=0.0562
transformer.layers.9.norm2.bias: mean=-0.0009, std=0.0618
transformer.layers.10.self_attn.in_proj_weight: mean=-0.0000, std=0.0139
transformer.layers.10.self_attn.in_proj_bias: mean=-0.0006, std=0.0151
transformer.layers.10.self_attn.out_proj.weight: mean=0.0000, std=0.0296
transformer.layers.10.self_attn.out_proj.bias: mean=0.0000, std=0.0465
transformer.layers.10.linear1.weight: mean=-0.0000, std=0.0090
transformer.layers.10.linear1.bias: mean=0.0002, std=0.0092
transformer.layers.10.linear2.weight: mean=-0.0000, std=0.0178
transformer.layers.10.linear2.bias: mean=-0.0001, std=0.0524
transformer.layers.10.norm1.weight: mean=-0.0001, std=0.0554
transformer.layers.10.norm1.bias: mean=0.0002, std=0.0567
transformer.layers.10.norm2.weight: mean=-0.0001, std=0.0543
transformer.layers.10.norm2.bias: mean=-0.0009, std=0.0572
transformer.layers.11.self_attn.in_proj_weight: mean=-0.0000, std=0.0134
transformer.layers.11.self_attn.in_proj_bias: mean=-0.0005, std=0.0140
transformer.layers.11.self_attn.out_proj.weight: mean=-0.0000, std=0.0283
transformer.layers.11.self_attn.out_proj.bias: mean=-0.0003, std=0.0429
transformer.layers.11.linear1.weight: mean=0.0000, std=0.0088
transformer.layers.11.linear1.bias: mean=0.0002, std=0.0088
transformer.layers.11.linear2.weight: mean=-0.0000, std=0.0172
transformer.layers.11.linear2.bias: mean=-0.0003, std=0.0493
transformer.layers.11.norm1.weight: mean=-0.0000, std=0.0529
transformer.layers.11.norm1.bias: mean=0.0001, std=0.0532
transformer.layers.11.norm2.weight: mean=-0.0000, std=0.0527
transformer.layers.11.norm2.bias: mean=-0.0008, std=0.0536
transformer.layers.12.self_attn.in_proj_weight: mean=-0.0000, std=0.0132
transformer.layers.12.self_attn.in_proj_bias: mean=-0.0006, std=0.0134
transformer.layers.12.self_attn.out_proj.weight: mean=0.0000, std=0.0274
transformer.layers.12.self_attn.out_proj.bias: mean=0.0003, std=0.0404
transformer.layers.12.linear1.weight: mean=-0.0000, std=0.0086
transformer.layers.12.linear1.bias: mean=0.0001, std=0.0085
transformer.layers.12.linear2.weight: mean=0.0000, std=0.0167
transformer.layers.12.linear2.bias: mean=0.0001, std=0.0467
transformer.layers.12.norm1.weight: mean=-0.0000, std=0.0512
transformer.layers.12.norm1.bias: mean=-0.0001, std=0.0504
transformer.layers.12.norm2.weight: mean=-0.0000, std=0.0510
transformer.layers.12.norm2.bias: mean=-0.0007, std=0.0506
transformer.layers.13.self_attn.in_proj_weight: mean=0.0000, std=0.0128
transformer.layers.13.self_attn.in_proj_bias: mean=-0.0005, std=0.0126
transformer.layers.13.self_attn.out_proj.weight: mean=-0.0000, std=0.0261
transformer.layers.13.self_attn.out_proj.bias: mean=-0.0000, std=0.0378
transformer.layers.13.linear1.weight: mean=0.0000, std=0.0083
transformer.layers.13.linear1.bias: mean=0.0001, std=0.0080
transformer.layers.13.linear2.weight: mean=-0.0000, std=0.0161
transformer.layers.13.linear2.bias: mean=-0.0001, std=0.0442
transformer.layers.13.norm1.weight: mean=-0.0000, std=0.0488
transformer.layers.13.norm1.bias: mean=0.0000, std=0.0477
transformer.layers.13.norm2.weight: mean=-0.0000, std=0.0491
transformer.layers.13.norm2.bias: mean=-0.0005, std=0.0477
transformer.layers.14.self_attn.in_proj_weight: mean=0.0000, std=0.0124
transformer.layers.14.self_attn.in_proj_bias: mean=-0.0004, std=0.0121
transformer.layers.14.self_attn.out_proj.weight: mean=0.0000, std=0.0253
transformer.layers.14.self_attn.out_proj.bias: mean=0.0005, std=0.0362
transformer.layers.14.linear1.weight: mean=-0.0000, std=0.0082
transformer.layers.14.linear1.bias: mean=0.0001, std=0.0078
transformer.layers.14.linear2.weight: mean=-0.0000, std=0.0156
transformer.layers.14.linear2.bias: mean=-0.0001, std=0.0420
transformer.layers.14.norm1.weight: mean=0.0001, std=0.0473
transformer.layers.14.norm1.bias: mean=-0.0000, std=0.0456
transformer.layers.14.norm2.weight: mean=-0.0000, std=0.0475
transformer.layers.14.norm2.bias: mean=-0.0006, std=0.0455
transformer.layers.15.self_attn.in_proj_weight: mean=-0.0000, std=0.0121
transformer.layers.15.self_attn.in_proj_bias: mean=-0.0003, std=0.0117
transformer.layers.15.self_attn.out_proj.weight: mean=-0.0000, std=0.0247
transformer.layers.15.self_attn.out_proj.bias: mean=-0.0000, std=0.0350
transformer.layers.15.linear1.weight: mean=-0.0000, std=0.0081
transformer.layers.15.linear1.bias: mean=0.0002, std=0.0077
transformer.layers.15.linear2.weight: mean=0.0000, std=0.0153
transformer.layers.15.linear2.bias: mean=0.0001, std=0.0408
transformer.layers.15.norm1.weight: mean=0.0001, std=0.0463
transformer.layers.15.norm1.bias: mean=-0.0001, std=0.0438
transformer.layers.15.norm2.weight: mean=-0.0000, std=0.0466
transformer.layers.15.norm2.bias: mean=-0.0004, std=0.0435
transformer.layers.16.self_attn.in_proj_weight: mean=0.0000, std=0.0121
transformer.layers.16.self_attn.in_proj_bias: mean=-0.0005, std=0.0116
transformer.layers.16.self_attn.out_proj.weight: mean=-0.0000, std=0.0240
transformer.layers.16.self_attn.out_proj.bias: mean=-0.0003, std=0.0334
transformer.layers.16.linear1.weight: mean=-0.0000, std=0.0079
transformer.layers.16.linear1.bias: mean=0.0001, std=0.0075
transformer.layers.16.linear2.weight: mean=0.0000, std=0.0150
transformer.layers.16.linear2.bias: mean=0.0004, std=0.0388
transformer.layers.16.norm1.weight: mean=0.0002, std=0.0453
transformer.layers.16.norm1.bias: mean=-0.0000, std=0.0418
transformer.layers.16.norm2.weight: mean=-0.0000, std=0.0451
transformer.layers.16.norm2.bias: mean=-0.0004, std=0.0415
transformer.layers.17.self_attn.in_proj_weight: mean=0.0000, std=0.0116
transformer.layers.17.self_attn.in_proj_bias: mean=-0.0001, std=0.0110
transformer.layers.17.self_attn.out_proj.weight: mean=-0.0000, std=0.0235
transformer.layers.17.self_attn.out_proj.bias: mean=-0.0002, std=0.0326
transformer.layers.17.linear1.weight: mean=0.0000, std=0.0081
transformer.layers.17.linear1.bias: mean=0.0001, std=0.0076
transformer.layers.17.linear2.weight: mean=-0.0000, std=0.0153
transformer.layers.17.linear2.bias: mean=-0.0003, std=0.0395
transformer.layers.17.norm1.weight: mean=0.0002, std=0.0445
transformer.layers.17.norm1.bias: mean=-0.0001, std=0.0407
transformer.layers.17.norm2.weight: mean=-0.0000, std=0.0445
transformer.layers.17.norm2.bias: mean=0.0001, std=0.0407
transformer.layers.18.self_attn.in_proj_weight: mean=-0.0000, std=0.0119
transformer.layers.18.self_attn.in_proj_bias: mean=-0.0002, std=0.0113
transformer.layers.18.self_attn.out_proj.weight: mean=-0.0000, std=0.0244
transformer.layers.18.self_attn.out_proj.bias: mean=-0.0008, std=0.0338
transformer.layers.18.linear1.weight: mean=-0.0000, std=0.0084
transformer.layers.18.linear1.bias: mean=0.0001, std=0.0078
transformer.layers.18.linear2.weight: mean=-0.0000, std=0.0161
transformer.layers.18.linear2.bias: mean=-0.0004, std=0.0405
transformer.layers.18.norm1.weight: mean=0.0003, std=0.0445
transformer.layers.18.norm1.bias: mean=0.0003, std=0.0413
transformer.layers.18.norm2.weight: mean=0.0000, std=0.0447
transformer.layers.18.norm2.bias: mean=0.0010, std=0.0416
transformer.layers.19.self_attn.in_proj_weight: mean=0.0000, std=0.0126
transformer.layers.19.self_attn.in_proj_bias: mean=0.0001, std=0.0120
transformer.layers.19.self_attn.out_proj.weight: mean=0.0000, std=0.0266
transformer.layers.19.self_attn.out_proj.bias: mean=0.0000, std=0.0373
transformer.layers.19.linear1.weight: mean=0.0000, std=0.0093
transformer.layers.19.linear1.bias: mean=0.0001, std=0.0087
transformer.layers.19.linear2.weight: mean=0.0001, std=0.0173
transformer.layers.19.linear2.bias: mean=0.0012, std=0.0428
transformer.layers.19.norm1.weight: mean=0.0004, std=0.0471
transformer.layers.19.norm1.bias: mean=0.0008, std=0.0426
transformer.layers.19.norm2.weight: mean=0.0000, std=0.0474
transformer.layers.19.norm2.bias: mean=0.0002, std=0.0432
transformer.layers.20.self_attn.in_proj_weight: mean=0.0000, std=0.0134
transformer.layers.20.self_attn.in_proj_bias: mean=-0.0001, std=0.0127
transformer.layers.20.self_attn.out_proj.weight: mean=0.0000, std=0.0285
transformer.layers.20.self_attn.out_proj.bias: mean=0.0020, std=0.0396
transformer.layers.20.linear1.weight: mean=0.0000, std=0.0103
transformer.layers.20.linear1.bias: mean=-0.0001, std=0.0097
transformer.layers.20.linear2.weight: mean=0.0002, std=0.0196
transformer.layers.20.linear2.bias: mean=0.0019, std=0.0471
transformer.layers.20.norm1.weight: mean=0.0004, std=0.0496
transformer.layers.20.norm1.bias: mean=0.0011, std=0.0451
transformer.layers.20.norm2.weight: mean=0.0000, std=0.0505
transformer.layers.20.norm2.bias: mean=-0.0011, std=0.0466
transformer.layers.21.self_attn.in_proj_weight: mean=-0.0000, std=0.0152
transformer.layers.21.self_attn.in_proj_bias: mean=0.0001, std=0.0146
transformer.layers.21.self_attn.out_proj.weight: mean=0.0000, std=0.0313
transformer.layers.21.self_attn.out_proj.bias: mean=0.0012, std=0.0436
transformer.layers.21.linear1.weight: mean=0.0000, std=0.0122
transformer.layers.21.linear1.bias: mean=-0.0000, std=0.0114
transformer.layers.21.linear2.weight: mean=0.0002, std=0.0230
transformer.layers.21.linear2.bias: mean=0.0020, std=0.0541
transformer.layers.21.norm1.weight: mean=0.0001, std=0.0547
transformer.layers.21.norm1.bias: mean=0.0006, std=0.0493
transformer.layers.21.norm2.weight: mean=0.0000, std=0.0573
transformer.layers.21.norm2.bias: mean=-0.0003, std=0.0505
transformer.layers.22.self_attn.in_proj_weight: mean=-0.0000, std=0.0173
transformer.layers.22.self_attn.in_proj_bias: mean=0.0001, std=0.0166
transformer.layers.22.self_attn.out_proj.weight: mean=0.0000, std=0.0359
transformer.layers.22.self_attn.out_proj.bias: mean=0.0003, std=0.0493
transformer.layers.22.linear1.weight: mean=0.0000, std=0.0148
transformer.layers.22.linear1.bias: mean=0.0001, std=0.0139
transformer.layers.22.linear2.weight: mean=-0.0001, std=0.0294
transformer.layers.22.linear2.bias: mean=-0.0005, std=0.0663
transformer.layers.22.norm1.weight: mean=0.0003, std=0.0630
transformer.layers.22.norm1.bias: mean=0.0006, std=0.0549
transformer.layers.22.norm2.weight: mean=0.0000, std=0.0650
transformer.layers.22.norm2.bias: mean=-0.0001, std=0.0557
transformer.layers.23.self_attn.in_proj_weight: mean=-0.0000, std=0.0218
transformer.layers.23.self_attn.in_proj_bias: mean=-0.0005, std=0.0207
transformer.layers.23.self_attn.out_proj.weight: mean=-0.0001, std=0.0467
transformer.layers.23.self_attn.out_proj.bias: mean=-0.0038, std=0.0642
transformer.layers.23.linear1.weight: mean=0.0000, std=0.0199
transformer.layers.23.linear1.bias: mean=0.0001, std=0.0185
transformer.layers.23.linear2.weight: mean=-0.0003, std=0.0379
transformer.layers.23.linear2.bias: mean=-0.0027, std=0.0846
transformer.layers.23.norm1.weight: mean=0.0006, std=0.0751
transformer.layers.23.norm1.bias: mean=-0.0007, std=0.0645
transformer.layers.23.norm2.weight: mean=0.0013, std=0.0860
transformer.layers.23.norm2.bias: mean=-0.0018, std=0.0668
connector.0.weight: mean=-0.0000, std=0.1027
connector.0.bias: mean=0.0012, std=0.0834
connector.2.weight: mean=0.0001, std=0.0494
connector.2.bias: mean=0.0007, std=0.1042
Gradients clipped from 1.0000 to 1.0
Gradient norm: 1.0000
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153948/2412-153948-0011.flac
Waveform stats - mean: -0.0001, std: 0.0416, min: -0.5389, max: 0.5299
Resampled waveform stats - mean: -0.0001, std: 0.0416, min: -0.5389, max: 0.5299
Raw mel spectrogram stats - mean: 0.6412, std: 5.6375, min: 0.0000, max: 709.1660
Log mel spectrogram stats - mean: -6.0211, std: 3.9141, min: -13.7538, max: 6.5641
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9756, max: 3.2153
Mel spec shape: torch.Size([1, 80, 975])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9756, max: 3.2148
CNN output shape: torch.Size([1, 512, 61])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31232
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31232
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 218624
Inf count: 0
audio_emb.shape torch.Size([1, 61, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SO LONELY AND SO SOLEMN WITH THE SAD GREY CLOUDS ABOVE AND NO SOUND SAVE A LOST LAMB BLEATING UPON THE MOUNTAIN SIDE AS THOUGH ITS LITTLE HEART WERE BREAKING
Prediction: 0000000000000000000000000000000000000000000000000
Loss: 16.0233
outputs.loss tensor(16.0233, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0031.flac
Waveform stats - mean: 0.0000, std: 0.0394, min: -0.6429, max: 0.5655
Resampled waveform stats - mean: 0.0000, std: 0.0394, min: -0.6429, max: 0.5655
Raw mel spectrogram stats - mean: 0.5825, std: 7.2483, min: 0.0000, max: 498.9593
Log mel spectrogram stats - mean: -6.0128, std: 3.5699, min: -13.8056, max: 6.2125
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1829, max: 3.4246
Mel spec shape: torch.Size([1, 80, 543])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1836, max: 3.4238
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HAT TOO CLOSE TO ME I COULDN'T GET IT EXPLAINED CHUNKY THE BOYS ROARED
Prediction: 00000000000000000000000
Loss: 16.2733
outputs.loss tensor(16.2733, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/128104/1272-128104-0003.flac
Waveform stats - mean: 0.0000, std: 0.0711, min: -0.5635, max: 0.6203
Resampled waveform stats - mean: 0.0000, std: 0.0711, min: -0.5635, max: 0.6203
Raw mel spectrogram stats - mean: 1.8939, std: 15.0017, min: 0.0000, max: 1011.1890
Log mel spectrogram stats - mean: -5.0578, std: 4.0679, min: -13.7238, max: 6.9189
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1303, max: 2.9442
Mel spec shape: torch.Size([1, 80, 991])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1309, max: 2.9434
CNN output shape: torch.Size([1, 512, 62])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31744
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31744
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 222208
Inf count: 0
audio_emb.shape torch.Size([1, 62, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HAS GRAVE DOUBTS WHETHER SIR FREDERICK LEIGHTON'S WORK IS REALLY GREEK AFTER ALL AND CAN DISCOVER IN IT BUT LITTLE OF ROCKY ITHACA
Prediction: 00000000000000000000000000000000000000000
Loss: 15.8272
outputs.loss tensor(15.8272, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43363/5536-43363-0008.flac
Waveform stats - mean: -0.0001, std: 0.0689, min: -0.2611, max: 0.4070
Resampled waveform stats - mean: -0.0001, std: 0.0689, min: -0.2611, max: 0.4070
Raw mel spectrogram stats - mean: 1.7753, std: 9.1922, min: 0.0000, max: 341.4494
Log mel spectrogram stats - mean: -5.4735, std: 4.0343, min: -13.7906, max: 5.8332
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0616, max: 2.8026
Mel spec shape: torch.Size([1, 80, 1030])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0625, max: 2.8027
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AT THE END OF A YEAR FROM THE TIME OF DEATH THE RELATIVES MADE A PUBLIC FEAST AND GAVE AWAY THE CLOTHING AND OTHER GIFTS WHILE THE LOCK OF HAIR WAS INTERRED WITH APPROPRIATE CEREMONIES
Prediction: 0000000000000000000000000000000000000000000000000000
Loss: 15.9295
outputs.loss tensor(15.9295, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/73752/7850-73752-0017.flac
Waveform stats - mean: -0.0000, std: 0.0748, min: -0.6841, max: 0.6393
Resampled waveform stats - mean: -0.0000, std: 0.0748, min: -0.6841, max: 0.6393
Raw mel spectrogram stats - mean: 2.0921, std: 16.7533, min: 0.0000, max: 710.0491
Log mel spectrogram stats - mean: -5.5435, std: 4.0614, min: -13.7107, max: 6.5653
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0109, max: 2.9814
Mel spec shape: torch.Size([1, 80, 598])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0117, max: 2.9805
CNN output shape: torch.Size([1, 512, 38])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN THE PRESENT UNSETTLED THOUGH HOPEFUL STATE OF AFFAIRS FERDINAND WOULD NOT GO HOME
Prediction: 0000000000000000000000000000
Loss: 15.7536
outputs.loss tensor(15.7536, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0019.flac
Waveform stats - mean: 0.0009, std: 0.0165, min: -0.1509, max: 0.1865
Resampled waveform stats - mean: 0.0009, std: 0.0165, min: -0.1509, max: 0.1865
Raw mel spectrogram stats - mean: 0.0956, std: 0.8825, min: 0.0000, max: 122.3949
Log mel spectrogram stats - mean: -5.9237, std: 2.9128, min: -12.6540, max: 4.8073
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3106, max: 3.6840
Mel spec shape: torch.Size([1, 80, 391])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3105, max: 3.6836
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I ASKED AS SOON AS GEORGE HAD RETURNED TO MY SIDE
Prediction: 000000000000000000
Loss: 14.6876
outputs.loss tensor(14.6876, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110523/7976-110523-0015.flac
Waveform stats - mean: 0.0000, std: 0.0684, min: -0.9518, max: 0.6764
Resampled waveform stats - mean: 0.0000, std: 0.0684, min: -0.9518, max: 0.6764
Raw mel spectrogram stats - mean: 1.7525, std: 15.9649, min: 0.0000, max: 1249.6906
Log mel spectrogram stats - mean: -6.2006, std: 4.3239, min: -13.8039, max: 7.1307
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7585, max: 3.0832
Mel spec shape: torch.Size([1, 80, 776])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7588, max: 3.0840
CNN output shape: torch.Size([1, 512, 49])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25088
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25088
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 175616
Inf count: 0
audio_emb.shape torch.Size([1, 49, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GRETHEL SHE CRIED IN A PASSION GET SOME WATER QUICKLY BE HANSEL FAT OR LEAN THIS MORNING I WILL KILL AND COOK HIM
Prediction: 00000000000000000000000000000000000
Loss: 15.7894
outputs.loss tensor(15.7894, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147965/1993-147965-0000.flac
Waveform stats - mean: -0.0001, std: 0.0548, min: -0.4060, max: 0.4056
Resampled waveform stats - mean: -0.0001, std: 0.0548, min: -0.4060, max: 0.4056
Raw mel spectrogram stats - mean: 1.1227, std: 11.5167, min: 0.0000, max: 600.0988
Log mel spectrogram stats - mean: -5.9171, std: 4.0802, min: -13.7387, max: 6.3971
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9169, max: 3.0180
Mel spec shape: torch.Size([1, 80, 438])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9170, max: 3.0176
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GRANDFATHER CAME DOWN WEARING A WHITE SHIRT AND HIS SUNDAY COAT
Prediction: 00000000000000000000
Loss: 16.1066
outputs.loss tensor(16.1066, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9008/2902-9008-0006.flac
Waveform stats - mean: -0.0000, std: 0.0338, min: -0.4991, max: 0.5136
Resampled waveform stats - mean: -0.0000, std: 0.0338, min: -0.4991, max: 0.5136
Raw mel spectrogram stats - mean: 0.4277, std: 10.0199, min: 0.0000, max: 1341.5509
Log mel spectrogram stats - mean: -6.4894, std: 3.1553, min: -13.7718, max: 7.2016
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3080, max: 4.3391
Mel spec shape: torch.Size([1, 80, 1835])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3086, max: 4.3398
CNN output shape: torch.Size([1, 512, 115])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 58880
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 58880
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 412160
Inf count: 0
audio_emb.shape torch.Size([1, 115, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TO BE WELCOMED INTO THE CELESTIAL RANKS OF THE HEROIC TO RISE TO THE IMMORTAL GODS TO THE INEFFABLE POWERS ONWARD UPWARD EVER THROUGH AGES AND THROUGH ETERNITIES TILL I FIND MY HOME AT LAST AND VANISH IN THE GLORY OF THE NAMELESS AND THE ABSOLUTE ONE
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.0375
outputs.loss tensor(16.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0005.flac
Waveform stats - mean: 0.0000, std: 0.0444, min: -0.5049, max: 0.3628
Resampled waveform stats - mean: 0.0000, std: 0.0444, min: -0.5049, max: 0.3628
Raw mel spectrogram stats - mean: 0.7372, std: 4.2110, min: 0.0000, max: 120.4140
Log mel spectrogram stats - mean: -5.8652, std: 3.9630, min: -13.4094, max: 4.7909
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9037, max: 2.6889
Mel spec shape: torch.Size([1, 80, 250])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9033, max: 2.6895
CNN output shape: torch.Size([1, 512, 16])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GWYNPLAINE WAS A MOUNTEBANK
Prediction: 000000000000
Loss: 14.9046
outputs.loss tensor(14.9046, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0009.flac
Waveform stats - mean: -0.0001, std: 0.0602, min: -0.6314, max: 0.5041
Resampled waveform stats - mean: -0.0001, std: 0.0602, min: -0.6314, max: 0.5041
Raw mel spectrogram stats - mean: 1.3044, std: 7.5795, min: 0.0000, max: 731.3617
Log mel spectrogram stats - mean: -5.2616, std: 4.2388, min: -13.7023, max: 6.5949
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9913, max: 2.7972
Mel spec shape: torch.Size([1, 80, 1398])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9912, max: 2.7969
CNN output shape: torch.Size([1, 512, 88])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 45056
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 45056
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 315392
Inf count: 0
audio_emb.shape torch.Size([1, 88, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: CELERY AND NUT SALAD CUT ENOUGH CELERY FINE TO MEASURE TWO CUPS ADD ONE CUP OF FINELY SHREDDED OR SHAVED CABBAGE AND ONE AND ONE HALF CUPS OF WALNUT MEATS BROKEN IN SMALL PIECES BUT NOT CHOPPED
Prediction: 000000000000000000000000000000000000000000000000000000000000
Loss: 16.4358
outputs.loss tensor(16.4358, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0005.flac
Waveform stats - mean: 0.0000, std: 0.0672, min: -0.4797, max: 0.6887
Resampled waveform stats - mean: 0.0000, std: 0.0672, min: -0.4797, max: 0.6887
Raw mel spectrogram stats - mean: 1.6896, std: 12.1798, min: 0.0000, max: 526.4161
Log mel spectrogram stats - mean: -6.5158, std: 4.9350, min: -13.8111, max: 6.2661
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4783, max: 2.5901
Mel spec shape: torch.Size([1, 80, 910])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4785, max: 2.5898
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BEES TOO STRANGE TO SAY HAD THOUGHT IT WORTH THEIR WHILE TO COME HITHER POSSIBLY FROM THE RANGE OF HIVES BESIDE SOME FARM HOUSE MILES AWAY
Prediction: 000000000000000000000000000000000000000000
Loss: 15.9243
outputs.loss tensor(15.9243, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0000.flac
Waveform stats - mean: -0.0001, std: 0.0756, min: -0.4945, max: 0.4052
Resampled waveform stats - mean: -0.0001, std: 0.0756, min: -0.4945, max: 0.4052
Raw mel spectrogram stats - mean: 2.1386, std: 16.2468, min: 0.0000, max: 336.7249
Log mel spectrogram stats - mean: -6.0472, std: 3.7839, min: -13.5268, max: 5.8193
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9767, max: 3.1361
Mel spec shape: torch.Size([1, 80, 220])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9766, max: 3.1367
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: KIRKLEATHAM YEAST
Prediction: 00000000
Loss: 16.1467
outputs.loss tensor(16.1467, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0012.flac
Waveform stats - mean: 0.0000, std: 0.0553, min: -0.6530, max: 0.5437
Resampled waveform stats - mean: 0.0000, std: 0.0553, min: -0.6530, max: 0.5437
Raw mel spectrogram stats - mean: 1.1428, std: 9.5039, min: 0.0000, max: 483.5042
Log mel spectrogram stats - mean: -4.8610, std: 3.5688, min: -13.3877, max: 6.1811
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3892, max: 3.0941
Mel spec shape: torch.Size([1, 80, 1034])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3887, max: 3.0938
CNN output shape: torch.Size([1, 512, 65])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33280
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33280
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 232960
Inf count: 0
audio_emb.shape torch.Size([1, 65, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE MANICHAEANS BELIEVED THE ABSOLUTE OCCASIONALLY GIVES WAY AND THAT GOD HIMSELF SOMETIMES ABDICATES FOR A TIME SO ALSO OF THE WILL
Prediction: 000000000000000000000000000000000000000
Loss: 16.6144
outputs.loss tensor(16.6144, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0017.flac
Waveform stats - mean: 0.0000, std: 0.1194, min: -0.8252, max: 0.6474
Resampled waveform stats - mean: 0.0000, std: 0.1194, min: -0.8252, max: 0.6474
Raw mel spectrogram stats - mean: 5.3500, std: 31.7984, min: 0.0000, max: 764.1427
Log mel spectrogram stats - mean: -4.9842, std: 4.4883, min: -13.6852, max: 6.6388
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9386, max: 2.5896
Mel spec shape: torch.Size([1, 80, 705])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9385, max: 2.5898
CNN output shape: torch.Size([1, 512, 45])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE'S NO FEAR OF HIM SAID BURGESS CHEERILY IF HE GROWS UPROARIOUS WE'LL SOON GIVE HIM A TOUCH OF THE CAT
Prediction: 000000000000000000000000000000000000000
Loss: 15.7325
outputs.loss tensor(15.7325, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0014.flac
Waveform stats - mean: 0.0000, std: 0.0646, min: -0.6259, max: 0.5875
Resampled waveform stats - mean: 0.0000, std: 0.0646, min: -0.6259, max: 0.5875
Raw mel spectrogram stats - mean: 1.5292, std: 7.4185, min: 0.0000, max: 169.6168
Log mel spectrogram stats - mean: -5.6751, std: 4.6171, min: -13.7205, max: 5.1335
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7425, max: 2.3410
Mel spec shape: torch.Size([1, 80, 395])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7422, max: 2.3418
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GARNISH DISH THAT DRESSING IS MADE IN WITH A LITTLE GARLIC
Prediction: 0000000000000000000
Loss: 15.7556
outputs.loss tensor(15.7556, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0011.flac
Waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Resampled waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Raw mel spectrogram stats - mean: 1.5914, std: 21.0549, min: 0.0000, max: 1490.1473
Log mel spectrogram stats - mean: -7.2657, std: 4.2950, min: -13.7301, max: 7.3066
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5051, max: 3.3928
Mel spec shape: torch.Size([1, 80, 433])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5049, max: 3.3926
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IS IT ONLY THAT YOU'RE POOR WHY THAT'S NOTHING I'M POOR TOO SHE LAUGHED
Prediction: 000000000000000000000
Loss: 16.6380
outputs.loss tensor(16.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0032.flac
Waveform stats - mean: 0.0000, std: 0.0561, min: -0.7102, max: 0.4934
Resampled waveform stats - mean: 0.0000, std: 0.0561, min: -0.7102, max: 0.4934
Raw mel spectrogram stats - mean: 1.1645, std: 11.1728, min: 0.0000, max: 1124.8353
Log mel spectrogram stats - mean: -5.5093, std: 3.9894, min: -13.8040, max: 7.0254
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0792, max: 3.1420
Mel spec shape: torch.Size([1, 80, 1337])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0801, max: 3.1426
CNN output shape: torch.Size([1, 512, 84])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 43008
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 43008
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 301056
Inf count: 0
audio_emb.shape torch.Size([1, 84, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: PROBABLY BUT FORTUNATELY NOTHING FOR MY OWN TEETH PERHAPS IT BETRAYS THE SPECIES TO WHICH I BELONG BUT NOT TO MYSELF AS IS SUFFICIENTLY AGREEABLE TO ME BUT WHAT HAS HAPPENED TO YOU
Prediction: 000000000000000000000000000000000000000000000000000000
Loss: 16.3040
outputs.loss tensor(16.3040, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/286674/7850-286674-0015.flac
Waveform stats - mean: -0.0000, std: 0.0486, min: -0.3323, max: 0.3558
Resampled waveform stats - mean: -0.0000, std: 0.0486, min: -0.3323, max: 0.3558
Raw mel spectrogram stats - mean: 0.8847, std: 11.6796, min: 0.0000, max: 606.9446
Log mel spectrogram stats - mean: -6.7406, std: 3.7770, min: -13.7378, max: 6.4084
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8526, max: 3.4814
Mel spec shape: torch.Size([1, 80, 354])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8525, max: 3.4805
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY THOUGHT HE MIGHT BE GOING TO TAKE A NAP AFTER HIS DINNER
Prediction: 00000000000000000000
Loss: 15.6969
outputs.loss tensor(15.6969, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121123/84-121123-0003.flac
Waveform stats - mean: 0.0000, std: 0.0541, min: -0.4156, max: 0.3679
Resampled waveform stats - mean: 0.0000, std: 0.0541, min: -0.4156, max: 0.3679
Raw mel spectrogram stats - mean: 1.0934, std: 8.8055, min: 0.0000, max: 511.0482
Log mel spectrogram stats - mean: -5.9887, std: 3.9600, min: -13.8155, max: 6.2365
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9765, max: 3.0872
Mel spec shape: torch.Size([1, 80, 681])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9766, max: 3.0879
CNN output shape: torch.Size([1, 512, 43])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND THE CRY ISSUED FROM HIS PORES IF WE MAY THUS SPEAK A CRY FRIGHTFUL IN ITS SILENCE
Prediction: 000000000000000000000000000000
Loss: 15.6741
outputs.loss tensor(15.6741, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0004.flac
Waveform stats - mean: -0.0000, std: 0.0758, min: -0.4810, max: 0.5725
Resampled waveform stats - mean: -0.0000, std: 0.0758, min: -0.4810, max: 0.5725
Raw mel spectrogram stats - mean: 2.1567, std: 12.0320, min: 0.0000, max: 671.1321
Log mel spectrogram stats - mean: -4.8509, std: 4.3452, min: -13.8155, max: 6.5090
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0631, max: 2.6144
Mel spec shape: torch.Size([1, 80, 1099])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0625, max: 2.6152
CNN output shape: torch.Size([1, 512, 69])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 35328
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 35328
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 247296
Inf count: 0
audio_emb.shape torch.Size([1, 69, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ELECTRONIC EQUIPMENT CASCADED FROM THE WALL SHELVES AND A HEAVY DUTY CHAIN HOIST CAME LOOSE FROM ITS OVERHEAD TRACK PLUNGING TO THE FLOOR WITH A TERRIFYING CRASH
Prediction: 000000000000000000000000000000000000000000000000000
Loss: 16.1642
outputs.loss tensor(16.1642, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0011.flac
Waveform stats - mean: -0.0001, std: 0.0564, min: -0.4846, max: 0.4945
Resampled waveform stats - mean: -0.0001, std: 0.0564, min: -0.4846, max: 0.4945
Raw mel spectrogram stats - mean: 1.1854, std: 8.5117, min: 0.0000, max: 399.6275
Log mel spectrogram stats - mean: -4.6934, std: 3.5419, min: -13.5598, max: 5.9905
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5033, max: 3.0164
Mel spec shape: torch.Size([1, 80, 391])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5039, max: 3.0156
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NEARLY THE WHOLE POPULATION OF THE TOWN WAS ON FOOT TO SEE US LAND
Prediction: 0000000000000000000
Loss: 16.2588
outputs.loss tensor(16.2588, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9006/2902-9006-0011.flac
Waveform stats - mean: -0.0000, std: 0.0418, min: -0.3388, max: 0.4063
Resampled waveform stats - mean: -0.0000, std: 0.0418, min: -0.3388, max: 0.4063
Raw mel spectrogram stats - mean: 0.6517, std: 8.7089, min: 0.0000, max: 492.3206
Log mel spectrogram stats - mean: -5.9023, std: 3.1227, min: -13.1657, max: 6.1991
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3260, max: 3.8752
Mel spec shape: torch.Size([1, 80, 433])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3262, max: 3.8750
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND THE NEW BLOOD AT THE ERA OF THIS STORY WAS AT HAND
Prediction: 00000000000000
Loss: 16.0618
outputs.loss tensor(16.0618, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/304647/8842-304647-0005.flac
Waveform stats - mean: 0.0000, std: 0.0475, min: -0.2825, max: 0.3677
Resampled waveform stats - mean: 0.0000, std: 0.0475, min: -0.2825, max: 0.3677
Raw mel spectrogram stats - mean: 0.8441, std: 8.3978, min: 0.0000, max: 350.7211
Log mel spectrogram stats - mean: -7.3832, std: 3.8647, min: -13.7862, max: 5.8600
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6568, max: 3.4267
Mel spec shape: torch.Size([1, 80, 223])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6572, max: 3.4258
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IL POPOLO E UNA BESTIA
Prediction: 000000000
Loss: 15.1961
outputs.loss tensor(15.1961, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0011.flac
Waveform stats - mean: -0.0001, std: 0.0641, min: -0.6404, max: 0.6007
Resampled waveform stats - mean: -0.0001, std: 0.0641, min: -0.6404, max: 0.6007
Raw mel spectrogram stats - mean: 1.5381, std: 8.9004, min: 0.0000, max: 275.4884
Log mel spectrogram stats - mean: -5.9074, std: 4.3219, min: -13.7210, max: 5.6185
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8079, max: 2.6668
Mel spec shape: torch.Size([1, 80, 612])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8076, max: 2.6660
CNN output shape: torch.Size([1, 512, 39])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I NEVER SAW PEOPLE LIKE THE SNELLINGS FOR POSSESSING RELATIVES IN ALL SORTS OF LINES
Prediction: 0000000000000000000000000
Loss: 16.0671
outputs.loss tensor(16.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/168635/174-168635-0013.flac
Waveform stats - mean: -0.0001, std: 0.0322, min: -0.4560, max: 0.5281
Resampled waveform stats - mean: -0.0001, std: 0.0322, min: -0.4560, max: 0.5281
Raw mel spectrogram stats - mean: 0.3890, std: 2.5819, min: 0.0000, max: 96.0714
Log mel spectrogram stats - mean: -6.7684, std: 3.9183, min: -13.7715, max: 4.5651
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7873, max: 2.8925
Mel spec shape: torch.Size([1, 80, 588])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7871, max: 2.8926
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WEEK FOLLOWED WEEK THESE TWO BEINGS LED A HAPPY LIFE IN THAT HOVEL
Prediction: 000000000000000000
Loss: 16.4920
outputs.loss tensor(16.4920, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43363/5536-43363-0006.flac
Waveform stats - mean: -0.0001, std: 0.0725, min: -0.2906, max: 0.4322
Resampled waveform stats - mean: -0.0001, std: 0.0725, min: -0.2906, max: 0.4322
Raw mel spectrogram stats - mean: 1.9675, std: 8.3408, min: 0.0000, max: 240.4639
Log mel spectrogram stats - mean: -4.9434, std: 4.0105, min: -13.6691, max: 5.4826
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1757, max: 2.5997
Mel spec shape: torch.Size([1, 80, 1199])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1758, max: 2.5996
CNN output shape: torch.Size([1, 512, 75])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 38400
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 38400
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 268800
Inf count: 0
audio_emb.shape torch.Size([1, 75, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IF A MAN WERE SLAIN IN BATTLE IT WAS AN OLD CUSTOM TO PLACE HIS BODY AGAINST A TREE OR ROCK IN A SITTING POSITION ALWAYS FACING THE ENEMY TO INDICATE HIS UNDAUNTED DEFIANCE AND BRAVERY EVEN IN DEATH
Prediction: 00000000000000000000000000000000000000000000000000000000
Loss: 16.3906
outputs.loss tensor(16.3906, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0016.flac
Waveform stats - mean: -0.0001, std: 0.0676, min: -0.3365, max: 0.4522
Resampled waveform stats - mean: -0.0001, std: 0.0676, min: -0.3365, max: 0.4522
Raw mel spectrogram stats - mean: 1.7001, std: 10.9124, min: 0.0000, max: 458.7444
Log mel spectrogram stats - mean: -5.3761, std: 4.0572, min: -13.7704, max: 6.1285
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0690, max: 2.8356
Mel spec shape: torch.Size([1, 80, 506])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0684, max: 2.8359
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NECESSITY HAD BECOME HIS GRIM MASTER AND THE FOLLOWING WEEK HE WAS GOING TO WORK
Prediction: 0000000000000000000000
Loss: 16.3034
outputs.loss tensor(16.3034, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/136532/251-136532-0012.flac
Waveform stats - mean: 0.0000, std: 0.0970, min: -0.6647, max: 0.8186
Resampled waveform stats - mean: 0.0000, std: 0.0970, min: -0.6647, max: 0.8186
Raw mel spectrogram stats - mean: 3.5212, std: 19.1377, min: 0.0000, max: 1345.3323
Log mel spectrogram stats - mean: -4.6238, std: 4.5603, min: -13.8102, max: 7.2044
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0144, max: 2.5937
Mel spec shape: torch.Size([1, 80, 880])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0137, max: 2.5938
CNN output shape: torch.Size([1, 512, 55])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A FLOODLIGHT WAS ON IN THE ROOM INSIDE AND LATTIMER WAS GOING AROUND LOOKING AT THINGS WHILE A SPACE FORCE OFFICER STOOD BY THE DOOR
Prediction: 00000000000000000000000000000000000000
Loss: 16.1684
outputs.loss tensor(16.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0047.flac
Waveform stats - mean: 0.0000, std: 0.0245, min: -0.1504, max: 0.2005
Resampled waveform stats - mean: 0.0000, std: 0.0245, min: -0.1504, max: 0.2005
Raw mel spectrogram stats - mean: 0.2250, std: 1.2161, min: 0.0000, max: 53.3577
Log mel spectrogram stats - mean: -6.9892, std: 4.1115, min: -13.8002, max: 3.9770
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6566, max: 2.6672
Mel spec shape: torch.Size([1, 80, 813])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6562, max: 2.6680
CNN output shape: torch.Size([1, 512, 51])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: LOAFING WAS ALL VERY WELL FOR THESE FELLOWS WHO KNEW NOT MISTER VLADIMIR AND HAD WOMEN TO FALL BACK UPON WHEREAS HE HAD A WOMAN TO PROVIDE FOR
Prediction: 000000000000000000000000000000000000000000000
Loss: 15.3775
outputs.loss tensor(15.3775, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0023.flac
Waveform stats - mean: -0.0000, std: 0.0444, min: -0.4143, max: 0.3372
Resampled waveform stats - mean: -0.0000, std: 0.0444, min: -0.4143, max: 0.3372
Raw mel spectrogram stats - mean: 0.7365, std: 3.3240, min: 0.0000, max: 68.4956
Log mel spectrogram stats - mean: -6.8383, std: 4.6446, min: -13.7697, max: 4.2268
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4924, max: 2.3824
Mel spec shape: torch.Size([1, 80, 261])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4922, max: 2.3828
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I DID NOT ASK I WAS BEYOND IT
Prediction: 00000000000
Loss: 14.6242
outputs.loss tensor(14.6242, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0023.flac
Waveform stats - mean: 0.0000, std: 0.0767, min: -0.5139, max: 0.4453
Resampled waveform stats - mean: 0.0000, std: 0.0767, min: -0.5139, max: 0.4453
Raw mel spectrogram stats - mean: 2.2013, std: 14.3749, min: 0.0000, max: 456.0528
Log mel spectrogram stats - mean: -5.6070, std: 4.9175, min: -13.8136, max: 6.1226
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6689, max: 2.3853
Mel spec shape: torch.Size([1, 80, 480])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6689, max: 2.3848
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE THIRD DAY IT WAS REPORTED THAT THE YANKEES HAD TAKEN POSITION ON THE MURFREESBORO PIKE
Prediction: 000000000000000000000000000000
Loss: 15.4271
outputs.loss tensor(15.4271, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0012.flac
Waveform stats - mean: 0.0000, std: 0.0801, min: -0.5250, max: 0.4955
Resampled waveform stats - mean: 0.0000, std: 0.0801, min: -0.5250, max: 0.4955
Raw mel spectrogram stats - mean: 2.4018, std: 16.1484, min: 0.0000, max: 520.1006
Log mel spectrogram stats - mean: -4.7466, std: 4.0307, min: -13.7783, max: 6.2540
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2407, max: 2.7292
Mel spec shape: torch.Size([1, 80, 288])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2402, max: 2.7285
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: OFFICERS COULD NOT CURB THE MEN TO KEEP IN LINE
Prediction: 00000000000000
Loss: 15.4261
outputs.loss tensor(15.4261, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0009.flac
Waveform stats - mean: -0.0000, std: 0.0640, min: -0.4945, max: 0.5479
Resampled waveform stats - mean: -0.0000, std: 0.0640, min: -0.4945, max: 0.5479
Raw mel spectrogram stats - mean: 1.5382, std: 7.7495, min: 0.0000, max: 443.0391
Log mel spectrogram stats - mean: -5.3782, std: 4.5558, min: -13.7942, max: 6.0937
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8473, max: 2.5181
Mel spec shape: torch.Size([1, 80, 584])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8477, max: 2.5176
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TOM'S EYES FOCUSED IN HORROR ON THE WRECKAGE ENVELOPED BY STILL BILLOWING DUST
Prediction: 00000000000000000000000000000
Loss: 16.0381
outputs.loss tensor(16.0381, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0019.flac
Waveform stats - mean: -0.0000, std: 0.0756, min: -0.7445, max: 0.4966
Resampled waveform stats - mean: -0.0000, std: 0.0756, min: -0.7445, max: 0.4966
Raw mel spectrogram stats - mean: 2.1339, std: 13.5401, min: 0.0000, max: 958.5149
Log mel spectrogram stats - mean: -4.7492, std: 3.8125, min: -13.3496, max: 6.8654
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2559, max: 3.0465
Mel spec shape: torch.Size([1, 80, 941])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2559, max: 3.0469
CNN output shape: torch.Size([1, 512, 59])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SOME OF THE PREPARATIONS OF MAIZE FLOUR ARE VERY GOOD AND WHEN PARTAKEN IN MODERATION SUITABLE FOOD FOR ALMOST EVERYBODY
Prediction: 000000000000000000000000000000000
Loss: 16.3695
outputs.loss tensor(16.3695, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0024.flac
Waveform stats - mean: -0.0001, std: 0.0393, min: -0.4776, max: 0.3272
Resampled waveform stats - mean: -0.0001, std: 0.0393, min: -0.4776, max: 0.3272
Raw mel spectrogram stats - mean: 0.5781, std: 4.1569, min: 0.0000, max: 177.9632
Log mel spectrogram stats - mean: -7.6556, std: 5.0337, min: -13.8155, max: 5.1816
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.2237, max: 2.5503
Mel spec shape: torch.Size([1, 80, 420])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.2236, max: 2.5508
CNN output shape: torch.Size([1, 512, 27])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE BEGAN PRESENTLY TO RELENT AND SPOKE TO ME IN A KINDER MANNER
Prediction: 0000000000000000000000
Loss: 15.3811
outputs.loss tensor(15.3811, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0017.flac
Waveform stats - mean: -0.0000, std: 0.0229, min: -0.1770, max: 0.1982
Resampled waveform stats - mean: -0.0000, std: 0.0229, min: -0.1770, max: 0.1982
Raw mel spectrogram stats - mean: 0.1956, std: 1.3353, min: 0.0000, max: 55.0962
Log mel spectrogram stats - mean: -7.8397, std: 3.9469, min: -13.7787, max: 4.0091
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5047, max: 3.0021
Mel spec shape: torch.Size([1, 80, 274])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5049, max: 3.0020
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: VERY CHARACTERISTIC PERFECTLY TYPICAL
Prediction: 000000000
Loss: 17.5726
outputs.loss tensor(17.5726, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0016.flac
Waveform stats - mean: -0.0000, std: 0.0566, min: -0.3160, max: 0.3138
Resampled waveform stats - mean: -0.0000, std: 0.0566, min: -0.3160, max: 0.3138
Raw mel spectrogram stats - mean: 1.1990, std: 6.7086, min: 0.0000, max: 185.1914
Log mel spectrogram stats - mean: -6.8431, std: 4.4072, min: -13.8133, max: 5.2214
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5815, max: 2.7374
Mel spec shape: torch.Size([1, 80, 898])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5811, max: 2.7383
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY CLUSTER AROUND ME THEIR HANDS ARE TALONED THEIR EYES ARE RED LIKE FLAME BURNING IN DARKNESS
Prediction: 00000000000000000000000000000
Loss: 16.0339
outputs.loss tensor(16.0339, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0011.flac
Waveform stats - mean: -0.0000, std: 0.0493, min: -0.4224, max: 0.3181
Resampled waveform stats - mean: -0.0000, std: 0.0493, min: -0.4224, max: 0.3181
Raw mel spectrogram stats - mean: 0.9108, std: 6.9663, min: 0.0000, max: 319.9955
Log mel spectrogram stats - mean: -5.5289, std: 3.5777, min: -13.6293, max: 5.7683
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2641, max: 3.1577
Mel spec shape: torch.Size([1, 80, 447])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2637, max: 3.1582
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE GREW RESTLESS AS HE RUMINATED AND THEN DECIDED THAT PERHAPS IT WAS NOTHING
Prediction: 00000000000000000000000
Loss: 15.7118
outputs.loss tensor(15.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0002.flac
Waveform stats - mean: 0.0000, std: 0.0886, min: -0.6021, max: 0.7272
Resampled waveform stats - mean: 0.0000, std: 0.0886, min: -0.6021, max: 0.7272
Raw mel spectrogram stats - mean: 2.9385, std: 18.4960, min: 0.0000, max: 854.0013
Log mel spectrogram stats - mean: -5.1029, std: 4.2929, min: -13.7748, max: 6.7499
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0200, max: 2.7610
Mel spec shape: torch.Size([1, 80, 1334])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0195, max: 2.7617
CNN output shape: torch.Size([1, 512, 84])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 43008
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 43008
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 301056
Inf count: 0
audio_emb.shape torch.Size([1, 84, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE CUT ON HIS CHEST STILL DRIPPING BLOOD THE ACHE OF HIS OVERSTRAINED EYES EVEN THE SOARING ARENA AROUND HIM WITH THE THOUSANDS OF SPECTATORS WERE TRIVIALITIES NOT WORTH THINKING ABOUT
Prediction: 00000000000000000000000000000000000000000000000000000
Loss: 16.6195
outputs.loss tensor(16.6195, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0062.flac
Waveform stats - mean: -0.0000, std: 0.0831, min: -0.5670, max: 0.4272
Resampled waveform stats - mean: -0.0000, std: 0.0831, min: -0.5670, max: 0.4272
Raw mel spectrogram stats - mean: 2.5761, std: 15.2234, min: 0.0000, max: 457.1154
Log mel spectrogram stats - mean: -5.8223, std: 4.3673, min: -13.7429, max: 6.1249
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8136, max: 2.7356
Mel spec shape: torch.Size([1, 80, 425])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8135, max: 2.7363
CNN output shape: torch.Size([1, 512, 27])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE MIXED A TEASPOONFUL OF THIS IN A PANNIKIN OF WATER AND DRANK IT
Prediction: 00000000000000000000000
Loss: 15.4792
outputs.loss tensor(15.4792, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0025.flac
Waveform stats - mean: -0.0000, std: 0.0665, min: -0.3561, max: 0.3847
Resampled waveform stats - mean: -0.0000, std: 0.0665, min: -0.3561, max: 0.3847
Raw mel spectrogram stats - mean: 1.6544, std: 8.5785, min: 0.0000, max: 315.1205
Log mel spectrogram stats - mean: -6.2132, std: 5.1112, min: -13.8101, max: 5.7530
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4863, max: 2.3412
Mel spec shape: torch.Size([1, 80, 247])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4863, max: 2.3418
CNN output shape: torch.Size([1, 512, 16])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I THOUGHT IT HAD BEEN TORN FROM MY SHOULDER
Prediction: 0000000000000000
Loss: 15.9108
outputs.loss tensor(15.9108, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0046.flac
Waveform stats - mean: -0.0000, std: 0.0562, min: -0.4093, max: 0.2816
Resampled waveform stats - mean: -0.0000, std: 0.0562, min: -0.4093, max: 0.2816
Raw mel spectrogram stats - mean: 1.1818, std: 7.0281, min: 0.0000, max: 286.8850
Log mel spectrogram stats - mean: -5.3443, std: 3.6300, min: -13.4643, max: 5.6591
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2369, max: 3.0313
Mel spec shape: torch.Size([1, 80, 536])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2363, max: 3.0312
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS NOT CULTIVATED IN ENGLAND BEING PRINCIPALLY CONFINED TO THE EAST
Prediction: 00000000000000000000000
Loss: 15.9731
outputs.loss tensor(15.9731, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0013.flac
Waveform stats - mean: -0.0000, std: 0.0750, min: -0.6378, max: 0.5604
Resampled waveform stats - mean: -0.0000, std: 0.0750, min: -0.6378, max: 0.5604
Raw mel spectrogram stats - mean: 2.0996, std: 8.8244, min: 0.0000, max: 256.2353
Log mel spectrogram stats - mean: -3.0818, std: 3.2775, min: -11.6929, max: 5.5461
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.6273, max: 2.6324
Mel spec shape: torch.Size([1, 80, 612])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.6270, max: 2.6328
CNN output shape: torch.Size([1, 512, 39])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TO NO HOME IN THE LAND DID THE GREAT TROUBLE BRING A MORE SUDDEN CHANGE THAN THE LITTLE COTTAGE IN THE LANE
Prediction: 0000000000000000000000000000000
Loss: 16.1024
outputs.loss tensor(16.1024, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0001.flac
Waveform stats - mean: -0.0000, std: 0.0262, min: -0.2143, max: 0.2651
Resampled waveform stats - mean: -0.0000, std: 0.0262, min: -0.2143, max: 0.2651
Raw mel spectrogram stats - mean: 0.2549, std: 1.9376, min: 0.0000, max: 88.5826
Log mel spectrogram stats - mean: -7.0835, std: 3.7183, min: -13.5797, max: 4.4839
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7471, max: 3.1110
Mel spec shape: torch.Size([1, 80, 323])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7471, max: 3.1113
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS IS A MISTAKE THOUGH A PERFECTLY NATURAL ONE
Prediction: 000000000000000
Loss: 16.2819
outputs.loss tensor(16.2819, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0008.flac
Waveform stats - mean: 0.0000, std: 0.0754, min: -0.4294, max: 0.5243
Resampled waveform stats - mean: 0.0000, std: 0.0754, min: -0.4294, max: 0.5243
Raw mel spectrogram stats - mean: 2.1342, std: 11.2328, min: 0.0000, max: 722.8625
Log mel spectrogram stats - mean: -5.4166, std: 4.7500, min: -13.8155, max: 6.5832
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7682, max: 2.5263
Mel spec shape: torch.Size([1, 80, 653])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7686, max: 2.5254
CNN output shape: torch.Size([1, 512, 41])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN TOM WHO HAD BEEN STUNNED BY SOME FALLING DEBRIS RAISED HIMSELF TO A SITTING POSITION
Prediction: 000000000000000000000000000
Loss: 16.1597
outputs.loss tensor(16.1597, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0033.flac
Waveform stats - mean: 0.0000, std: 0.0190, min: -0.1432, max: 0.1988
Resampled waveform stats - mean: 0.0000, std: 0.0190, min: -0.1432, max: 0.1988
Raw mel spectrogram stats - mean: 0.1354, std: 0.8812, min: 0.0000, max: 25.9748
Log mel spectrogram stats - mean: -7.6185, std: 3.8914, min: -13.7798, max: 3.2571
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5833, max: 2.7948
Mel spec shape: torch.Size([1, 80, 366])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5830, max: 2.7949
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: COMRADE OSSIPON'S FACE TWITCHED WITH EXASPERATION
Prediction: 0000000000000000
Loss: 16.1151
outputs.loss tensor(16.1151, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0017.flac
Waveform stats - mean: 0.0000, std: 0.0756, min: -0.4384, max: 0.4123
Resampled waveform stats - mean: 0.0000, std: 0.0756, min: -0.4384, max: 0.4123
Raw mel spectrogram stats - mean: 2.1392, std: 12.9353, min: 0.0000, max: 460.6570
Log mel spectrogram stats - mean: -6.6588, std: 5.0938, min: -13.8096, max: 6.1327
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4038, max: 2.5112
Mel spec shape: torch.Size([1, 80, 302])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4033, max: 2.5117
CNN output shape: torch.Size([1, 512, 19])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ON MONDAY MORNING I TOO CAPTURED ME A MULE
Prediction: 00000000000000
Loss: 15.9399
outputs.loss tensor(15.9399, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0009.flac
Waveform stats - mean: -0.0001, std: 0.0708, min: -0.2902, max: 0.4914
Resampled waveform stats - mean: -0.0001, std: 0.0708, min: -0.2902, max: 0.4914
Raw mel spectrogram stats - mean: 1.8763, std: 8.5139, min: 0.0000, max: 315.3568
Log mel spectrogram stats - mean: -4.9818, std: 4.1377, min: -13.4305, max: 5.7537
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0419, max: 2.5946
Mel spec shape: torch.Size([1, 80, 665])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0410, max: 2.5938
CNN output shape: torch.Size([1, 512, 42])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BEFORE THIS CALAMITY CAME UPON US YOU COULD NOT FIND ANYWHERE A HAPPIER HOME THAN THAT CREATED BY THE INDIAN WOMAN
Prediction: 0000000000000000000000000000000000
Loss: 15.8576
outputs.loss tensor(15.8576, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0046.flac
Waveform stats - mean: -0.0000, std: 0.0695, min: -0.6309, max: 0.5536
Resampled waveform stats - mean: -0.0000, std: 0.0695, min: -0.6309, max: 0.5536
Raw mel spectrogram stats - mean: 1.8046, std: 14.8726, min: 0.0000, max: 738.8250
Log mel spectrogram stats - mean: -5.8297, std: 3.8099, min: -13.7599, max: 6.6051
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0815, max: 3.2638
Mel spec shape: torch.Size([1, 80, 264])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0820, max: 3.2637
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ILLUSTRATION BASIL
Prediction: 000000
Loss: 16.2821
outputs.loss tensor(16.2821, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0000.flac
Waveform stats - mean: -0.0000, std: 0.0514, min: -0.5088, max: 0.5272
Resampled waveform stats - mean: -0.0000, std: 0.0514, min: -0.5088, max: 0.5272
Raw mel spectrogram stats - mean: 0.9899, std: 6.1632, min: 0.0000, max: 259.5643
Log mel spectrogram stats - mean: -7.5788, std: 5.3289, min: -13.8148, max: 5.5590
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.1702, max: 2.4654
Mel spec shape: torch.Size([1, 80, 260])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.1699, max: 2.4648
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ADVANCE INTO TENNESSEE
Prediction: 000000
Loss: 17.3288
outputs.loss tensor(17.3288, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0015.flac
Waveform stats - mean: -0.0000, std: 0.0658, min: -0.6584, max: 0.6744
Resampled waveform stats - mean: -0.0000, std: 0.0658, min: -0.6584, max: 0.6744
Raw mel spectrogram stats - mean: 1.6201, std: 30.8144, min: 0.0000, max: 2927.8120
Log mel spectrogram stats - mean: -6.9002, std: 4.2131, min: -13.8146, max: 7.9820
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6412, max: 3.5324
Mel spec shape: torch.Size([1, 80, 833])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6416, max: 3.5332
CNN output shape: torch.Size([1, 512, 53])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27136
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27136
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 189952
Inf count: 0
audio_emb.shape torch.Size([1, 53, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WELL THEN I WENT INTO LODGINGS THAT WICKED WOMAN HAD LEFT ME ONE STREET SUIT AND TO DAY THEY TURNED ME OUT BECAUSE MY MONEY WAS ALL GONE
Prediction: 00000000000000000000000000000000000000000
Loss: 15.9415
outputs.loss tensor(15.9415, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302196/8842-302196-0009.flac
Waveform stats - mean: -0.0002, std: 0.0610, min: -0.3867, max: 0.5738
Resampled waveform stats - mean: -0.0002, std: 0.0610, min: -0.3867, max: 0.5738
Raw mel spectrogram stats - mean: 1.3710, std: 8.1082, min: 0.0000, max: 273.9251
Log mel spectrogram stats - mean: -4.0695, std: 3.3668, min: -13.8053, max: 5.6129
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.8917, max: 2.8758
Mel spec shape: torch.Size([1, 80, 706])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.8926, max: 2.8750
CNN output shape: torch.Size([1, 512, 45])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS THEREFORE AND ON ALL ACCOUNTS UNNECESSARY TO SAY MUCH MORE OF THE WORK HERE THAN IT SAYS FOR ITSELF
Prediction: 0000000000000000000000000000
Loss: 15.9729
outputs.loss tensor(15.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0011.flac
Waveform stats - mean: -0.0001, std: 0.0661, min: -0.4437, max: 0.4780
Resampled waveform stats - mean: -0.0001, std: 0.0661, min: -0.4437, max: 0.4780
Raw mel spectrogram stats - mean: 1.6291, std: 11.1419, min: 0.0000, max: 644.8027
Log mel spectrogram stats - mean: -5.0474, std: 3.8436, min: -13.1866, max: 6.4689
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1176, max: 2.9962
Mel spec shape: torch.Size([1, 80, 1073])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1172, max: 2.9961
CNN output shape: torch.Size([1, 512, 68])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34816
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34816
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 243712
Inf count: 0
audio_emb.shape torch.Size([1, 68, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: CONSEQUENTLY BOTH MOTHER AND FATHER BEGAN THEIR EDUCATION AT THE POST THEY WERE SENT TO THE FACTOR'S SCHOOL AND TWO WINTERS WERE PASSED IN PORT ARTHUR THAT THEY MIGHT HAVE THE ADVANTAGE OF THOROUGHLY EQUIPPED SCHOOLS
Prediction: 00000000000000000000000000000000000000000000000000000000000000
Loss: 16.3569
outputs.loss tensor(16.3569, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0004.flac
Waveform stats - mean: -0.0001, std: 0.0567, min: -0.5422, max: 0.4881
Resampled waveform stats - mean: -0.0001, std: 0.0567, min: -0.5422, max: 0.4881
Raw mel spectrogram stats - mean: 1.2020, std: 6.2435, min: 0.0000, max: 210.8438
Log mel spectrogram stats - mean: -4.6835, std: 3.8201, min: -13.4405, max: 5.3511
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2924, max: 2.6268
Mel spec shape: torch.Size([1, 80, 552])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2930, max: 2.6270
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WE TOOK OUR WAY THROUGH POOR AND SPARSE MEADOWS WHICH MADE A DESPERATE EFFORT EVERY YEAR TO SHOW A LITTLE GREEN
Prediction: 000000000000000000000000000000
Loss: 16.2923
outputs.loss tensor(16.2923, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0029.flac
Waveform stats - mean: -0.0001, std: 0.0631, min: -0.4686, max: 0.4521
Resampled waveform stats - mean: -0.0001, std: 0.0631, min: -0.4686, max: 0.4521
Raw mel spectrogram stats - mean: 1.3914, std: 8.5212, min: 0.0000, max: 530.0928
Log mel spectrogram stats - mean: -4.4173, std: 3.7784, min: -13.6821, max: 6.2731
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4521, max: 2.8294
Mel spec shape: torch.Size([1, 80, 778])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4512, max: 2.8301
CNN output shape: torch.Size([1, 512, 49])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25088
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25088
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 175616
Inf count: 0
audio_emb.shape torch.Size([1, 49, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: STRAIN THE SAUCE THROUGH A FINE COLLANDER AND ADD A FEW RAISINS A PIECE OF HONEY CAKE OR GINGER SNAPS AND THE MEAT OF ONE FRESH TOMATO
Prediction: 0000000000000000000000000000000000000000000
Loss: 15.6597
outputs.loss tensor(15.6597, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0013.flac
Waveform stats - mean: 0.0001, std: 0.0688, min: -0.3453, max: 0.4264
Resampled waveform stats - mean: 0.0001, std: 0.0688, min: -0.3453, max: 0.4264
Raw mel spectrogram stats - mean: 1.7730, std: 9.4522, min: 0.0000, max: 294.8514
Log mel spectrogram stats - mean: -6.4226, std: 4.8454, min: -13.8149, max: 5.6865
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5256, max: 2.4991
Mel spec shape: torch.Size([1, 80, 823])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5254, max: 2.5000
CNN output shape: torch.Size([1, 512, 52])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26624
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26624
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 186368
Inf count: 0
audio_emb.shape torch.Size([1, 52, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BY WHICH A SOUL IS DRAWN FROM ITS BODY AND ACROSS GULFS OF ECHOING SPACE RETURNED THE MAN ON THE MAT
Prediction: 00000000000000000000000000000
Loss: 15.3322
outputs.loss tensor(15.3322, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0002.flac
Waveform stats - mean: -0.0001, std: 0.0571, min: -0.5296, max: 0.3871
Resampled waveform stats - mean: -0.0001, std: 0.0571, min: -0.5296, max: 0.3871
Raw mel spectrogram stats - mean: 1.2200, std: 7.4960, min: 0.0000, max: 271.3760
Log mel spectrogram stats - mean: -5.2020, std: 3.8755, min: -13.2224, max: 5.6035
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0695, max: 2.7881
Mel spec shape: torch.Size([1, 80, 549])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0703, max: 2.7891
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: OUR TWO HORSES WITH THE LUGGAGE FOLLOWED OF THEIR OWN ACCORD WITHOUT REQUIRING WHIP OR SPUR
Prediction: 00000000000000000000000000
Loss: 16.4905
outputs.loss tensor(16.4905, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0003.flac
Waveform stats - mean: 0.0000, std: 0.0836, min: -0.6313, max: 0.6075
Resampled waveform stats - mean: 0.0000, std: 0.0836, min: -0.6313, max: 0.6075
Raw mel spectrogram stats - mean: 2.6140, std: 18.8614, min: 0.0000, max: 758.2056
Log mel spectrogram stats - mean: -4.7209, std: 3.5506, min: -13.6142, max: 6.6310
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5048, max: 3.1972
Mel spec shape: torch.Size([1, 80, 541])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5039, max: 3.1973
CNN output shape: torch.Size([1, 512, 34])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE LONG PEPPER IS LESS AROMATIC THAN THE BLACK BUT ITS OIL IS MORE PUNGENT
Prediction: 000000000000000000000
Loss: 16.5181
outputs.loss tensor(16.5181, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0014.flac
Waveform stats - mean: -0.0000, std: 0.0674, min: -0.4295, max: 0.4696
Resampled waveform stats - mean: -0.0000, std: 0.0674, min: -0.4295, max: 0.4696
Raw mel spectrogram stats - mean: 1.7033, std: 14.3222, min: 0.0000, max: 1010.5431
Log mel spectrogram stats - mean: -5.8296, std: 4.2417, min: -13.8155, max: 6.9182
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8827, max: 3.0053
Mel spec shape: torch.Size([1, 80, 819])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8828, max: 3.0059
CNN output shape: torch.Size([1, 512, 52])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26624
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26624
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 186368
Inf count: 0
audio_emb.shape torch.Size([1, 52, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THESE STANDARDS TO THE REARWARD LONGER WERE THAN WAS MY SIGHT AND AS IT SEEMED TO ME TEN PACES WERE THE OUTERMOST APART
Prediction: 00000000000000000000000000000000000000
Loss: 15.9944
outputs.loss tensor(15.9944, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/161169/2803-161169-0011.flac
Waveform stats - mean: -0.0000, std: 0.0303, min: -0.2442, max: 0.1827
Resampled waveform stats - mean: -0.0000, std: 0.0303, min: -0.2442, max: 0.1827
Raw mel spectrogram stats - mean: 0.3405, std: 1.9172, min: 0.0000, max: 69.9845
Log mel spectrogram stats - mean: -7.2647, std: 4.2494, min: -13.7729, max: 4.2483
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5316, max: 2.7093
Mel spec shape: torch.Size([1, 80, 550])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5312, max: 2.7090
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT SHOULD WE HAVE DONE IF EVERYBODY HAD KEPT ON BURNING WOOD TO THIS DAY
Prediction: 000000000000000000000
Loss: 15.8669
outputs.loss tensor(15.8669, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/304647/8842-304647-0004.flac
Waveform stats - mean: 0.0000, std: 0.0619, min: -0.5187, max: 0.7082
Resampled waveform stats - mean: 0.0000, std: 0.0619, min: -0.5187, max: 0.7082
Raw mel spectrogram stats - mean: 1.4374, std: 18.1199, min: 0.0000, max: 1394.8689
Log mel spectrogram stats - mean: -5.6920, std: 3.6082, min: -13.7828, max: 7.2406
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2423, max: 3.5842
Mel spec shape: torch.Size([1, 80, 1016])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.5840
CNN output shape: torch.Size([1, 512, 64])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MONEY IS FALSE AND LIGHT UNLESS IT BE BOUGHT BY A MAN'S OWN WORTHY QUALITIES AND BLOOD IS SUCH THAT ITS CORRUPT DISEASE AND IGNORANT PRETENCE ARE FOUL TO SEE
Prediction: 00000000000000000000000000000000000000000000000
Loss: 16.3248
outputs.loss tensor(16.3248, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0024.flac
Waveform stats - mean: 0.0000, std: 0.0591, min: -0.4736, max: 0.5216
Resampled waveform stats - mean: 0.0000, std: 0.0591, min: -0.4736, max: 0.5216
Raw mel spectrogram stats - mean: 1.3038, std: 12.0380, min: 0.0000, max: 1518.7847
Log mel spectrogram stats - mean: -4.6001, std: 3.3752, min: -12.8753, max: 7.3257
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4518, max: 3.5333
Mel spec shape: torch.Size([1, 80, 975])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4512, max: 3.5332
CNN output shape: torch.Size([1, 512, 61])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 31232
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 31232
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 218624
Inf count: 0
audio_emb.shape torch.Size([1, 61, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A MAN'S ESTIMATES OF VALUE BETRAY SOMETHING OF THE STRUCTURE OF HIS SOUL AND WHEREIN IT SEES ITS CONDITIONS OF LIFE ITS INTRINSIC NEEDS
Prediction: 00000000000000000000000000000000000000
Loss: 16.4536
outputs.loss tensor(16.4536, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0007.flac
Waveform stats - mean: -0.0000, std: 0.0669, min: -0.4588, max: 0.5973
Resampled waveform stats - mean: -0.0000, std: 0.0669, min: -0.4588, max: 0.5973
Raw mel spectrogram stats - mean: 1.6715, std: 14.6528, min: 0.0000, max: 735.1730
Log mel spectrogram stats - mean: -5.5676, std: 4.3755, min: -13.8026, max: 6.6001
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8821, max: 2.7809
Mel spec shape: torch.Size([1, 80, 371])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8818, max: 2.7812
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE PEOPLE MUST WAIT OUTSIDE FOR THERE IS NO ROOM FOR THEM IN THE PALACE
Prediction: 00000000000000000
Loss: 16.2427
outputs.loss tensor(16.2427, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275156/8297-275156-0001.flac
Waveform stats - mean: -0.0000, std: 0.0899, min: -0.4417, max: 0.9065
Resampled waveform stats - mean: -0.0000, std: 0.0899, min: -0.4417, max: 0.9065
Raw mel spectrogram stats - mean: 3.0140, std: 17.1538, min: 0.0000, max: 402.4704
Log mel spectrogram stats - mean: -6.1868, std: 5.1805, min: -13.8152, max: 5.9976
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4725, max: 2.3520
Mel spec shape: torch.Size([1, 80, 465])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4727, max: 2.3516
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU HAVE BEEN TO THE HOTEL HE BURST OUT YOU HAVE SEEN CATHERINE
Prediction: 0000000000000000000
Loss: 16.1718
outputs.loss tensor(16.1718, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0017.flac
Waveform stats - mean: -0.0001, std: 0.0584, min: -0.3536, max: 0.4509
Resampled waveform stats - mean: -0.0001, std: 0.0584, min: -0.3536, max: 0.4509
Raw mel spectrogram stats - mean: 1.2787, std: 6.3676, min: 0.0000, max: 233.9500
Log mel spectrogram stats - mean: -5.2422, std: 4.0183, min: -13.6776, max: 5.4551
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0993, max: 2.6621
Mel spec shape: torch.Size([1, 80, 1381])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0996, max: 2.6621
CNN output shape: torch.Size([1, 512, 87])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 44544
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 44544
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 311808
Inf count: 0
audio_emb.shape torch.Size([1, 87, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HERE IS THE SUPREME MYSTERY THAT IS THE ESSENCE OF WORSHIP WITHOUT WHICH THERE CAN BE NO RELIGION AND IN THE PRESENCE OF THIS MYSTERY OUR ATTITUDE CANNOT BE VERY UNLIKE THAT OF THE NATURAL PHILOSOPHER WHO BEHOLDS WITH AWE THE DIVINE IN ALL CREATION
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2357
outputs.loss tensor(16.2357, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0071.flac
Waveform stats - mean: 0.0008, std: 0.0124, min: -0.0967, max: 0.1086
Resampled waveform stats - mean: 0.0008, std: 0.0124, min: -0.0967, max: 0.1086
Raw mel spectrogram stats - mean: 0.0536, std: 0.2746, min: 0.0000, max: 14.1020
Log mel spectrogram stats - mean: -6.6280, std: 2.9531, min: -13.6392, max: 2.6463
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3741, max: 3.1405
Mel spec shape: torch.Size([1, 80, 635])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3750, max: 3.1406
CNN output shape: torch.Size([1, 512, 40])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AS HER QUIET FIGURE APPEARED IN THE DOORWAY SWEETWATER STOLE A GLANCE AT MISTER GRYCE
Prediction: 000000000000000000000000000000
Loss: 16.0899
outputs.loss tensor(16.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/148538/1988-148538-0010.flac
Waveform stats - mean: 0.0000, std: 0.0373, min: -0.5441, max: 0.5114
Resampled waveform stats - mean: 0.0000, std: 0.0373, min: -0.5441, max: 0.5114
Raw mel spectrogram stats - mean: 0.5208, std: 5.5263, min: 0.0000, max: 399.0233
Log mel spectrogram stats - mean: -7.3292, std: 4.3206, min: -13.8133, max: 5.9890
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5008, max: 3.0825
Mel spec shape: torch.Size([1, 80, 869])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5010, max: 3.0820
CNN output shape: torch.Size([1, 512, 55])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IF I APPLAUD THE FREEDOM WHICH ITS INHABITANTS ENJOY HE ANSWERS FREEDOM IS A FINE THING BUT FEW NATIONS ARE WORTHY TO ENJOY IT
Prediction: 000000000000000000000000000000000000000000000
Loss: 15.4199
outputs.loss tensor(15.4199, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0023.flac
Waveform stats - mean: -0.0001, std: 0.0346, min: -0.3754, max: 0.2733
Resampled waveform stats - mean: -0.0001, std: 0.0346, min: -0.3754, max: 0.2733
Raw mel spectrogram stats - mean: 0.4393, std: 5.1328, min: 0.0000, max: 256.2221
Log mel spectrogram stats - mean: -8.6782, std: 4.9341, min: -13.8154, max: 5.5460
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.0412, max: 2.8829
Mel spec shape: torch.Size([1, 80, 266])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.0410, max: 2.8828
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS HAD SOME EFFECT IN CALMING HIM
Prediction: 000000000
Loss: 16.3939
outputs.loss tensor(16.3939, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0015.flac
Waveform stats - mean: 0.0000, std: 0.0801, min: -0.5728, max: 0.4766
Resampled waveform stats - mean: 0.0000, std: 0.0801, min: -0.5728, max: 0.4766
Raw mel spectrogram stats - mean: 2.3967, std: 14.6033, min: 0.0000, max: 408.7937
Log mel spectrogram stats - mean: -5.7276, std: 4.4241, min: -13.7465, max: 6.0132
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8126, max: 2.6538
Mel spec shape: torch.Size([1, 80, 569])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8125, max: 2.6543
CNN output shape: torch.Size([1, 512, 36])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE WAS SILENCE THEN AND STILL WONDERING BRION WAS ONCE MORE ASLEEP
Prediction: 000000000000000000000
Loss: 15.9057
outputs.loss tensor(15.9057, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0017.flac
Waveform stats - mean: 0.0000, std: 0.0479, min: -0.2902, max: 0.2256
Resampled waveform stats - mean: 0.0000, std: 0.0479, min: -0.2902, max: 0.2256
Raw mel spectrogram stats - mean: 0.8607, std: 4.0941, min: 0.0000, max: 83.4999
Log mel spectrogram stats - mean: -7.4424, std: 5.3505, min: -13.8154, max: 4.4248
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.1911, max: 2.2180
Mel spec shape: torch.Size([1, 80, 287])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.1914, max: 2.2188
CNN output shape: torch.Size([1, 512, 18])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE YANKEES MARCHED OVER THE HILL OUT OF SIGHT
Prediction: 0000000000000000
Loss: 15.2844
outputs.loss tensor(15.2844, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0015.flac
Waveform stats - mean: -0.0001, std: 0.0632, min: -0.5365, max: 0.4511
Resampled waveform stats - mean: -0.0001, std: 0.0632, min: -0.5365, max: 0.4511
Raw mel spectrogram stats - mean: 1.4263, std: 8.9870, min: 0.0000, max: 1214.7031
Log mel spectrogram stats - mean: -5.1914, std: 4.2047, min: -13.6959, max: 7.1023
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0226, max: 2.9238
Mel spec shape: torch.Size([1, 80, 1181])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0234, max: 2.9238
CNN output shape: torch.Size([1, 512, 74])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: PUT THE PULP INTO A BASIN WITH TWO OUNCES OF MELTED BUTTER TWO TABLESPOONFULS OF LEMON JUICE HALF A POUND OF CHESTNUTS BOILED AND GRATED AND SEASONING OF SALT AND WHITE PEPPER TO TASTE
Prediction: 00000000000000000000000000000000000000000000000000000000000000
Loss: 15.7979
outputs.loss tensor(15.7979, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147965/1993-147965-0008.flac
Waveform stats - mean: -0.0001, std: 0.0570, min: -0.2828, max: 0.2452
Resampled waveform stats - mean: -0.0001, std: 0.0570, min: -0.2828, max: 0.2452
Raw mel spectrogram stats - mean: 1.2144, std: 10.1856, min: 0.0000, max: 296.1441
Log mel spectrogram stats - mean: -6.2807, std: 4.0183, min: -13.7491, max: 5.6908
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8586, max: 2.9793
Mel spec shape: torch.Size([1, 80, 551])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8584, max: 2.9785
CNN output shape: torch.Size([1, 512, 35])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE MADE THE SIGN OF THE CROSS OVER ME PUT ON HIS CAP AND WENT OFF IN THE DARK
Prediction: 00000000000000000000
Loss: 15.7658
outputs.loss tensor(15.7658, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/152373/2035-152373-0002.flac
Waveform stats - mean: -0.0001, std: 0.0404, min: -0.3035, max: 0.3388
Resampled waveform stats - mean: -0.0001, std: 0.0404, min: -0.3035, max: 0.3388
Raw mel spectrogram stats - mean: 0.6113, std: 6.3077, min: 0.0000, max: 438.9736
Log mel spectrogram stats - mean: -6.0699, std: 3.4753, min: -13.6616, max: 6.0844
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1845, max: 3.4973
Mel spec shape: torch.Size([1, 80, 935])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1836, max: 3.4980
CNN output shape: torch.Size([1, 512, 59])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS PRETTY CLEAR ALSO THAT THE LAST RALLY OF DRUIDISM AGAINST CHRISTIANITY TOOK PLACE BEHIND HIS BANNER ON THE PLAIN OF MOIRA
Prediction: 000000000000000000000000000000000000
Loss: 16.7267
outputs.loss tensor(16.7267, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0007.flac
Waveform stats - mean: -0.0001, std: 0.0757, min: -0.3941, max: 0.5916
Resampled waveform stats - mean: -0.0001, std: 0.0757, min: -0.3941, max: 0.5916
Raw mel spectrogram stats - mean: 2.1466, std: 12.1729, min: 0.0000, max: 802.0408
Log mel spectrogram stats - mean: -5.1583, std: 4.0833, min: -13.7327, max: 6.6872
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0999, max: 2.9009
Mel spec shape: torch.Size([1, 80, 1718])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0996, max: 2.9004
CNN output shape: torch.Size([1, 512, 108])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 55296
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 55296
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 387072
Inf count: 0
audio_emb.shape torch.Size([1, 108, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE REMOTER DEGREES OF KINSHIP WERE FULLY RECOGNIZED AND THAT NOT AS A MATTER OF FORM ONLY FIRST COUSINS WERE KNOWN AS BROTHERS AND SISTERS THE NAME OF COUSIN CONSTITUTED A BINDING CLAIM AND OUR RIGID MORALITY FORBADE MARRIAGE BETWEEN COUSINS IN ANY KNOWN DEGREE OR IN OTHER WORDS WITHIN THE CLAN
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.6814
outputs.loss tensor(15.6814, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0023.flac
Waveform stats - mean: 0.0000, std: 0.0256, min: -0.1653, max: 0.2173
Resampled waveform stats - mean: 0.0000, std: 0.0256, min: -0.1653, max: 0.2173
Raw mel spectrogram stats - mean: 0.2454, std: 1.4341, min: 0.0000, max: 50.1473
Log mel spectrogram stats - mean: -6.5483, std: 3.8915, min: -13.7767, max: 3.9150
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8575, max: 2.6888
Mel spec shape: torch.Size([1, 80, 703])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8574, max: 2.6895
CNN output shape: torch.Size([1, 512, 44])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE FAMOUS TERRORIST HAD NEVER IN HIS LIFE RAISED PERSONALLY AS MUCH AS HIS LITTLE FINGER AGAINST THE SOCIAL EDIFICE
Prediction: 00000000000000000000000000000000
Loss: 16.5420
outputs.loss tensor(16.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/23268/3536-23268-0016.flac
Waveform stats - mean: -0.0001, std: 0.0550, min: -0.7800, max: 0.4424
Resampled waveform stats - mean: -0.0001, std: 0.0550, min: -0.7800, max: 0.4424
Raw mel spectrogram stats - mean: 1.0224, std: 18.0824, min: 0.0000, max: 4494.3794
Log mel spectrogram stats - mean: -5.5104, std: 3.7763, min: -13.7455, max: 8.4106
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1807, max: 3.6864
Mel spec shape: torch.Size([1, 80, 1133])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1816, max: 3.6855
CNN output shape: torch.Size([1, 512, 71])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 36352
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 36352
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 254464
Inf count: 0
audio_emb.shape torch.Size([1, 71, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MISS WOODLEY OBEDIENTLY SAT DOWN AND THOUGH HER THOUGHTS AND HEART WERE IN THE CHAMBER OF HER FRIEND SHE NEVER MARKED BY ONE IMPERTINENT WORD OR BY ONE LINE OF HER FACE THE RESTRAINT SHE SUFFERED
Prediction: 00000000000000000000000000000000000000000000000000000000
Loss: 16.3782
outputs.loss tensor(16.3782, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0033.flac
Waveform stats - mean: 0.0000, std: 0.0666, min: -0.4408, max: 0.4090
Resampled waveform stats - mean: 0.0000, std: 0.0666, min: -0.4408, max: 0.4090
Raw mel spectrogram stats - mean: 1.6601, std: 11.3942, min: 0.0000, max: 582.3739
Log mel spectrogram stats - mean: -5.5453, std: 4.0220, min: -13.8155, max: 6.3671
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0562, max: 2.9618
Mel spec shape: torch.Size([1, 80, 803])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0566, max: 2.9609
CNN output shape: torch.Size([1, 512, 51])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: CONFUSION AND DISMAY TOGETHER MINGLED FORCED SUCH A YES FROM OUT MY MOUTH THAT SIGHT WAS NEEDFUL TO THE UNDERSTANDING OF IT
Prediction: 000000000000000000000000000000000000
Loss: 15.9072
outputs.loss tensor(15.9072, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0018.flac
Waveform stats - mean: -0.0000, std: 0.0584, min: -0.6794, max: 0.5349
Resampled waveform stats - mean: -0.0000, std: 0.0584, min: -0.6794, max: 0.5349
Raw mel spectrogram stats - mean: 1.2745, std: 8.3065, min: 0.0000, max: 540.3329
Log mel spectrogram stats - mean: -4.5620, std: 3.4324, min: -12.6053, max: 6.2922
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3434, max: 3.1623
Mel spec shape: torch.Size([1, 80, 1457])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3438, max: 3.1621
CNN output shape: torch.Size([1, 512, 92])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 47104
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 47104
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 329728
Inf count: 0
audio_emb.shape torch.Size([1, 92, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN OUR VERY DEMOCRATIC OR RATHER VERY PLEBEIAN AGE EDUCATION AND CULTURE MUST BE ESSENTIALLY THE ART OF DECEIVING DECEIVING WITH REGARD TO ORIGIN WITH REGARD TO THE INHERITED PLEBEIANISM IN BODY AND SOUL
Prediction: 0000000000000000000000000000000000000000000000000000000000
Loss: 16.3800
outputs.loss tensor(16.3800, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0016.flac
Waveform stats - mean: -0.0000, std: 0.0807, min: -0.4952, max: 0.4627
Resampled waveform stats - mean: -0.0000, std: 0.0807, min: -0.4952, max: 0.4627
Raw mel spectrogram stats - mean: 2.4337, std: 18.8173, min: 0.0000, max: 583.5837
Log mel spectrogram stats - mean: -5.3992, std: 4.4598, min: -13.7851, max: 6.3692
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8803, max: 2.6388
Mel spec shape: torch.Size([1, 80, 338])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8799, max: 2.6387
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE IS NOTHING MAJESTIC ABOUT ME AS YOU KNOW VERY WELL
Prediction: 000000000000000
Loss: 16.1289
outputs.loss tensor(16.1289, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0029.flac
Waveform stats - mean: -0.0000, std: 0.0655, min: -0.4871, max: 0.4783
Resampled waveform stats - mean: -0.0000, std: 0.0655, min: -0.4871, max: 0.4783
Raw mel spectrogram stats - mean: 1.6087, std: 15.7090, min: 0.0000, max: 1028.9625
Log mel spectrogram stats - mean: -6.0956, std: 4.4551, min: -13.8155, max: 6.9363
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7328, max: 2.9252
Mel spec shape: torch.Size([1, 80, 1180])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7324, max: 2.9258
CNN output shape: torch.Size([1, 512, 74])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37888
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37888
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 265216
Inf count: 0
audio_emb.shape torch.Size([1, 74, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A WEEK AFTER THE BATTLE MY BROTHER RODE BY THERE ON A CAVALRY EXPEDITION AND MADE THE HORRIBLE DISCOVERY THAT HOGS WERE EATING UP THE BODIES OF OUR DEAD HEROES THAT TOO WAS WAR
Prediction: 0000000000000000000000000000000000000000000000000000
Loss: 16.2963
outputs.loss tensor(16.2963, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0030.flac
Waveform stats - mean: -0.0000, std: 0.0466, min: -0.5010, max: 0.4959
Resampled waveform stats - mean: -0.0000, std: 0.0466, min: -0.5010, max: 0.4959
Raw mel spectrogram stats - mean: 0.8118, std: 11.3694, min: 0.0000, max: 602.8261
Log mel spectrogram stats - mean: -5.9399, std: 3.7086, min: -13.8076, max: 6.4016
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1215, max: 3.3278
Mel spec shape: torch.Size([1, 80, 344])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1211, max: 3.3281
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT'S THAT FOR DEMANDED NED WONDERINGLY
Prediction: 0000000000000
Loss: 16.2577
outputs.loss tensor(16.2577, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0010.flac
Waveform stats - mean: -0.0001, std: 0.0595, min: -0.5567, max: 0.4098
Resampled waveform stats - mean: -0.0001, std: 0.0595, min: -0.5567, max: 0.4098
Raw mel spectrogram stats - mean: 1.3228, std: 7.9599, min: 0.0000, max: 251.3782
Log mel spectrogram stats - mean: -6.1554, std: 4.6549, min: -13.7176, max: 5.5270
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6245, max: 2.5097
Mel spec shape: torch.Size([1, 80, 264])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6250, max: 2.5098
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: FROM A COUSIN OF OURS WHO'S IN THAT LINE
Prediction: 0000000000000
Loss: 15.0463
outputs.loss tensor(15.0463, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0002.flac
Waveform stats - mean: 0.0000, std: 0.0703, min: -0.4262, max: 0.6908
Resampled waveform stats - mean: 0.0000, std: 0.0703, min: -0.4262, max: 0.6908
Raw mel spectrogram stats - mean: 1.8517, std: 7.9998, min: 0.0000, max: 212.4122
Log mel spectrogram stats - mean: -4.9491, std: 4.0782, min: -13.6826, max: 5.3585
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1415, max: 2.5275
Mel spec shape: torch.Size([1, 80, 722])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1406, max: 2.5273
CNN output shape: torch.Size([1, 512, 46])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23552
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23552
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 164864
Inf count: 0
audio_emb.shape torch.Size([1, 46, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'M NOT TO GO IN THERE SAYS THE EX BANK CLERK DRAWING BACK IN DISMAY FROM THE CLOUD OF FOUL FACES WHICH LOWERED UPON HIM
Prediction: 0000000000000000000000000000000000000
Loss: 15.5442
outputs.loss tensor(15.5442, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93302/6345-93302-0028.flac
Waveform stats - mean: -0.0000, std: 0.0467, min: -0.2639, max: 0.4160
Resampled waveform stats - mean: -0.0000, std: 0.0467, min: -0.2639, max: 0.4160
Raw mel spectrogram stats - mean: 0.8129, std: 9.9920, min: 0.0000, max: 388.3541
Log mel spectrogram stats - mean: -8.5893, std: 4.3540, min: -13.8146, max: 5.9619
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.2001, max: 3.3420
Mel spec shape: torch.Size([1, 80, 195])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.2002, max: 3.3418
CNN output shape: torch.Size([1, 512, 13])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE CHECKED THE SILLY IMPULSE
Prediction: 00000000
Loss: 17.0252
outputs.loss tensor(17.0252, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0039.flac
Waveform stats - mean: 0.0001, std: 0.0729, min: -0.5210, max: 0.6526
Resampled waveform stats - mean: 0.0001, std: 0.0729, min: -0.5210, max: 0.6526
Raw mel spectrogram stats - mean: 1.8506, std: 15.1191, min: 0.0000, max: 1916.5338
Log mel spectrogram stats - mean: -6.0274, std: 4.9592, min: -13.8097, max: 7.5583
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5693, max: 2.7395
Mel spec shape: torch.Size([1, 80, 915])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5693, max: 2.7402
CNN output shape: torch.Size([1, 512, 58])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS WAS NOT THE FASHION OF A YEAR AGO WHEN THEY EXCHANGED A FRIENDLY WORD OR TWO BUT HARRY KNEW ITS CAUSE NOW NOBODY COULD TRUST ANYBODY ELSE
Prediction: 00000000000000000000000000000000000000000
Loss: 15.9969
outputs.loss tensor(15.9969, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0013.flac
Waveform stats - mean: 0.0000, std: 0.0967, min: -1.0000, max: 0.9969
Resampled waveform stats - mean: 0.0000, std: 0.0967, min: -1.0000, max: 0.9969
Raw mel spectrogram stats - mean: 2.8485, std: 43.0880, min: 0.0000, max: 6134.0249
Log mel spectrogram stats - mean: -5.6727, std: 4.7784, min: -13.7915, max: 8.7216
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6990, max: 3.0124
Mel spec shape: torch.Size([1, 80, 672])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6992, max: 3.0117
CNN output shape: torch.Size([1, 512, 42])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AN EXTRAORDINARY WAVE OF EMOTION SWEPT OVER THE SOUTH CARRYING EVERYBODY WITH IT
Prediction: 0000000000000000000000
Loss: 16.4390
outputs.loss tensor(16.4390, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0012.flac
Waveform stats - mean: -0.0000, std: 0.0666, min: -0.4920, max: 0.4506
Resampled waveform stats - mean: -0.0000, std: 0.0666, min: -0.4920, max: 0.4506
Raw mel spectrogram stats - mean: 1.6621, std: 15.0035, min: 0.0000, max: 645.8819
Log mel spectrogram stats - mean: -6.4275, std: 4.4239, min: -13.8149, max: 6.4706
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6699, max: 2.9155
Mel spec shape: torch.Size([1, 80, 516])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6699, max: 2.9160
CNN output shape: torch.Size([1, 512, 33])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE SURVIVED THE WAR ONLY TO BE MURDERED LATER ON A PLANTATION IN MISSISSIPPI
Prediction: 0000000000000000000000000
Loss: 15.9006
outputs.loss tensor(15.9006, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0065.flac
Waveform stats - mean: 0.0009, std: 0.0171, min: -0.0948, max: 0.1218
Resampled waveform stats - mean: 0.0009, std: 0.0171, min: -0.0948, max: 0.1218
Raw mel spectrogram stats - mean: 0.1075, std: 0.5569, min: 0.0000, max: 20.2720
Log mel spectrogram stats - mean: -6.3180, std: 3.2473, min: -13.2913, max: 3.0092
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1474, max: 2.8724
Mel spec shape: torch.Size([1, 80, 329])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1465, max: 2.8730
CNN output shape: torch.Size([1, 512, 21])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NOT ALTOGETHER BY ME
Prediction: 0000000
Loss: 15.6922
outputs.loss tensor(15.6922, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/84280/174-84280-0012.flac
Waveform stats - mean: -0.0001, std: 0.0692, min: -0.6937, max: 0.9229
Resampled waveform stats - mean: -0.0001, std: 0.0692, min: -0.6937, max: 0.9229
Raw mel spectrogram stats - mean: 1.7943, std: 14.3280, min: 0.0000, max: 960.2328
Log mel spectrogram stats - mean: -4.5138, std: 3.7359, min: -13.7784, max: 6.8672
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4799, max: 3.0464
Mel spec shape: torch.Size([1, 80, 1444])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4805, max: 3.0469
CNN output shape: torch.Size([1, 512, 91])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 46592
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 46592
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 326144
Inf count: 0
audio_emb.shape torch.Size([1, 91, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND IT IS UPON THIS EFFECT OF SWEET AND BEAUTIFUL POSSIBILITIES CAUGHT IN THE NET OF ANIMAL JEALOUSIES AND THOUGHTLESS MOTIVES AND ANCIENT RIGID INSTITUTIONS THAT I WOULD END THIS WRITING
Prediction: 000000000000000000000000000000000000000000000000000000000
Loss: 16.4427
outputs.loss tensor(16.4427, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0012.flac
Waveform stats - mean: -0.0000, std: 0.0580, min: -0.3394, max: 0.3745
Resampled waveform stats - mean: -0.0000, std: 0.0580, min: -0.3394, max: 0.3745
Raw mel spectrogram stats - mean: 1.2598, std: 8.6692, min: 0.0000, max: 491.3949
Log mel spectrogram stats - mean: -5.5292, std: 4.0670, min: -13.8155, max: 6.1972
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0375, max: 2.8833
Mel spec shape: torch.Size([1, 80, 925])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0371, max: 2.8828
CNN output shape: torch.Size([1, 512, 58])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN BACK I TURNED MY FACE TO THOSE HIGH THINGS WHICH MOVED THEMSELVES TOWARDS US SO SEDATELY THEY HAD BEEN DISTANCED BY NEW WEDDED BRIDES
Prediction: 00000000000000000000000000000000000000000
Loss: 16.6332
outputs.loss tensor(16.6332, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147149/1993-147149-0008.flac
Waveform stats - mean: -0.0000, std: 0.0064, min: -0.0333, max: 0.0274
Resampled waveform stats - mean: -0.0000, std: 0.0064, min: -0.0333, max: 0.0274
Raw mel spectrogram stats - mean: 0.0151, std: 0.1233, min: 0.0000, max: 3.4015
Log mel spectrogram stats - mean: -9.4405, std: 3.0538, min: -13.8073, max: 1.2242
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4299, max: 3.4923
Mel spec shape: torch.Size([1, 80, 340])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4297, max: 3.4922
CNN output shape: torch.Size([1, 512, 22])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IS THERE ANY CHANCE FOR THE OTHER ONE THINK YOU
Prediction: 00000000000
Loss: 16.3291
outputs.loss tensor(16.3291, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0022.flac
Waveform stats - mean: -0.0000, std: 0.0303, min: -0.1873, max: 0.3600
Resampled waveform stats - mean: -0.0000, std: 0.0303, min: -0.1873, max: 0.3600
Raw mel spectrogram stats - mean: 0.3420, std: 3.1028, min: 0.0000, max: 110.2912
Log mel spectrogram stats - mean: -9.6204, std: 4.4280, min: -13.8151, max: 4.7031
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -0.9473, max: 3.2347
Mel spec shape: torch.Size([1, 80, 226])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -0.9473, max: 3.2344
CNN output shape: torch.Size([1, 512, 15])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU CAN'T DO IT
Prediction: 00000
Loss: 15.6809
outputs.loss tensor(15.6809, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0005.flac
Waveform stats - mean: -0.0000, std: 0.0497, min: -0.4393, max: 0.4384
Resampled waveform stats - mean: -0.0000, std: 0.0497, min: -0.4393, max: 0.4384
Raw mel spectrogram stats - mean: 0.9268, std: 10.8472, min: 0.0000, max: 889.0689
Log mel spectrogram stats - mean: -5.3228, std: 3.6866, min: -13.8096, max: 6.7902
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3021, max: 3.2857
Mel spec shape: torch.Size([1, 80, 582])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3027, max: 3.2852
CNN output shape: torch.Size([1, 512, 37])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WALTER HAD GONE OUT WITH THE SECOND GUARD AND THE OTHERS HAD GATHERED AROUND THE CAMP FIRE FOR THEIR NIGHTLY STORY TELLING
Prediction: 00000000000000000000000000000000000
Loss: 15.7622
outputs.loss tensor(15.7622, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0009.flac
Waveform stats - mean: -0.0001, std: 0.0433, min: -0.4822, max: 0.4526
Resampled waveform stats - mean: -0.0001, std: 0.0433, min: -0.4822, max: 0.4526
Raw mel spectrogram stats - mean: 0.7019, std: 5.1776, min: 0.0000, max: 252.9731
Log mel spectrogram stats - mean: -5.7149, std: 3.5947, min: -13.1087, max: 5.5333
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0568, max: 3.1291
Mel spec shape: torch.Size([1, 80, 385])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0566, max: 3.1289
CNN output shape: torch.Size([1, 512, 25])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE VALKYRIE KEPT OFF THE COAST STEERING TO THE WESTWARD
Prediction: 00000000000000000
Loss: 16.1672
outputs.loss tensor(16.1672, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275156/8297-275156-0000.flac
Waveform stats - mean: -0.0000, std: 0.0519, min: -0.2552, max: 0.5094
Resampled waveform stats - mean: -0.0000, std: 0.0519, min: -0.2552, max: 0.5094
Raw mel spectrogram stats - mean: 0.9986, std: 6.4869, min: 0.0000, max: 201.4281
Log mel spectrogram stats - mean: -7.6780, std: 4.7466, min: -13.8126, max: 5.3054
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.2924, max: 2.7353
Mel spec shape: torch.Size([1, 80, 359])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.2920, max: 2.7344
CNN output shape: torch.Size([1, 512, 23])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT ARE YOU DOING HERE HE ASKED
Prediction: 0000000000
Loss: 14.9041
outputs.loss tensor(14.9041, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0019.flac
Waveform stats - mean: 0.0000, std: 0.0405, min: -0.2791, max: 0.3686
Resampled waveform stats - mean: 0.0000, std: 0.0405, min: -0.2791, max: 0.3686
Raw mel spectrogram stats - mean: 0.6150, std: 6.1629, min: 0.0000, max: 349.6954
Log mel spectrogram stats - mean: -7.1394, std: 4.0129, min: -13.5953, max: 5.8571
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6088, max: 3.2387
Mel spec shape: torch.Size([1, 80, 272])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6084, max: 3.2383
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I LOOK AT MY WATCH IT'S A QUARTER TO ELEVEN
Prediction: 000000000000000
Loss: 16.0190
outputs.loss tensor(16.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0036.flac
Waveform stats - mean: -0.0000, std: 0.0741, min: -0.6154, max: 0.6440
Resampled waveform stats - mean: -0.0000, std: 0.0741, min: -0.6154, max: 0.6440
Raw mel spectrogram stats - mean: 2.0591, std: 16.7477, min: 0.0000, max: 1257.7764
Log mel spectrogram stats - mean: -6.1329, std: 4.7841, min: -13.8155, max: 7.1371
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6059, max: 2.7738
Mel spec shape: torch.Size([1, 80, 1223])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6055, max: 2.7734
CNN output shape: torch.Size([1, 512, 77])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 39424
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 39424
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 275968
Inf count: 0
audio_emb.shape torch.Size([1, 77, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: UNDER CERTAIN CONDITIONS YOU MAY HEAR THE ROAR OF THE WATER RUSHING FROM THE ROCK AT A DISTANCE OF HALF A MILE OR EVEN MORE OR YOU MAY NOT HEAR IT UNTIL WITHIN A FEW RODS
Prediction: 00000000000000000000000000000000000000000000000000
Loss: 15.5812
outputs.loss tensor(15.5812, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0004.flac
Waveform stats - mean: 0.0000, std: 0.0685, min: -0.3698, max: 0.4172
Resampled waveform stats - mean: 0.0000, std: 0.0685, min: -0.3698, max: 0.4172
Raw mel spectrogram stats - mean: 1.7543, std: 9.9398, min: 0.0000, max: 286.3204
Log mel spectrogram stats - mean: -6.1938, std: 4.7703, min: -13.8128, max: 5.6571
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5972, max: 2.4843
Mel spec shape: torch.Size([1, 80, 370])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5967, max: 2.4844
CNN output shape: torch.Size([1, 512, 24])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AS GLADDEN RODE BY US A COURIER RODE UP AND TOLD HIM SOMETHING
Prediction: 000000000000000000000
Loss: 16.5612
outputs.loss tensor(16.5612, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0016.flac
Waveform stats - mean: 0.0000, std: 0.0496, min: -0.3889, max: 0.4070
Resampled waveform stats - mean: 0.0000, std: 0.0496, min: -0.3889, max: 0.4070
Raw mel spectrogram stats - mean: 0.9176, std: 6.1643, min: 0.0000, max: 217.4745
Log mel spectrogram stats - mean: -4.9998, std: 3.7737, min: -13.4090, max: 5.3821
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2284, max: 2.7511
Mel spec shape: torch.Size([1, 80, 259])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2285, max: 2.7520
CNN output shape: torch.Size([1, 512, 17])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: URSUS AND HOMO TOOK CHARGE OF EACH OTHER
Prediction: 00000000000000
Loss: 14.9889
outputs.loss tensor(14.9889, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0023.flac
Waveform stats - mean: -0.0000, std: 0.0650, min: -0.4803, max: 0.3954
Resampled waveform stats - mean: -0.0000, std: 0.0650, min: -0.4803, max: 0.3954
Raw mel spectrogram stats - mean: 1.5804, std: 14.3380, min: 0.0000, max: 609.7508
Log mel spectrogram stats - mean: -6.9101, std: 4.5129, min: -13.8154, max: 6.4131
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5301, max: 2.9522
Mel spec shape: torch.Size([1, 80, 479])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5303, max: 2.9531
CNN output shape: torch.Size([1, 512, 30])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY LAY IN HEAPS OF DOZENS EVEN CLOSE UP TO THE WORKS
Prediction: 000000000000000000
Loss: 15.2068
outputs.loss tensor(15.2068, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/24833/1988-24833-0023.flac
Waveform stats - mean: 0.0000, std: 0.0512, min: -0.5890, max: 0.6801
Resampled waveform stats - mean: 0.0000, std: 0.0512, min: -0.5890, max: 0.6801
Raw mel spectrogram stats - mean: 0.9823, std: 18.3533, min: 0.0000, max: 1588.2155
Log mel spectrogram stats - mean: -7.2530, std: 3.9189, min: -13.8049, max: 7.3704
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6719, max: 3.7315
Mel spec shape: torch.Size([1, 80, 433])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6719, max: 3.7324
CNN output shape: torch.Size([1, 512, 28])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TOM SAYS THANKS AND LOOKS AT HILDA AND SHE BLUSHES REALLY
Prediction: 0000000000000000000
Loss: 15.4920
outputs.loss tensor(15.4920, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0010.flac
Waveform stats - mean: -0.0001, std: 0.0769, min: -0.4505, max: 0.4471
Resampled waveform stats - mean: -0.0001, std: 0.0769, min: -0.4505, max: 0.4471
Raw mel spectrogram stats - mean: 2.2140, std: 12.3484, min: 0.0000, max: 360.2791
Log mel spectrogram stats - mean: -4.5244, std: 3.4704, min: -13.5461, max: 5.8869
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5996, max: 3.0000
Mel spec shape: torch.Size([1, 80, 900])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5996, max: 3.0000
CNN output shape: torch.Size([1, 512, 57])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN FACT THERE IS NOTHING HE CAN DO IN THESE DOMINIONS AS WELL AS OUR NOMES WHOSE NUMBERS ARE SO GREAT THAT IT WORRIES US TO KEEP THEM ALL BUSY
Prediction: 00000000000000000000000000000000000000
Loss: 16.2127
outputs.loss tensor(16.2127, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0011.flac
Waveform stats - mean: 0.0000, std: 0.0672, min: -0.4762, max: 0.4875
Resampled waveform stats - mean: 0.0000, std: 0.0672, min: -0.4762, max: 0.4875
Raw mel spectrogram stats - mean: 1.6900, std: 10.5405, min: 0.0000, max: 370.9644
Log mel spectrogram stats - mean: -5.3064, std: 4.0971, min: -13.6875, max: 5.9161
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0456, max: 2.7391
Mel spec shape: torch.Size([1, 80, 502])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0449, max: 2.7383
CNN output shape: torch.Size([1, 512, 32])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE OTHER VOICE SNAPPED WITH A HARSH URGENCY CLEARLY USED TO COMMAND
Prediction: 00000000000000000
Loss: 17.1154
outputs.loss tensor(17.1154, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0015.flac
Waveform stats - mean: 0.0000, std: 0.0795, min: -0.7724, max: 0.4973
Resampled waveform stats - mean: 0.0000, std: 0.0795, min: -0.7724, max: 0.4973
Raw mel spectrogram stats - mean: 1.9982, std: 20.8969, min: 0.0000, max: 1938.3251
Log mel spectrogram stats - mean: -6.1270, std: 4.8678, min: -13.7876, max: 7.5696
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5737, max: 2.8137
Mel spec shape: torch.Size([1, 80, 224])
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5742, max: 2.8145
CNN output shape: torch.Size([1, 512, 14])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: COLONEL KENTON WRITES WISELY
Prediction: 00000000000
Loss: 16.4326
outputs.loss tensor(16.4326, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0037.flac
Waveform stats - mean: -0.0000, std: 0.0843, min: -0.4235, max: 0.3957
Resampled waveform stats - mean: -0.0000, std: 0.0843, min: -0.4235, max: 0.3957
Raw mel spectrogram stats - mean: 2.6604, std: 19.6888, min: 0.0000, max: 939.7090
Log mel spectrogram stats - mean: -5.3249, std: 4.4316, min: -13.7690, max: 6.8456
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9054, max: 2.7463
Mel spec shape: torch.Size([1, 80, 574])
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9053, max: 2.7461
CNN output shape: torch.Size([1, 512, 36])
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY ALL ASKED ME HOW LONG I WOULD REQUIRE TO TEACH THEM THE RULES OF MY SUBLIME CALCULUS
Prediction: 0000000000000000000000000000
Loss: 15.5417
outputs.loss tensor(15.5417, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275155/8297-275155-0013.flac
Waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Resampled waveform stats - mean: 0.0000, std: 0.0564, min: -0.2751, max: 0.5938
Raw mel spectrogram stats - mean: 1.2006, std: 7.3066, min: 0.0000, max: 190.1952
