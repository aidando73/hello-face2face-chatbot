Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0015.flac
Waveform stats - mean: -0.0000, std: 0.0435, min: -0.3718, max: 0.3438
Resampled waveform stats - mean: -0.0000, std: 0.0435, min: -0.3718, max: 0.3438
Raw mel spectrogram stats - mean: 0.7078, std: 4.3257, min: 0.0000, max: 191.3521
Log mel spectrogram stats - mean: -5.4064, std: 3.6166, min: -13.5681, max: 5.2541
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2567, max: 2.9476
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2559, max: 2.9473
CNN output stats - mean: 0.2842, std: 0.5840, min: -0.1699, max: 3.5293
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.7422, max: 4.4688
Final output stats - mean: 0.0003, std: 0.1888, min: -0.8623, max: 0.7075
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0003, std: 0.1887

Sample prediction:
Target: SHE WANTED TO MAKE SOME REFERENCE TO THEIR RELATIONS UPON THE TRAIN BUT WAS TOO TIMID
Prediction: 伊托托1奥奥士托托尼托托托托托伊托托奥亚托托
Loss: 13.6202
outputs.loss tensor(13.6202, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0001.flac
Waveform stats - mean: -0.0000, std: 0.0753, min: -0.4690, max: 0.6448
Resampled waveform stats - mean: -0.0000, std: 0.0753, min: -0.4690, max: 0.6448
Raw mel spectrogram stats - mean: 2.1271, std: 10.3380, min: 0.0000, max: 449.4019
Log mel spectrogram stats - mean: -5.6251, std: 4.6853, min: -13.8127, max: 6.1079
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7475, max: 2.5042
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7471, max: 2.5039
CNN output stats - mean: 0.2825, std: 0.5889, min: -0.1699, max: 3.4922
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.9883, max: 4.6523
Final output stats - mean: 0.0009, std: 0.1875, min: -0.8042, max: 0.6968
audio_emb.shape torch.Size([1, 65, 3584])
Audio embedding stats - mean: 0.0008, std: 0.1875

Sample prediction:
Target: THE SAILING MASTER ANNOUNCED THAT HE HAD ORDERS TO TAKE THE VESSEL BACK TO HER PORT WITH NO OTHER EXPLANATION THAN THAT THE CRUISE WAS OVER
Prediction: 尼尔伊利利 利 奥奥托尔奥奥奥尔尔  尔奥 尔子尔利尔奥 尔托奥尔 尔尔子托
Loss: 13.5335
outputs.loss tensor(13.5335, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0036.flac
Waveform stats - mean: -0.0001, std: 0.0682, min: -0.5068, max: 0.3561
Resampled waveform stats - mean: -0.0001, std: 0.0682, min: -0.5068, max: 0.3561
Raw mel spectrogram stats - mean: 1.7299, std: 9.7408, min: 0.0000, max: 251.8305
Log mel spectrogram stats - mean: -5.2355, std: 3.9630, min: -13.5679, max: 5.5288
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1026, max: 2.7162
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1035, max: 2.7168
CNN output stats - mean: 0.2844, std: 0.5894, min: -0.1699, max: 3.4141
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.9805, max: 4.6914
Final output stats - mean: -0.0001, std: 0.1831, min: -0.8018, max: 0.7300
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: -0.0002, std: 0.1827

Sample prediction:
Target: PLAIN BUNS SEVENTEEN TWENTY NINE
Prediction: 00  托  索利托  
Loss: 13.8828
outputs.loss tensor(13.8828, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0002.flac
Waveform stats - mean: -0.0000, std: 0.0598, min: -0.4253, max: 0.4026
Resampled waveform stats - mean: -0.0000, std: 0.0598, min: -0.4253, max: 0.4026
Raw mel spectrogram stats - mean: 1.3351, std: 9.2442, min: 0.0000, max: 325.4347
Log mel spectrogram stats - mean: -7.2182, std: 5.1956, min: -13.8146, max: 5.7852
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.2696, max: 2.5028
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.2695, max: 2.5020
CNN output stats - mean: 0.2861, std: 0.5796, min: -0.1699, max: 3.1562
Transformer output stats - mean: -0.0000, std: 1.0000, min: -4.3750, max: 4.4883
Final output stats - mean: 0.0016, std: 0.1801, min: -0.7778, max: 0.6577
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0016, std: 0.1803

Sample prediction:
Target: OUR ARMY STOPPED AT MURFREESBORO
Prediction: 尔奥奥奥奥尔奥奥奥亚亚奥奥
Loss: 13.1966
outputs.loss tensor(13.1966, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0003.flac
Waveform stats - mean: -0.0000, std: 0.0747, min: -0.4685, max: 0.4020
Resampled waveform stats - mean: -0.0000, std: 0.0747, min: -0.4685, max: 0.4020
Raw mel spectrogram stats - mean: 2.0897, std: 14.1161, min: 0.0000, max: 609.8784
Log mel spectrogram stats - mean: -5.5661, std: 4.0017, min: -13.7843, max: 6.4133
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0537, max: 2.9936
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0527, max: 2.9941
CNN output stats - mean: 0.2825, std: 0.5874, min: -0.1699, max: 3.7754
Transformer output stats - mean: 0.0000, std: 1.0000, min: -4.0234, max: 4.6992
Final output stats - mean: 0.0005, std: 0.1848, min: -0.7959, max: 0.7285
audio_emb.shape torch.Size([1, 73, 3584])
Audio embedding stats - mean: 0.0005, std: 0.1849

Sample prediction:
Target: MY YOUNG PLANTS REQUIRE HEAT OR THEY WOULD NOT LIVE AND THE POTS WE ARE KEPT IN PROTECT US FROM THOSE CRUEL WIRE WORMS WHO DELIGHT TO DESTROY OUR ROOTS
Prediction: 尔 \
 \奥奥奥尔意 \ \亚 \奥奥 \拉奥
斯尔奥奥 \斯尔奥
尔斯尔 \

斯奥尔亚尔
奥意亚

尔
Loss: 13.9443
outputs.loss tensor(13.9443, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0017.flac
Waveform stats - mean: -0.0000, std: 0.0521, min: -0.6740, max: 0.5371
Resampled waveform stats - mean: -0.0000, std: 0.0521, min: -0.6740, max: 0.5371
Raw mel spectrogram stats - mean: 1.0142, std: 7.5283, min: 0.0000, max: 267.8093
Log mel spectrogram stats - mean: -5.5039, std: 3.7302, min: -13.4318, max: 5.5903
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1253, max: 2.9741
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1250, max: 2.9746
CNN output stats - mean: 0.2830, std: 0.5928, min: -0.1699, max: 3.5176
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.6055, max: 5.0391
Final output stats - mean: 0.0002, std: 0.1866, min: -0.7783, max: 0.7158
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0002, std: 0.1868

Sample prediction:
Target: A SHOP GIRL WAS THE DESTINY PREFIGURED FOR THE NEWCOMER
Prediction: 尔斯尔斯斯托斯奥斯尔尔斯斯利亚斯
Loss: 15.3676
outputs.loss tensor(15.3676, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0009.flac
Waveform stats - mean: -0.0001, std: 0.0313, min: -0.2184, max: 0.2630
Resampled waveform stats - mean: -0.0001, std: 0.0313, min: -0.2184, max: 0.2630
Raw mel spectrogram stats - mean: 0.3661, std: 3.3332, min: 0.0000, max: 238.0160
Log mel spectrogram stats - mean: -6.2483, std: 3.2440, min: -13.7245, max: 5.4723
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3046, max: 3.6130
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3047, max: 3.6133
CNN output stats - mean: 0.2849, std: 0.5869, min: -0.1699, max: 3.7480
Transformer output stats - mean: 0.0000, std: 1.0000, min: -3.3535, max: 4.7656
Final output stats - mean: -0.0004, std: 0.1860, min: -0.7666, max: 0.7349
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: -0.0003, std: 0.1862

Sample prediction:
Target: THE SICK MAN RAGED AND SHOOK HIS FIST
Prediction: 奥奥伊奥 \斯
奥亚
尔斯
Loss: 14.0227
outputs.loss tensor(14.0227, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0019.flac
Waveform stats - mean: -0.0001, std: 0.0479, min: -0.6028, max: 0.3453
Resampled waveform stats - mean: -0.0001, std: 0.0479, min: -0.6028, max: 0.3453
Raw mel spectrogram stats - mean: 0.8577, std: 6.9576, min: 0.0000, max: 500.4137
Log mel spectrogram stats - mean: -6.7175, std: 4.8897, min: -13.8155, max: 6.2154
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4516, max: 2.6449
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4512, max: 2.6445
CNN output stats - mean: 0.2822, std: 0.5845, min: -0.1699, max: 3.8047
Transformer output stats - mean: -0.0000, std: 1.0000, min: -3.8516, max: 4.7344
Final output stats - mean: -0.0002, std: 0.1824, min: -0.7529, max: 0.7471
audio_emb.shape torch.Size([1, 47, 3584])
Audio embedding stats - mean: -0.0001, std: 0.1823

Sample prediction:
Target: BUT BY AND BY THEY CAME TO MY WATCH WHICH I HAD HIDDEN AWAY IN THE INMOST POCKET THAT I HAD AND HAD FORGOTTEN WHEN THEY BEGAN THEIR SEARCH
Prediction: 0奥尼0托奥伊0奥托0托拉 0托托0托 托奥托奥  \,奥奥奥亚托托托托托亚0托亚 \奥
Loss: 13.3483
outputs.loss tensor(13.3483, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=0.0043, std=0.0641
cnn_layers.0.bias: mean=0.0000, std=0.0000
cnn_layers.1.weight: mean=0.0000, std=0.0296
cnn_layers.1.bias: mean=0.0001, std=0.0222
cnn_layers.3.weight: mean=0.0003, std=0.0463
cnn_layers.3.bias: mean=-0.0000, std=0.0000
cnn_layers.4.weight: mean=0.0000, std=0.0231
cnn_layers.4.bias: mean=-0.0000, std=0.0178
cnn_layers.6.weight: mean=0.0004, std=0.0381
cnn_layers.6.bias: mean=-0.0000, std=0.0000
cnn_layers.7.weight: mean=-0.0002, std=0.0216
cnn_layers.7.bias: mean=-0.0001, std=0.0158
cnn_layers.9.weight: mean=0.0001, std=0.0339
cnn_layers.9.bias: mean=0.0000, std=0.0001
cnn_layers.10.weight: mean=0.0004, std=0.2463
cnn_layers.10.bias: mean=0.0003, std=0.2952
transformer.layers.0.self_attn.in_proj_weight: mean=-0.0005, std=0.0425
transformer.layers.0.self_attn.in_proj_bias: mean=-0.0017, std=0.1493
transformer.layers.0.self_attn.out_proj.weight: mean=-0.0000, std=0.0904
transformer.layers.0.self_attn.out_proj.bias: mean=0.0001, std=0.4373
transformer.layers.0.linear1.weight: mean=-0.0000, std=0.0080
transformer.layers.0.linear1.bias: mean=-0.0006, std=0.0339
transformer.layers.0.linear2.weight: mean=0.0000, std=0.0312
transformer.layers.0.linear2.bias: mean=0.0004, std=0.2379
transformer.layers.0.norm1.weight: mean=-0.0005, std=0.0540
transformer.layers.0.norm1.bias: mean=-0.0010, std=0.2539
transformer.layers.0.norm2.weight: mean=0.0001, std=0.0531
transformer.layers.0.norm2.bias: mean=0.0004, std=0.2433
transformer.layers.1.self_attn.in_proj_weight: mean=0.0000, std=0.0131
transformer.layers.1.self_attn.in_proj_bias: mean=-0.0005, std=0.0617
transformer.layers.1.self_attn.out_proj.weight: mean=0.0000, std=0.0269
transformer.layers.1.self_attn.out_proj.bias: mean=-0.0002, std=0.1814
transformer.layers.1.linear1.weight: mean=0.0000, std=0.0062
transformer.layers.1.linear1.bias: mean=-0.0003, std=0.0245
transformer.layers.1.linear2.weight: mean=0.0000, std=0.0230
transformer.layers.1.linear2.bias: mean=0.0004, std=0.1700
transformer.layers.1.norm1.weight: mean=-0.0003, std=0.0414
transformer.layers.1.norm1.bias: mean=-0.0008, std=0.1818
transformer.layers.1.norm2.weight: mean=0.0001, std=0.0435
transformer.layers.1.norm2.bias: mean=-0.0000, std=0.1798
transformer.layers.2.self_attn.in_proj_weight: mean=-0.0000, std=0.0111
transformer.layers.2.self_attn.in_proj_bias: mean=-0.0001, std=0.0458
transformer.layers.2.self_attn.out_proj.weight: mean=0.0000, std=0.0233
transformer.layers.2.self_attn.out_proj.bias: mean=-0.0001, std=0.1346
transformer.layers.2.linear1.weight: mean=-0.0000, std=0.0056
transformer.layers.2.linear1.bias: mean=-0.0002, std=0.0184
transformer.layers.2.linear2.weight: mean=0.0000, std=0.0184
transformer.layers.2.linear2.bias: mean=0.0003, std=0.1271
transformer.layers.2.norm1.weight: mean=-0.0002, std=0.0385
transformer.layers.2.norm1.bias: mean=-0.0007, std=0.1362
transformer.layers.2.norm2.weight: mean=0.0001, std=0.0407
transformer.layers.2.norm2.bias: mean=-0.0001, std=0.1382
transformer.layers.3.self_attn.in_proj_weight: mean=-0.0000, std=0.0106
transformer.layers.3.self_attn.in_proj_bias: mean=0.0002, std=0.0351
transformer.layers.3.self_attn.out_proj.weight: mean=0.0000, std=0.0226
transformer.layers.3.self_attn.out_proj.bias: mean=-0.0001, std=0.1040
transformer.layers.3.linear1.weight: mean=-0.0000, std=0.0055
transformer.layers.3.linear1.bias: mean=-0.0001, std=0.0147
transformer.layers.3.linear2.weight: mean=-0.0000, std=0.0161
transformer.layers.3.linear2.bias: mean=-0.0002, std=0.0993
transformer.layers.3.norm1.weight: mean=-0.0001, std=0.0384
transformer.layers.3.norm1.bias: mean=-0.0005, std=0.1068
transformer.layers.3.norm2.weight: mean=0.0000, std=0.0404
transformer.layers.3.norm2.bias: mean=-0.0004, std=0.1103
transformer.layers.4.self_attn.in_proj_weight: mean=0.0000, std=0.0105
transformer.layers.4.self_attn.in_proj_bias: mean=0.0004, std=0.0280
transformer.layers.4.self_attn.out_proj.weight: mean=-0.0000, std=0.0225
transformer.layers.4.self_attn.out_proj.bias: mean=0.0002, std=0.0833
transformer.layers.4.linear1.weight: mean=-0.0000, std=0.0057
transformer.layers.4.linear1.bias: mean=-0.0001, std=0.0124
transformer.layers.4.linear2.weight: mean=-0.0000, std=0.0150
transformer.layers.4.linear2.bias: mean=-0.0000, std=0.0816
transformer.layers.4.norm1.weight: mean=-0.0000, std=0.0390
transformer.layers.4.norm1.bias: mean=-0.0005, std=0.0873
transformer.layers.4.norm2.weight: mean=0.0001, std=0.0409
transformer.layers.4.norm2.bias: mean=-0.0004, std=0.0913
transformer.layers.5.self_attn.in_proj_weight: mean=-0.0000, std=0.0105
transformer.layers.5.self_attn.in_proj_bias: mean=0.0006, std=0.0230
transformer.layers.5.self_attn.out_proj.weight: mean=0.0000, std=0.0225
transformer.layers.5.self_attn.out_proj.bias: mean=-0.0000, std=0.0690
transformer.layers.5.linear1.weight: mean=0.0000, std=0.0059
transformer.layers.5.linear1.bias: mean=-0.0001, std=0.0107
transformer.layers.5.linear2.weight: mean=0.0000, std=0.0144
transformer.layers.5.linear2.bias: mean=0.0001, std=0.0690
transformer.layers.5.norm1.weight: mean=-0.0000, std=0.0397
transformer.layers.5.norm1.bias: mean=-0.0006, std=0.0741
transformer.layers.5.norm2.weight: mean=0.0000, std=0.0415
transformer.layers.5.norm2.bias: mean=-0.0005, std=0.0779
transformer.layers.6.self_attn.in_proj_weight: mean=-0.0000, std=0.0107
transformer.layers.6.self_attn.in_proj_bias: mean=0.0007, std=0.0196
transformer.layers.6.self_attn.out_proj.weight: mean=-0.0000, std=0.0227
transformer.layers.6.self_attn.out_proj.bias: mean=0.0001, std=0.0589
transformer.layers.6.linear1.weight: mean=-0.0000, std=0.0062
transformer.layers.6.linear1.bias: mean=-0.0000, std=0.0097
transformer.layers.6.linear2.weight: mean=0.0000, std=0.0142
transformer.layers.6.linear2.bias: mean=0.0001, std=0.0606
transformer.layers.6.norm1.weight: mean=0.0000, std=0.0406
transformer.layers.6.norm1.bias: mean=-0.0005, std=0.0651
transformer.layers.6.norm2.weight: mean=0.0000, std=0.0424
transformer.layers.6.norm2.bias: mean=-0.0005, std=0.0685
transformer.layers.7.self_attn.in_proj_weight: mean=0.0000, std=0.0108
transformer.layers.7.self_attn.in_proj_bias: mean=0.0007, std=0.0172
transformer.layers.7.self_attn.out_proj.weight: mean=-0.0000, std=0.0230
transformer.layers.7.self_attn.out_proj.bias: mean=0.0000, std=0.0518
transformer.layers.7.linear1.weight: mean=0.0000, std=0.0065
transformer.layers.7.linear1.bias: mean=0.0000, std=0.0090
transformer.layers.7.linear2.weight: mean=0.0000, std=0.0142
transformer.layers.7.linear2.bias: mean=0.0000, std=0.0551
transformer.layers.7.norm1.weight: mean=0.0000, std=0.0419
transformer.layers.7.norm1.bias: mean=-0.0006, std=0.0590
transformer.layers.7.norm2.weight: mean=0.0000, std=0.0433
transformer.layers.7.norm2.bias: mean=-0.0005, std=0.0620
transformer.layers.8.self_attn.in_proj_weight: mean=-0.0000, std=0.0110
transformer.layers.8.self_attn.in_proj_bias: mean=0.0007, std=0.0156
transformer.layers.8.self_attn.out_proj.weight: mean=-0.0000, std=0.0233
transformer.layers.8.self_attn.out_proj.bias: mean=0.0001, std=0.0466
transformer.layers.8.linear1.weight: mean=-0.0000, std=0.0068
transformer.layers.8.linear1.bias: mean=0.0000, std=0.0086
transformer.layers.8.linear2.weight: mean=-0.0000, std=0.0143
transformer.layers.8.linear2.bias: mean=-0.0000, std=0.0513
transformer.layers.8.norm1.weight: mean=0.0001, std=0.0431
transformer.layers.8.norm1.bias: mean=-0.0006, std=0.0550
transformer.layers.8.norm2.weight: mean=0.0000, std=0.0442
transformer.layers.8.norm2.bias: mean=-0.0005, std=0.0575
transformer.layers.9.self_attn.in_proj_weight: mean=0.0000, std=0.0112
transformer.layers.9.self_attn.in_proj_bias: mean=0.0007, std=0.0145
transformer.layers.9.self_attn.out_proj.weight: mean=0.0000, std=0.0238
transformer.layers.9.self_attn.out_proj.bias: mean=-0.0000, std=0.0432
transformer.layers.9.linear1.weight: mean=-0.0000, std=0.0070
transformer.layers.9.linear1.bias: mean=0.0000, std=0.0083
transformer.layers.9.linear2.weight: mean=0.0000, std=0.0145
transformer.layers.9.linear2.bias: mean=0.0001, std=0.0489
transformer.layers.9.norm1.weight: mean=0.0001, std=0.0443
transformer.layers.9.norm1.bias: mean=-0.0006, std=0.0527
transformer.layers.9.norm2.weight: mean=0.0000, std=0.0451
transformer.layers.9.norm2.bias: mean=-0.0005, std=0.0547
transformer.layers.10.self_attn.in_proj_weight: mean=-0.0000, std=0.0115
transformer.layers.10.self_attn.in_proj_bias: mean=0.0007, std=0.0139
transformer.layers.10.self_attn.out_proj.weight: mean=0.0000, std=0.0243
transformer.layers.10.self_attn.out_proj.bias: mean=-0.0001, std=0.0411
transformer.layers.10.linear1.weight: mean=-0.0000, std=0.0073
transformer.layers.10.linear1.bias: mean=0.0001, std=0.0083
transformer.layers.10.linear2.weight: mean=0.0000, std=0.0149
transformer.layers.10.linear2.bias: mean=0.0001, std=0.0479
transformer.layers.10.norm1.weight: mean=0.0001, std=0.0454
transformer.layers.10.norm1.bias: mean=-0.0005, std=0.0516
transformer.layers.10.norm2.weight: mean=0.0000, std=0.0460
transformer.layers.10.norm2.bias: mean=-0.0006, std=0.0532
transformer.layers.11.self_attn.in_proj_weight: mean=-0.0000, std=0.0117
transformer.layers.11.self_attn.in_proj_bias: mean=0.0007, std=0.0134
transformer.layers.11.self_attn.out_proj.weight: mean=-0.0000, std=0.0247
transformer.layers.11.self_attn.out_proj.bias: mean=0.0001, std=0.0397
transformer.layers.11.linear1.weight: mean=0.0000, std=0.0075
transformer.layers.11.linear1.bias: mean=0.0001, std=0.0083
transformer.layers.11.linear2.weight: mean=-0.0000, std=0.0153
transformer.layers.11.linear2.bias: mean=-0.0000, std=0.0476
transformer.layers.11.norm1.weight: mean=0.0001, std=0.0465
transformer.layers.11.norm1.bias: mean=-0.0005, std=0.0513
transformer.layers.11.norm2.weight: mean=-0.0000, std=0.0468
transformer.layers.11.norm2.bias: mean=-0.0006, std=0.0526
transformer.layers.12.self_attn.in_proj_weight: mean=0.0000, std=0.0120
transformer.layers.12.self_attn.in_proj_bias: mean=0.0006, std=0.0133
transformer.layers.12.self_attn.out_proj.weight: mean=-0.0000, std=0.0254
transformer.layers.12.self_attn.out_proj.bias: mean=0.0001, std=0.0393
transformer.layers.12.linear1.weight: mean=0.0000, std=0.0077
transformer.layers.12.linear1.bias: mean=0.0001, std=0.0083
transformer.layers.12.linear2.weight: mean=0.0000, std=0.0157
transformer.layers.12.linear2.bias: mean=0.0001, std=0.0480
transformer.layers.12.norm1.weight: mean=0.0000, std=0.0474
transformer.layers.12.norm1.bias: mean=-0.0004, std=0.0517
transformer.layers.12.norm2.weight: mean=-0.0000, std=0.0477
transformer.layers.12.norm2.bias: mean=-0.0007, std=0.0526
transformer.layers.13.self_attn.in_proj_weight: mean=0.0000, std=0.0121
transformer.layers.13.self_attn.in_proj_bias: mean=0.0006, std=0.0131
transformer.layers.13.self_attn.out_proj.weight: mean=0.0000, std=0.0260
transformer.layers.13.self_attn.out_proj.bias: mean=-0.0001, std=0.0392
transformer.layers.13.linear1.weight: mean=0.0000, std=0.0080
transformer.layers.13.linear1.bias: mean=0.0001, std=0.0085
transformer.layers.13.linear2.weight: mean=-0.0000, std=0.0162
transformer.layers.13.linear2.bias: mean=-0.0000, std=0.0490
transformer.layers.13.norm1.weight: mean=0.0000, std=0.0483
transformer.layers.13.norm1.bias: mean=-0.0004, std=0.0526
transformer.layers.13.norm2.weight: mean=-0.0000, std=0.0486
transformer.layers.13.norm2.bias: mean=-0.0007, std=0.0534
transformer.layers.14.self_attn.in_proj_weight: mean=-0.0000, std=0.0126
transformer.layers.14.self_attn.in_proj_bias: mean=0.0005, std=0.0134
transformer.layers.14.self_attn.out_proj.weight: mean=0.0000, std=0.0269
transformer.layers.14.self_attn.out_proj.bias: mean=0.0000, std=0.0398
transformer.layers.14.linear1.weight: mean=-0.0000, std=0.0082
transformer.layers.14.linear1.bias: mean=0.0001, std=0.0086
transformer.layers.14.linear2.weight: mean=-0.0000, std=0.0166
transformer.layers.14.linear2.bias: mean=-0.0001, std=0.0500
transformer.layers.14.norm1.weight: mean=0.0000, std=0.0492
transformer.layers.14.norm1.bias: mean=-0.0003, std=0.0539
transformer.layers.14.norm2.weight: mean=-0.0000, std=0.0493
transformer.layers.14.norm2.bias: mean=-0.0008, std=0.0545
transformer.layers.15.self_attn.in_proj_weight: mean=0.0000, std=0.0131
transformer.layers.15.self_attn.in_proj_bias: mean=0.0006, std=0.0137
transformer.layers.15.self_attn.out_proj.weight: mean=-0.0000, std=0.0280
transformer.layers.15.self_attn.out_proj.bias: mean=-0.0000, std=0.0408
transformer.layers.15.linear1.weight: mean=0.0000, std=0.0084
transformer.layers.15.linear1.bias: mean=0.0002, std=0.0087
transformer.layers.15.linear2.weight: mean=-0.0000, std=0.0170
transformer.layers.15.linear2.bias: mean=-0.0001, std=0.0516
transformer.layers.15.norm1.weight: mean=0.0001, std=0.0500
transformer.layers.15.norm1.bias: mean=-0.0003, std=0.0556
transformer.layers.15.norm2.weight: mean=-0.0000, std=0.0499
transformer.layers.15.norm2.bias: mean=-0.0009, std=0.0562
transformer.layers.16.self_attn.in_proj_weight: mean=0.0000, std=0.0135
transformer.layers.16.self_attn.in_proj_bias: mean=0.0004, std=0.0141
transformer.layers.16.self_attn.out_proj.weight: mean=-0.0000, std=0.0292
transformer.layers.16.self_attn.out_proj.bias: mean=-0.0000, std=0.0424
transformer.layers.16.linear1.weight: mean=0.0000, std=0.0088
transformer.layers.16.linear1.bias: mean=0.0001, std=0.0091
transformer.layers.16.linear2.weight: mean=0.0000, std=0.0179
transformer.layers.16.linear2.bias: mean=0.0001, std=0.0541
transformer.layers.16.norm1.weight: mean=0.0001, std=0.0509
transformer.layers.16.norm1.bias: mean=-0.0002, std=0.0578
transformer.layers.16.norm2.weight: mean=0.0000, std=0.0510
transformer.layers.16.norm2.bias: mean=-0.0009, std=0.0583
transformer.layers.17.self_attn.in_proj_weight: mean=-0.0000, std=0.0143
transformer.layers.17.self_attn.in_proj_bias: mean=0.0004, std=0.0148
transformer.layers.17.self_attn.out_proj.weight: mean=0.0000, std=0.0307
transformer.layers.17.self_attn.out_proj.bias: mean=-0.0001, std=0.0445
transformer.layers.17.linear1.weight: mean=-0.0000, std=0.0092
transformer.layers.17.linear1.bias: mean=0.0002, std=0.0096
transformer.layers.17.linear2.weight: mean=-0.0000, std=0.0184
transformer.layers.17.linear2.bias: mean=-0.0002, std=0.0564
transformer.layers.17.norm1.weight: mean=0.0001, std=0.0525
transformer.layers.17.norm1.bias: mean=-0.0002, std=0.0604
transformer.layers.17.norm2.weight: mean=-0.0000, std=0.0526
transformer.layers.17.norm2.bias: mean=-0.0008, std=0.0609
transformer.layers.18.self_attn.in_proj_weight: mean=-0.0000, std=0.0150
transformer.layers.18.self_attn.in_proj_bias: mean=0.0003, std=0.0156
transformer.layers.18.self_attn.out_proj.weight: mean=-0.0000, std=0.0321
transformer.layers.18.self_attn.out_proj.bias: mean=0.0002, std=0.0463
transformer.layers.18.linear1.weight: mean=-0.0000, std=0.0098
transformer.layers.18.linear1.bias: mean=0.0001, std=0.0101
transformer.layers.18.linear2.weight: mean=0.0000, std=0.0194
transformer.layers.18.linear2.bias: mean=0.0002, std=0.0599
transformer.layers.18.norm1.weight: mean=0.0001, std=0.0550
transformer.layers.18.norm1.bias: mean=-0.0001, std=0.0637
transformer.layers.18.norm2.weight: mean=0.0000, std=0.0555
transformer.layers.18.norm2.bias: mean=-0.0009, std=0.0643
transformer.layers.19.self_attn.in_proj_weight: mean=0.0000, std=0.0163
transformer.layers.19.self_attn.in_proj_bias: mean=0.0003, std=0.0169
transformer.layers.19.self_attn.out_proj.weight: mean=0.0000, std=0.0341
transformer.layers.19.self_attn.out_proj.bias: mean=-0.0003, std=0.0492
transformer.layers.19.linear1.weight: mean=-0.0000, std=0.0106
transformer.layers.19.linear1.bias: mean=0.0001, std=0.0109
transformer.layers.19.linear2.weight: mean=0.0000, std=0.0204
transformer.layers.19.linear2.bias: mean=0.0001, std=0.0632
transformer.layers.19.norm1.weight: mean=0.0000, std=0.0587
transformer.layers.19.norm1.bias: mean=0.0000, std=0.0676
transformer.layers.19.norm2.weight: mean=-0.0000, std=0.0598
transformer.layers.19.norm2.bias: mean=-0.0007, std=0.0681
transformer.layers.20.self_attn.in_proj_weight: mean=0.0000, std=0.0177
transformer.layers.20.self_attn.in_proj_bias: mean=0.0001, std=0.0183
transformer.layers.20.self_attn.out_proj.weight: mean=0.0000, std=0.0366
transformer.layers.20.self_attn.out_proj.bias: mean=-0.0000, std=0.0531
transformer.layers.20.linear1.weight: mean=-0.0000, std=0.0116
transformer.layers.20.linear1.bias: mean=-0.0001, std=0.0119
transformer.layers.20.linear2.weight: mean=-0.0000, std=0.0215
transformer.layers.20.linear2.bias: mean=-0.0002, std=0.0669
transformer.layers.20.norm1.weight: mean=-0.0001, std=0.0641
transformer.layers.20.norm1.bias: mean=0.0002, std=0.0720
transformer.layers.20.norm2.weight: mean=0.0000, std=0.0659
transformer.layers.20.norm2.bias: mean=-0.0003, std=0.0724
transformer.layers.21.self_attn.in_proj_weight: mean=0.0000, std=0.0195
transformer.layers.21.self_attn.in_proj_bias: mean=0.0001, std=0.0201
transformer.layers.21.self_attn.out_proj.weight: mean=-0.0000, std=0.0399
transformer.layers.21.self_attn.out_proj.bias: mean=0.0001, std=0.0581
transformer.layers.21.linear1.weight: mean=0.0000, std=0.0130
transformer.layers.21.linear1.bias: mean=-0.0000, std=0.0133
transformer.layers.21.linear2.weight: mean=0.0001, std=0.0239
transformer.layers.21.linear2.bias: mean=0.0005, std=0.0743
transformer.layers.21.norm1.weight: mean=-0.0000, std=0.0722
transformer.layers.21.norm1.bias: mean=-0.0001, std=0.0789
transformer.layers.21.norm2.weight: mean=-0.0000, std=0.0742
transformer.layers.21.norm2.bias: mean=0.0002, std=0.0794
transformer.layers.22.self_attn.in_proj_weight: mean=0.0000, std=0.0221
transformer.layers.22.self_attn.in_proj_bias: mean=-0.0001, std=0.0228
transformer.layers.22.self_attn.out_proj.weight: mean=0.0000, std=0.0459
transformer.layers.22.self_attn.out_proj.bias: mean=-0.0005, std=0.0668
transformer.layers.22.linear1.weight: mean=-0.0000, std=0.0157
transformer.layers.22.linear1.bias: mean=-0.0002, std=0.0161
transformer.layers.22.linear2.weight: mean=0.0001, std=0.0281
transformer.layers.22.linear2.bias: mean=0.0007, std=0.0875
transformer.layers.22.norm1.weight: mean=-0.0001, std=0.0873
transformer.layers.22.norm1.bias: mean=-0.0002, std=0.0913
transformer.layers.22.norm2.weight: mean=-0.0000, std=0.0908
transformer.layers.22.norm2.bias: mean=0.0006, std=0.0933
transformer.layers.23.self_attn.in_proj_weight: mean=0.0000, std=0.0282
transformer.layers.23.self_attn.in_proj_bias: mean=0.0001, std=0.0291
transformer.layers.23.self_attn.out_proj.weight: mean=0.0000, std=0.0599
transformer.layers.23.self_attn.out_proj.bias: mean=-0.0003, std=0.0884
transformer.layers.23.linear1.weight: mean=-0.0000, std=0.0207
transformer.layers.23.linear1.bias: mean=-0.0002, std=0.0212
transformer.layers.23.linear2.weight: mean=-0.0000, std=0.0369
transformer.layers.23.linear2.bias: mean=-0.0002, std=0.1155
transformer.layers.23.norm1.weight: mean=-0.0003, std=0.1207
transformer.layers.23.norm1.bias: mean=-0.0002, std=0.1184
transformer.layers.23.norm2.weight: mean=0.0105, std=0.1289
transformer.layers.23.norm2.bias: mean=-0.0094, std=0.1244
connector.0.weight: mean=-0.0000, std=0.1500
connector.0.bias: mean=0.0067, std=0.1541
connector.2.weight: mean=-0.0008, std=0.0731
connector.2.bias: mean=-0.0070, std=0.2366
Gradients clipped from 1.0000 to 1.0
Gradient norm: 1.0000
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0024.flac
Waveform stats - mean: -0.0000, std: 0.0503, min: -0.4133, max: 0.5659
Resampled waveform stats - mean: -0.0000, std: 0.0503, min: -0.4133, max: 0.5659
Raw mel spectrogram stats - mean: 0.9469, std: 7.3449, min: 0.0000, max: 370.8018
Log mel spectrogram stats - mean: -7.0682, std: 4.6971, min: -13.8153, max: 5.9157
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4364, max: 2.7642
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4365, max: 2.7637
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE IS A WONDERFUL INSIGHT IN HEAVEN'S BROAD AND SIMPLE SUNSHINE
Prediction: 000000000000000000000
Loss: 16.2207
outputs.loss tensor(16.2207, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0025.flac
Waveform stats - mean: -0.0000, std: 0.0804, min: -0.5038, max: 0.4544
Resampled waveform stats - mean: -0.0000, std: 0.0804, min: -0.5038, max: 0.4544
Raw mel spectrogram stats - mean: 2.4115, std: 21.6813, min: 0.0000, max: 669.3558
Log mel spectrogram stats - mean: -5.8977, std: 3.8795, min: -13.7039, max: 6.5063
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0122, max: 3.1973
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0117, max: 3.1973
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I HAVE ONE GREAT PRIVILEGE
Prediction: 0000000
Loss: 17.2578
outputs.loss tensor(17.2578, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2902/9008/2902-9008-0009.flac
Waveform stats - mean: -0.0000, std: 0.0911, min: -0.6763, max: 0.8244
Resampled waveform stats - mean: -0.0000, std: 0.0911, min: -0.6763, max: 0.8244
Raw mel spectrogram stats - mean: 3.1096, std: 40.7833, min: 0.0000, max: 2313.0964
Log mel spectrogram stats - mean: -5.1293, std: 3.7534, min: -13.7292, max: 7.7463
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2912, max: 3.4304
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2910, max: 3.4297
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HOW CAN HE WHOSE SPHERE LIES ABOVE THE STARS STOOP EVERY MOMENT TO EARTH
Prediction: 0000000000000000000000
Loss: 16.2017
outputs.loss tensor(16.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0017.flac
Waveform stats - mean: -0.0000, std: 0.0592, min: -0.4659, max: 0.4071
Resampled waveform stats - mean: -0.0000, std: 0.0592, min: -0.4659, max: 0.4071
Raw mel spectrogram stats - mean: 1.3095, std: 6.7339, min: 0.0000, max: 338.3148
Log mel spectrogram stats - mean: -6.0141, std: 4.5612, min: -13.7738, max: 5.8240
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7013, max: 2.5954
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7012, max: 2.5957
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: LETTY TOO WAS OVERCOME MORE THAN EVER SHE HAD BEEN BY MUSIC
Prediction: 000000000000000
Loss: 16.9882
outputs.loss tensor(16.9882, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0054.flac
Waveform stats - mean: -0.0000, std: 0.1065, min: -0.5529, max: 0.4826
Resampled waveform stats - mean: -0.0000, std: 0.1065, min: -0.5529, max: 0.4826
Raw mel spectrogram stats - mean: 4.2234, std: 17.5593, min: 0.0000, max: 343.4982
Log mel spectrogram stats - mean: -5.1527, std: 4.6180, min: -13.8070, max: 5.8392
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8740, max: 2.3802
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8740, max: 2.3809
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL SHOW YOU WHO'S MASTER HERE MY GOOD SIR
Prediction: 0000000000000
Loss: 15.9092
outputs.loss tensor(15.9092, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0019.flac
Waveform stats - mean: 0.0001, std: 0.1203, min: -0.6684, max: 0.6441
Resampled waveform stats - mean: 0.0001, std: 0.1203, min: -0.6684, max: 0.6441
Raw mel spectrogram stats - mean: 5.3541, std: 39.7015, min: 0.0000, max: 2288.4668
Log mel spectrogram stats - mean: -4.3030, std: 3.8185, min: -12.7837, max: 7.7356
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2209, max: 3.1527
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2207, max: 3.1523
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32256
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32256
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 225792
Inf count: 0
audio_emb.shape torch.Size([1, 63, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GIVE ME MY HORSE AND ARMS AND WAIT FOR ME HERE I WILL GO IN QUEST OF THIS KNIGHT AND DEAD OR ALIVE I WILL MAKE HIM KEEP HIS WORD PLIGHTED TO SO GREAT BEAUTY
Prediction: 00000000000000000000000000000000000000000000
Loss: 15.8391
outputs.loss tensor(15.8391, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0031.flac
Waveform stats - mean: 0.0013, std: 0.0266, min: -0.2657, max: 0.3010
Resampled waveform stats - mean: 0.0013, std: 0.0266, min: -0.2657, max: 0.3010
Raw mel spectrogram stats - mean: 0.2542, std: 3.0313, min: 0.0000, max: 220.4211
Log mel spectrogram stats - mean: -5.5131, std: 3.0468, min: -12.6116, max: 5.3955
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3298, max: 3.5804
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3301, max: 3.5801
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33792
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33792
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 236544
Inf count: 0
audio_emb.shape torch.Size([1, 66, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS IS VERY GOOD OF YOU HE BEGAN GLANCING DOWN AT THE AGED DETECTIVE'S BUNDLED UP LEGS AND GENTLY PUSHING A CHAIR TOWARDS HIM
Prediction: 0000000000000000000000000000000000000000
Loss: 16.1805
outputs.loss tensor(16.1805, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0042.flac
Waveform stats - mean: -0.0000, std: 0.0479, min: -0.3588, max: 0.4510
Resampled waveform stats - mean: -0.0000, std: 0.0479, min: -0.3588, max: 0.4510
Raw mel spectrogram stats - mean: 0.8608, std: 4.5071, min: 0.0000, max: 158.1669
Log mel spectrogram stats - mean: -6.1175, std: 4.4553, min: -13.8153, max: 5.0637
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7278, max: 2.5096
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7275, max: 2.5098
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WE'VE LOST THE KEY OF THE CELLAR AND THERE'S NOTHING OUT EXCEPT WATER AND I DON'T THINK YOU'D CARE FOR THAT
Prediction: 00000000000000000000000000000
Loss: 15.9931
outputs.loss tensor(15.9931, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0007.flac
Waveform stats - mean: -0.0000, std: 0.0637, min: -0.3165, max: 0.6448
Resampled waveform stats - mean: -0.0000, std: 0.0637, min: -0.3165, max: 0.6448
Raw mel spectrogram stats - mean: 1.5152, std: 7.6342, min: 0.0000, max: 219.8752
Log mel spectrogram stats - mean: -5.8413, std: 4.4515, min: -13.8144, max: 5.3931
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7911, max: 2.5237
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7910, max: 2.5234
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AFTER MONTHS OF SEPARATION HE RECEIVED A VISIT FROM HERBERT
Prediction: 00000000000000000
Loss: 16.0122
outputs.loss tensor(16.0122, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130737/652-130737-0005.flac
Waveform stats - mean: -0.0001, std: 0.0557, min: -0.4267, max: 0.3801
Resampled waveform stats - mean: -0.0001, std: 0.0557, min: -0.4267, max: 0.3801
Raw mel spectrogram stats - mean: 1.0832, std: 5.7238, min: 0.0000, max: 275.0829
Log mel spectrogram stats - mean: -5.3356, std: 4.2073, min: -13.7887, max: 5.6171
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0092, max: 2.6033
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0098, max: 2.6035
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: CLARETS ARE VALUED FOR THEIR FLAVOR AND FOR THEIR TONIC PROPERTIES
Prediction: 000000000000000000
Loss: 16.8159
outputs.loss tensor(16.8159, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275156/8297-275156-0006.flac
Waveform stats - mean: -0.0000, std: 0.0656, min: -0.3223, max: 0.5814
Resampled waveform stats - mean: -0.0000, std: 0.0656, min: -0.3223, max: 0.5814
Raw mel spectrogram stats - mean: 1.6124, std: 8.9421, min: 0.0000, max: 332.2232
Log mel spectrogram stats - mean: -5.9797, std: 4.7396, min: -13.8144, max: 5.8058
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6530, max: 2.4866
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6533, max: 2.4863
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HAD PROMISED TO DO HIS BEST TOWARD PERSUADING CATHERINE TO GRANT SYDNEY AN INTERVIEW
Prediction: 0000000000000000000000000000
Loss: 16.2725
outputs.loss tensor(16.2725, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0015.flac
Waveform stats - mean: -0.0000, std: 0.0501, min: -0.3195, max: 0.3040
Resampled waveform stats - mean: -0.0000, std: 0.0501, min: -0.3195, max: 0.3040
Raw mel spectrogram stats - mean: 0.9397, std: 6.1545, min: 0.0000, max: 212.2032
Log mel spectrogram stats - mean: -5.4754, std: 3.6501, min: -13.4892, max: 5.3575
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1955, max: 2.9678
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1953, max: 2.9688
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE WENT IN AND EXAMINED HIS LETTERS BUT THERE WAS NOTHING FROM CARRIE
Prediction: 0000000000000000000
Loss: 15.6527
outputs.loss tensor(15.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149897/2277-149897-0036.flac
Waveform stats - mean: -0.0000, std: 0.0723, min: -0.5601, max: 0.5479
Resampled waveform stats - mean: -0.0000, std: 0.0723, min: -0.5601, max: 0.5479
Raw mel spectrogram stats - mean: 1.9575, std: 18.3887, min: 0.0000, max: 1061.4790
Log mel spectrogram stats - mean: -5.8649, std: 3.9727, min: -13.7608, max: 6.9674
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9875, max: 3.2301
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9873, max: 3.2305
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SO HERE IT WAS SPREAD OUT CLEAR BEFORE HIM AND NOW HE KNEW WHAT TO EXPECT
Prediction: 000000000000000000
Loss: 15.9780
outputs.loss tensor(15.9780, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0022.flac
Waveform stats - mean: -0.0002, std: 0.0701, min: -0.4161, max: 0.4670
Resampled waveform stats - mean: -0.0002, std: 0.0701, min: -0.4161, max: 0.4670
Raw mel spectrogram stats - mean: 1.8303, std: 14.0295, min: 0.0000, max: 543.4526
Log mel spectrogram stats - mean: -4.9337, std: 3.7471, min: -13.2266, max: 6.2979
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2131, max: 2.9974
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2129, max: 2.9980
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WE WILL HUNT WOLVES THE COUNTRY IS ALIVE WITH THEM AND THE GOVERNMENT GIVES A BOUNTY OF FIFTEEN DOLLARS FOR EVERY SCALP TAKEN
Prediction: 000000000000000000000000000000000000000000
Loss: 15.6639
outputs.loss tensor(15.6639, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0010.flac
Waveform stats - mean: 0.0000, std: 0.1031, min: -0.9995, max: 0.9996
Resampled waveform stats - mean: 0.0000, std: 0.1031, min: -0.9995, max: 0.9996
Raw mel spectrogram stats - mean: 3.4972, std: 37.1823, min: 0.0000, max: 5752.9712
Log mel spectrogram stats - mean: -5.3350, std: 4.7997, min: -13.8036, max: 8.6575
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7644, max: 2.9153
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7646, max: 2.9160
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE PROGRESS OF PRESIDENT DAVIS TO THE NEW CAPITAL SET IN THE VERY FACE OF THE FOE WAS TO BE ONE HUGE TRIUMPH OF FAITH AND LOYALTY
Prediction: 0000000000000000000000000000000000000
Loss: 15.8797
outputs.loss tensor(15.8797, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0005.flac
Waveform stats - mean: 0.0001, std: 0.0824, min: -0.6341, max: 0.5141
Resampled waveform stats - mean: 0.0001, std: 0.0824, min: -0.6341, max: 0.5141
Raw mel spectrogram stats - mean: 2.5297, std: 12.1372, min: 0.0000, max: 649.3073
Log mel spectrogram stats - mean: -2.7737, std: 3.1865, min: -12.8779, max: 6.4759
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -3.1710, max: 2.9028
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -3.1719, max: 2.9023
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34304
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34304
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 240128
Inf count: 0
audio_emb.shape torch.Size([1, 67, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'M NOT A TALKER YOU KNOW AND AS THE LAWS OF GRAVITATION FORBID MY SOARING ALOFT ANYWHERE I CAN ONLY EXPRESS MY JOYFULLY UPLIFTED STATE OF MIND BY PRANCING AS YOU CALL IT
Prediction: 0000000000000000000000000000000000000000000000000000000
Loss: 15.4788
outputs.loss tensor(15.4788, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0023.flac
Waveform stats - mean: -0.0001, std: 0.0648, min: -0.5707, max: 0.4488
Resampled waveform stats - mean: -0.0001, std: 0.0648, min: -0.5707, max: 0.4488
Raw mel spectrogram stats - mean: 1.5746, std: 8.6437, min: 0.0000, max: 334.3831
Log mel spectrogram stats - mean: -4.3405, std: 3.6639, min: -13.6250, max: 5.8123
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5340, max: 2.7710
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5332, max: 2.7715
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ACCUSTOMED AS I HAD BEEN TO THE STEAM FERRY BOATS OF THE ELBE I FOUND THE LONG OARS OF THE BOATMEN BUT SORRY MEANS OF LOCOMOTION
Prediction: 000000000000000000000000000000000000000000
Loss: 15.5189
outputs.loss tensor(15.5189, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/148538/1988-148538-0015.flac
Waveform stats - mean: 0.0000, std: 0.0288, min: -0.2755, max: 0.3732
Resampled waveform stats - mean: 0.0000, std: 0.0288, min: -0.2755, max: 0.3732
Raw mel spectrogram stats - mean: 0.3112, std: 2.3355, min: 0.0000, max: 144.4161
Log mel spectrogram stats - mean: -6.9260, std: 3.8717, min: -13.8148, max: 4.9727
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7793, max: 3.0732
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7793, max: 3.0723
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 55808
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 55808
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 390656
Inf count: 0
audio_emb.shape torch.Size([1, 109, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THESE PERSONS THEN DISPLAYED TOWARDS EACH OTHER PRECISELY THE SAME PUERILE JEALOUSIES WHICH ANIMATE THE MEN OF DEMOCRACIES THE SAME EAGERNESS TO SNATCH THE SMALLEST ADVANTAGES WHICH THEIR EQUALS CONTESTED AND THE SAME DESIRE TO PARADE OSTENTATIOUSLY THOSE OF WHICH THEY WERE IN POSSESSION
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.6122
outputs.loss tensor(16.6122, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/281318/7850-281318-0007.flac
Waveform stats - mean: 0.0000, std: 0.0522, min: -0.4217, max: 0.3608
Resampled waveform stats - mean: 0.0000, std: 0.0522, min: -0.4217, max: 0.3608
Raw mel spectrogram stats - mean: 1.0194, std: 7.4785, min: 0.0000, max: 326.5267
Log mel spectrogram stats - mean: -5.7826, std: 4.0543, min: -13.7657, max: 5.7885
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9690, max: 2.8540
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9688, max: 2.8535
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13824
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13824
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 96768
Inf count: 0
audio_emb.shape torch.Size([1, 27, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND SHE SAW THE OTHER BIRDS HOPPING ABOUT AND TWITTERING HELPLESSLY
Prediction: 000000000000000000000
Loss: 16.1991
outputs.loss tensor(16.1991, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0022.flac
Waveform stats - mean: -0.0001, std: 0.0369, min: -0.3686, max: 0.2873
Resampled waveform stats - mean: -0.0001, std: 0.0369, min: -0.3686, max: 0.2873
Raw mel spectrogram stats - mean: 0.5080, std: 3.9626, min: 0.0000, max: 179.4570
Log mel spectrogram stats - mean: -7.5740, std: 4.9697, min: -13.8155, max: 5.1899
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.2559, max: 2.5684
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.2559, max: 2.5684
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE DESIGN WAS DIFFERENT BUT THE THING WAS CLEARLY THE SAME
Prediction: 000000000000000
Loss: 16.4734
outputs.loss tensor(16.4734, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0011.flac
Waveform stats - mean: -0.0001, std: 0.0490, min: -0.3145, max: 0.4568
Resampled waveform stats - mean: -0.0001, std: 0.0490, min: -0.3145, max: 0.4568
Raw mel spectrogram stats - mean: 0.8995, std: 7.8129, min: 0.0000, max: 418.1047
Log mel spectrogram stats - mean: -6.1335, std: 3.6381, min: -13.5484, max: 6.0357
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0381, max: 3.3449
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0391, max: 3.3457
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: QUICKLY IT WAS COVERED WITH BRIGHT RED SPOTS I THOUGHT I HAD NEVER SEEN ANY BLOOD SO BRIGHT
Prediction: 00000000000000000000000000000
Loss: 15.8590
outputs.loss tensor(15.8590, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0007.flac
Waveform stats - mean: -0.0000, std: 0.0512, min: -0.3123, max: 0.3803
Resampled waveform stats - mean: -0.0000, std: 0.0512, min: -0.3123, max: 0.3803
Raw mel spectrogram stats - mean: 0.9818, std: 5.9947, min: 0.0000, max: 281.2787
Log mel spectrogram stats - mean: -6.5429, std: 4.9196, min: -13.8113, max: 5.6393
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4775, max: 2.4763
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4775, max: 2.4766
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I SOON FOUND OUT THAT HE HAD CAUGHT SIGHT OF THE RELIEF ON THE ROAD AND WAS AFRAID TO SHOOT I QUICKLY MADE UP MY MIND
Prediction: 0000000000000000000000000000000000000
Loss: 15.8101
outputs.loss tensor(15.8101, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64038/5694-64038-0025.flac
Waveform stats - mean: 0.0000, std: 0.0765, min: -0.4011, max: 0.4225
Resampled waveform stats - mean: 0.0000, std: 0.0765, min: -0.4011, max: 0.4225
Raw mel spectrogram stats - mean: 2.1945, std: 13.9909, min: 0.0000, max: 585.4449
Log mel spectrogram stats - mean: -5.4524, std: 4.7965, min: -13.8152, max: 6.3724
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7435, max: 2.4653
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7432, max: 2.4648
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20992
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20992
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 146944
Inf count: 0
audio_emb.shape torch.Size([1, 41, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HADN'T SEEN ANYTHING TO SHOOT AT BUT HE BLAZED AWAY HE LOADED AND FIRED THE SECOND TIME WHEN THEY WERE ORDERED TO RETREAT
Prediction: 00000000000000000000000000000000000000
Loss: 16.0329
outputs.loss tensor(16.0329, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0011.flac
Waveform stats - mean: -0.0001, std: 0.0395, min: -0.3542, max: 0.3333
Resampled waveform stats - mean: -0.0001, std: 0.0395, min: -0.3542, max: 0.3333
Raw mel spectrogram stats - mean: 0.5616, std: 5.6367, min: 0.0000, max: 288.0907
Log mel spectrogram stats - mean: -6.4094, std: 3.8036, min: -13.7791, max: 5.6633
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9375, max: 3.1740
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9375, max: 3.1738
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL TELL YOU WHAT IT IS B EXCLAIMED MISSUS BOZZLE IT'S MY BELIEF AS HE AIN'T QUITE RIGHT UP HERE AND MISSUS BOZZLE TOUCHED HER FOREHEAD
Prediction: 000000000000000000000000000000000000000000000
Loss: 15.8136
outputs.loss tensor(15.8136, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143396/1673-143396-0006.flac
Waveform stats - mean: -0.0000, std: 0.1181, min: -0.8010, max: 0.8257
Resampled waveform stats - mean: -0.0000, std: 0.1181, min: -0.8010, max: 0.8257
Raw mel spectrogram stats - mean: 5.2239, std: 91.4881, min: 0.0000, max: 7406.9180
Log mel spectrogram stats - mean: -4.4308, std: 3.5467, min: -13.7457, max: 8.9102
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.6263, max: 3.7615
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.6270, max: 3.7617
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 51200
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 51200
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 358400
Inf count: 0
audio_emb.shape torch.Size([1, 100, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NOR COULD IT SEEM STRANGE OR INCREDIBLE THAT THE FIRST OF THESE AEONS THE LOGOS OR WORD OF GOD OF THE SAME SUBSTANCE WITH THE FATHER SHOULD DESCEND UPON EARTH TO DELIVER THE HUMAN RACE FROM VICE AND ERROR AND TO CONDUCT THEM IN THE PATHS OF LIFE AND IMMORTALITY
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2043
outputs.loss tensor(16.2043, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0029.flac
Waveform stats - mean: -0.0000, std: 0.0380, min: -0.2986, max: 0.3060
Resampled waveform stats - mean: -0.0000, std: 0.0380, min: -0.2986, max: 0.3060
Raw mel spectrogram stats - mean: 0.5383, std: 5.1853, min: 0.0000, max: 268.7053
Log mel spectrogram stats - mean: -6.1980, std: 3.4900, min: -13.2660, max: 5.5936
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0252, max: 3.3787
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0254, max: 3.3789
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE WOULD HAVE SOME ARRANGEMENT OF THIS THING
Prediction: 000000000000
Loss: 17.0651
outputs.loss tensor(17.0651, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0000.flac
Waveform stats - mean: 0.0000, std: 0.0343, min: -0.5701, max: 0.3462
Resampled waveform stats - mean: 0.0000, std: 0.0343, min: -0.5701, max: 0.3462
Raw mel spectrogram stats - mean: 0.4391, std: 3.1848, min: 0.0000, max: 156.1952
Log mel spectrogram stats - mean: -6.0194, std: 3.6784, min: -13.3990, max: 5.0511
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0062, max: 3.0096
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0059, max: 3.0098
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT IS LAUGHTER A SYNONYM OF JOY
Prediction: 0000000000000
Loss: 16.4836
outputs.loss tensor(16.4836, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/50561/174-50561-0018.flac
Waveform stats - mean: -0.0001, std: 0.0245, min: -0.2177, max: 0.2594
Resampled waveform stats - mean: -0.0001, std: 0.0245, min: -0.2177, max: 0.2594
Raw mel spectrogram stats - mean: 0.2230, std: 1.9387, min: 0.0000, max: 147.6365
Log mel spectrogram stats - mean: -8.5469, std: 4.4617, min: -13.7927, max: 4.9948
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.1757, max: 3.0351
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.1758, max: 3.0352
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BED TIME CHILDREN
Prediction: 0000
Loss: 17.7270
outputs.loss tensor(17.7270, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0002.flac
Waveform stats - mean: -0.0001, std: 0.0667, min: -0.4828, max: 0.5173
Resampled waveform stats - mean: -0.0001, std: 0.0667, min: -0.4828, max: 0.5173
Raw mel spectrogram stats - mean: 1.6712, std: 12.2240, min: 0.0000, max: 548.5284
Log mel spectrogram stats - mean: -5.5093, std: 4.0643, min: -13.7117, max: 6.3072
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0181, max: 2.9074
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0176, max: 2.9082
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS SILENT BECAUSE ALL SPEECH IS OF NECESSITY FEEBLE AND IMPERFECT THEREFORE THE SOULS OF MY ANCESTORS ASCENDED TO GOD IN WORDLESS ADORATION
Prediction: 0000000000000000000000000000000000000000000000
Loss: 16.0178
outputs.loss tensor(16.0178, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0029.flac
Waveform stats - mean: 0.0000, std: 0.0599, min: -0.4927, max: 0.4382
Resampled waveform stats - mean: 0.0000, std: 0.0599, min: -0.4927, max: 0.4382
Raw mel spectrogram stats - mean: 1.3398, std: 11.7433, min: 0.0000, max: 937.2130
Log mel spectrogram stats - mean: -6.8368, std: 5.1412, min: -13.8154, max: 6.8429
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.3574, max: 2.6608
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.3574, max: 2.6602
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT I COULD NOT BEAR THE THOUGHT OF WEARING DEAD MEN'S SHOES
Prediction: 000000000000000000000
Loss: 16.1211
outputs.loss tensor(16.1211, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0030.flac
Waveform stats - mean: -0.0000, std: 0.0466, min: -0.5010, max: 0.4959
Resampled waveform stats - mean: -0.0000, std: 0.0466, min: -0.5010, max: 0.4959
Raw mel spectrogram stats - mean: 0.8118, std: 11.3694, min: 0.0000, max: 602.8261
Log mel spectrogram stats - mean: -5.9399, std: 3.7086, min: -13.8076, max: 6.4016
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1215, max: 3.3278
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1211, max: 3.3281
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT'S THAT FOR DEMANDED NED WONDERINGLY
Prediction: 0000000000000
Loss: 16.2577
outputs.loss tensor(16.2577, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/137823/251-137823-0012.flac
Waveform stats - mean: -0.0000, std: 0.0636, min: -0.4066, max: 0.5438
Resampled waveform stats - mean: -0.0000, std: 0.0636, min: -0.4066, max: 0.5438
Raw mel spectrogram stats - mean: 1.5113, std: 9.5809, min: 0.0000, max: 329.1862
Log mel spectrogram stats - mean: -6.5023, std: 4.8929, min: -13.8124, max: 5.7966
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4940, max: 2.5136
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4941, max: 2.5137
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS FRIEND'S EYELIDS FLICKERED
Prediction: 000000000000
Loss: 16.3273
outputs.loss tensor(16.3273, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0027.flac
Waveform stats - mean: -0.0001, std: 0.0513, min: -0.8021, max: 0.4748
Resampled waveform stats - mean: -0.0001, std: 0.0513, min: -0.8021, max: 0.4748
Raw mel spectrogram stats - mean: 0.9846, std: 5.9958, min: 0.0000, max: 292.4465
Log mel spectrogram stats - mean: -4.8374, std: 3.6104, min: -13.8016, max: 5.6783
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4829, max: 2.9126
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4824, max: 2.9121
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 57344
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 57344
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 401408
Inf count: 0
audio_emb.shape torch.Size([1, 112, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE MEN APPEARED ROBUST BUT HEAVY FAIR HAIRED LIKE GERMANS BUT OF PENSIVE MIEN EXILES OF A HIGHER SCALE IN THE LADDER OF HUMANITY THAN THE ESKIMOS BUT I THOUGHT MUCH MORE UNHAPPY SINCE WITH SUPERIOR PERCEPTIONS THEY ARE COMPELLED TO LIVE WITHIN THE LIMITS OF THE POLAR CIRCLE
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.8893
outputs.loss tensor(15.8893, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0017.flac
Waveform stats - mean: -0.0000, std: 0.0452, min: -0.4393, max: 0.5070
Resampled waveform stats - mean: -0.0000, std: 0.0452, min: -0.4393, max: 0.5070
Raw mel spectrogram stats - mean: 0.7662, std: 7.5359, min: 0.0000, max: 476.4933
Log mel spectrogram stats - mean: -5.7492, std: 3.5959, min: -13.8120, max: 6.1665
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.3137
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2422, max: 3.3145
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: LUMPY BATES CAME RUNNING TOWARD HIM NOT DARING TO CALL OUT FOR FEAR OF WAKING THE CAMP
Prediction: 0000000000000000000000000000
Loss: 15.7542
outputs.loss tensor(15.7542, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/24640/5338-24640-0001.flac
Waveform stats - mean: -0.0000, std: 0.0466, min: -0.3828, max: 0.5609
Resampled waveform stats - mean: -0.0000, std: 0.0466, min: -0.3828, max: 0.5609
Raw mel spectrogram stats - mean: 0.8100, std: 7.2544, min: 0.0000, max: 497.3045
Log mel spectrogram stats - mean: -6.9410, std: 4.1210, min: -13.7824, max: 6.2092
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6601, max: 3.1910
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6602, max: 3.1914
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 36352
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 36352
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 254464
Inf count: 0
audio_emb.shape torch.Size([1, 71, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MISTER MORTON REPLIED THAT FAR FROM MAKING ANY CLAIM UPON HIS GOOD OPINION HIS ONLY WISH AND THE SOLE PURPOSE OF HIS VISIT WAS TO FIND OUT THE MEANS OF DESERVING IT
Prediction: 0000000000000000000000000000000000000000000000
Loss: 16.5414
outputs.loss tensor(16.5414, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0036.flac
Waveform stats - mean: -0.0001, std: 0.0435, min: -0.4622, max: 0.3531
Resampled waveform stats - mean: -0.0001, std: 0.0435, min: -0.4622, max: 0.3531
Raw mel spectrogram stats - mean: 0.7079, std: 5.8074, min: 0.0000, max: 220.2569
Log mel spectrogram stats - mean: -9.1443, std: 4.6227, min: -13.8077, max: 5.3948
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.0088, max: 3.1452
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.0088, max: 3.1445
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SOMEONE SNIGGERED
Prediction: 000000
Loss: 17.4932
outputs.loss tensor(17.4932, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34615/5895-34615-0020.flac
Waveform stats - mean: 0.0001, std: 0.0557, min: -0.6934, max: 0.5453
Resampled waveform stats - mean: 0.0001, std: 0.0557, min: -0.6934, max: 0.5453
Raw mel spectrogram stats - mean: 1.1618, std: 9.2575, min: 0.0000, max: 661.0150
Log mel spectrogram stats - mean: -5.3884, std: 3.8480, min: -13.7284, max: 6.4938
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1674, max: 3.0879
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1680, max: 3.0879
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30720
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30720
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 215040
Inf count: 0
audio_emb.shape torch.Size([1, 60, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ITS YELLOW BRISTLES RATHER A MANE THAN A HEAD OF HAIR COVERED AND CONCEALED A LOFTY BROW EVIDENTLY MADE TO CONTAIN THOUGHT
Prediction: 0000000000000000000000000000000000000000
Loss: 15.5895
outputs.loss tensor(15.5895, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0017.flac
Waveform stats - mean: -0.0001, std: 0.0439, min: -0.4574, max: 0.3349
Resampled waveform stats - mean: -0.0001, std: 0.0439, min: -0.4574, max: 0.3349
Raw mel spectrogram stats - mean: 0.7160, std: 5.1329, min: 0.0000, max: 180.6521
Log mel spectrogram stats - mean: -6.8547, std: 4.8066, min: -13.8155, max: 5.1966
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4482, max: 2.5072
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4482, max: 2.5078
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE OTHER LOOKED PALE AND ILL BUT HE WAS MARVELLOUSLY SELF CONTAINED AND IT WAS IMPOSSIBLE TO SAY WHAT WAS THE MATTER WITH HIM
Prediction: 000000000000000000000000000000000000
Loss: 16.3540
outputs.loss tensor(16.3540, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0005.flac
Waveform stats - mean: -0.0001, std: 0.0592, min: -0.4625, max: 0.6341
Resampled waveform stats - mean: -0.0001, std: 0.0592, min: -0.4625, max: 0.6341
Raw mel spectrogram stats - mean: 1.3107, std: 11.8119, min: 0.0000, max: 731.4451
Log mel spectrogram stats - mean: -5.9335, std: 3.5780, min: -13.7589, max: 6.5950
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1871, max: 3.5015
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1875, max: 3.5020
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY MADE ME THINK OF DEFEATED ARMIES RETREATING OR OF GHOSTS WHO WERE TRYING DESPERATELY TO GET IN FOR SHELTER AND THEN WENT MOANING ON
Prediction: 0000000000000000000000000000000000000000000
Loss: 15.9492
outputs.loss tensor(15.9492, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0011.flac
Waveform stats - mean: 0.0000, std: 0.0576, min: -0.3817, max: 0.3428
Resampled waveform stats - mean: 0.0000, std: 0.0576, min: -0.3817, max: 0.3428
Raw mel spectrogram stats - mean: 1.2420, std: 7.4609, min: 0.0000, max: 492.3312
Log mel spectrogram stats - mean: -5.5704, std: 4.5204, min: -13.8145, max: 6.1992
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8238, max: 2.6036
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8242, max: 2.6035
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I THINK WE MUST HAVE KILLED A GOOD MANY IN THE OLD FIELD BECAUSE WE WERE FIRING ALL THE TIME AT THE SOLID LINE AS THEY ADVANCED UPON US
Prediction: 0000000000000000000000000000000000000
Loss: 16.3667
outputs.loss tensor(16.3667, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0011.flac
Waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Resampled waveform stats - mean: -0.0000, std: 0.0652, min: -0.4684, max: 0.5253
Raw mel spectrogram stats - mean: 1.5914, std: 21.0549, min: 0.0000, max: 1490.1473
Log mel spectrogram stats - mean: -7.2657, std: 4.2950, min: -13.7301, max: 7.3066
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5051, max: 3.3928
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5049, max: 3.3926
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IS IT ONLY THAT YOU'RE POOR WHY THAT'S NOTHING I'M POOR TOO SHE LAUGHED
Prediction: 000000000000000000000
Loss: 16.6380
outputs.loss tensor(16.6380, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0028.flac
Waveform stats - mean: 0.0000, std: 0.0664, min: -0.7006, max: 0.4942
Resampled waveform stats - mean: 0.0000, std: 0.0664, min: -0.7006, max: 0.4942
Raw mel spectrogram stats - mean: 1.6459, std: 23.8564, min: 0.0000, max: 1400.7875
Log mel spectrogram stats - mean: -5.2052, std: 3.7089, min: -13.7578, max: 7.2448
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3060, max: 3.3568
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3066, max: 3.3574
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS MUCH MORE DESIRABLE TO BE A PRIVATE CITIZEN HAPPY AND CARE FREE
Prediction: 0000000000000000000
Loss: 16.2560
outputs.loss tensor(16.2560, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0018.flac
Waveform stats - mean: -0.0000, std: 0.0652, min: -0.3950, max: 0.2966
Resampled waveform stats - mean: -0.0000, std: 0.0652, min: -0.3950, max: 0.2966
Raw mel spectrogram stats - mean: 1.5929, std: 13.3930, min: 0.0000, max: 782.0540
Log mel spectrogram stats - mean: -6.0404, std: 4.1364, min: -13.7740, max: 6.6619
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8696, max: 3.0708
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8701, max: 3.0703
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE MISTRESS HAD RETURNED AND THE YOUNG LADY WAS WITH HER AND HURRIED AT ONCE TO HER FAVOURITE GARDEN
Prediction: 000000000000000000000000000000000000
Loss: 15.1843
outputs.loss tensor(15.1843, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0058.flac
Waveform stats - mean: -0.0000, std: 0.1271, min: -0.8773, max: 0.7233
Resampled waveform stats - mean: -0.0000, std: 0.1271, min: -0.8773, max: 0.7233
Raw mel spectrogram stats - mean: 6.0262, std: 37.2706, min: 0.0000, max: 821.0695
Log mel spectrogram stats - mean: -5.6701, std: 4.6104, min: -13.6953, max: 6.7106
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7407, max: 2.6854
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7402, max: 2.6855
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 46592
Inf count: 0
audio_emb.shape torch.Size([1, 13, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I'LL REPORT THIS TO THE GOVERNMENT
Prediction: 0000000000
Loss: 15.6604
outputs.loss tensor(15.6604, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/161169/2803-161169-0012.flac
Waveform stats - mean: -0.0000, std: 0.0259, min: -0.2217, max: 0.1887
Resampled waveform stats - mean: -0.0000, std: 0.0259, min: -0.2217, max: 0.1887
Raw mel spectrogram stats - mean: 0.2498, std: 1.3980, min: 0.0000, max: 59.2596
Log mel spectrogram stats - mean: -7.3871, std: 3.8932, min: -13.7786, max: 4.0819
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6417, max: 2.9459
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6416, max: 2.9453
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 50688
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 50688
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 354816
Inf count: 0
audio_emb.shape torch.Size([1, 99, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THESE FORESTS WERE OF TREES DIFFERENT IN SOME WAYS FROM THOSE WE HAVE NOW GREAT FERNS AS TALL AS THIS HOUSE AND MOSSES AS HIGH AS LITTLE TREES AND PALM LEAVES OF ENORMOUS SIZE
Prediction: 00000000000000000000000000000000000000000000000000000
Loss: 15.9337
outputs.loss tensor(15.9337, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/23268/3536-23268-0018.flac
Waveform stats - mean: -0.0001, std: 0.0558, min: -0.8885, max: 0.7786
Resampled waveform stats - mean: -0.0001, std: 0.0558, min: -0.8885, max: 0.7786
Raw mel spectrogram stats - mean: 1.1620, std: 12.8595, min: 0.0000, max: 859.7589
Log mel spectrogram stats - mean: -5.2171, std: 3.6074, min: -13.6973, max: 6.7567
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3508, max: 3.3192
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3516, max: 3.3184
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24576
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24576
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 172032
Inf count: 0
audio_emb.shape torch.Size([1, 48, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MISS WOODLEY THOUGHT IT HER DUTY TO BE MUTE AND NOW THE GINGLE OF A TEA SPOON WAS LIKE A DEEP TONED BELL ALL WAS SO QUIET
Prediction: 0000000000000000000000000000000000000000000
Loss: 15.3657
outputs.loss tensor(15.3657, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/154320/2803-154320-0007.flac
Waveform stats - mean: 0.0000, std: 0.0725, min: -0.6896, max: 0.6681
Resampled waveform stats - mean: 0.0000, std: 0.0725, min: -0.6896, max: 0.6681
Raw mel spectrogram stats - mean: 1.9513, std: 13.8554, min: 0.0000, max: 686.0099
Log mel spectrogram stats - mean: -5.8603, std: 4.0741, min: -13.5986, max: 6.5309
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8994, max: 3.0415
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8994, max: 3.0410
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GOD KEEP US FROM SUCH A MEETING WHY JOHN
Prediction: 00000000000
Loss: 16.5595
outputs.loss tensor(16.5595, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0008.flac
Waveform stats - mean: 0.0001, std: 0.1028, min: -0.5422, max: 0.5703
Resampled waveform stats - mean: 0.0001, std: 0.1028, min: -0.5422, max: 0.5703
Raw mel spectrogram stats - mean: 3.7942, std: 33.3297, min: 0.0000, max: 1770.4165
Log mel spectrogram stats - mean: -4.7707, std: 3.6029, min: -13.1853, max: 7.4790
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3355, max: 3.3999
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3359, max: 3.4004
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: DOST THOU RISE AGAINST HIM WHO GIVES THEE HIS BREAD
Prediction: 00000000000000000
Loss: 16.1308
outputs.loss tensor(16.1308, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0019.flac
Waveform stats - mean: -0.0000, std: 0.0601, min: -0.3260, max: 0.3352
Resampled waveform stats - mean: -0.0000, std: 0.0601, min: -0.3260, max: 0.3352
Raw mel spectrogram stats - mean: 1.3527, std: 9.9502, min: 0.0000, max: 445.9651
Log mel spectrogram stats - mean: -5.9461, std: 4.1378, min: -13.8155, max: 6.1002
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9018, max: 2.9113
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9014, max: 2.9121
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN REAR OF ALL THE GROUP HERE TREATED OF TWO OLD MEN I BEHELD UNLIKE IN HABIT BUT LIKE IN GAIT EACH DIGNIFIED AND GRAVE
Prediction: 000000000000000000000000000000000000
Loss: 15.8584
outputs.loss tensor(15.8584, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93302/6345-93302-0019.flac
Waveform stats - mean: 0.0000, std: 0.0727, min: -0.4382, max: 0.4741
Resampled waveform stats - mean: 0.0000, std: 0.0727, min: -0.4382, max: 0.4741
Raw mel spectrogram stats - mean: 1.9788, std: 25.2769, min: 0.0000, max: 1401.5687
Log mel spectrogram stats - mean: -7.8325, std: 4.5003, min: -13.8104, max: 7.2453
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3283, max: 3.3504
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.3281, max: 3.3496
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THOSE FOUR TRUE WORDS WOUNDED HER MORE THAN ALL THE REST
Prediction: 000000000000000
Loss: 16.2023
outputs.loss tensor(16.2023, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130737/652-130737-0010.flac
Waveform stats - mean: -0.0001, std: 0.0566, min: -0.5212, max: 0.3912
Resampled waveform stats - mean: -0.0001, std: 0.0566, min: -0.5212, max: 0.3912
Raw mel spectrogram stats - mean: 1.1542, std: 5.8758, min: 0.0000, max: 251.9846
Log mel spectrogram stats - mean: -5.4684, std: 4.2484, min: -13.7620, max: 5.5294
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9522, max: 2.5887
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9521, max: 2.5879
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SAUTERNE IS A WHITE BORDEAUX A STRONG LUSCIOUS WINE THE BEST KNOWN VARIETIES BEING
Prediction: 0000000000000000000000000000000
Loss: 15.5753
outputs.loss tensor(15.5753, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/64257/6345-64257-0001.flac
Waveform stats - mean: -0.0000, std: 0.0638, min: -0.3972, max: 0.5468
Resampled waveform stats - mean: -0.0000, std: 0.0638, min: -0.3972, max: 0.5468
Raw mel spectrogram stats - mean: 1.5237, std: 14.7153, min: 0.0000, max: 835.1009
Log mel spectrogram stats - mean: -4.9011, std: 3.4266, min: -13.8078, max: 6.7276
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5993, max: 3.3937
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5996, max: 3.3945
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 70656
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 70656
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 494592
Inf count: 0
audio_emb.shape torch.Size([1, 138, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AT THE TIME MARY HAD NOTED NOTHING OF THESE THINGS NOW SHE SAW THEM ALL AS FOR THE FIRST TIME IN MINUTE DETAIL WHILE SLOWLY SHE WENT UP THE STAIR AND THROUGH THE NARROWED WAYS AND HEARD THE SAME WIND THAT RAVED ALIKE ABOUT THE NEW GRAVE AND THE OLD HOUSE INTO WHICH LATTER FOR ALL THE BALES BANKED AGAINST THE WALLS IT FOUND MANY A CHINK OF ENTRANCE
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.9930
outputs.loss tensor(15.9930, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3576/138058/3576-138058-0020.flac
Waveform stats - mean: 0.0000, std: 0.0896, min: -0.6111, max: 0.5883
Resampled waveform stats - mean: 0.0000, std: 0.0896, min: -0.6111, max: 0.5883
Raw mel spectrogram stats - mean: 2.9911, std: 25.3096, min: 0.0000, max: 1578.1338
Log mel spectrogram stats - mean: -4.5953, std: 3.6415, min: -13.2679, max: 7.3640
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3816, max: 3.2841
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3809, max: 3.2832
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 78848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 78848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 551936
Inf count: 0
audio_emb.shape torch.Size([1, 154, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NOBODY NEED HAVE ANY DOUBT ABOUT THAT SAID SANCHO FOR MY MASTER HAS A VERY HAPPY KNACK OF MATCHMAKING IT'S NOT MANY DAYS SINCE HE FORCED ANOTHER MAN TO MARRY WHO IN THE SAME WAY BACKED OUT OF HIS PROMISE TO ANOTHER MAIDEN AND IF IT HAD NOT BEEN FOR HIS PERSECUTORS THE ENCHANTERS CHANGING THE MAN'S PROPER SHAPE INTO A LACQUEY'S THE SAID MAIDEN WOULD NOT BE ONE THIS MINUTE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.0305
outputs.loss tensor(16.0305, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0024.flac
Waveform stats - mean: -0.0000, std: 0.0674, min: -0.3807, max: 0.4552
Resampled waveform stats - mean: -0.0000, std: 0.0674, min: -0.3807, max: 0.4552
Raw mel spectrogram stats - mean: 1.7029, std: 12.1382, min: 0.0000, max: 569.6877
Log mel spectrogram stats - mean: -6.1805, std: 4.4875, min: -13.8155, max: 6.3451
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7014, max: 2.7912
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7012, max: 2.7910
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27648
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27648
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 193536
Inf count: 0
audio_emb.shape torch.Size([1, 54, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TO SAY UNTO VIRGILIUS NOT A DRACHM OF BLOOD REMAINS IN ME THAT DOES NOT TREMBLE I KNOW THE TRACES OF THE ANCIENT FLAME
Prediction: 0000000000000000000000000000000000000
Loss: 15.8075
outputs.loss tensor(15.8075, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0004.flac
Waveform stats - mean: -0.0000, std: 0.0577, min: -0.2461, max: 0.3859
Resampled waveform stats - mean: -0.0000, std: 0.0577, min: -0.2461, max: 0.3859
Raw mel spectrogram stats - mean: 1.2311, std: 6.4187, min: 0.0000, max: 222.1449
Log mel spectrogram stats - mean: -5.4951, std: 4.0133, min: -13.6637, max: 5.4033
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0354, max: 2.7156
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0352, max: 2.7148
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN DUE TIME THE CHILD TAKES OF HIS OWN ACCORD THE ATTITUDE OF PRAYER AND SPEAKS REVERENTLY OF THE POWERS
Prediction: 00000000000000000000000000000000
Loss: 16.3149
outputs.loss tensor(16.3149, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170138/1462-170138-0000.flac
Waveform stats - mean: -0.0006, std: 0.0668, min: -0.3298, max: 0.4713
Resampled waveform stats - mean: -0.0006, std: 0.0668, min: -0.3298, max: 0.4713
Raw mel spectrogram stats - mean: 1.6723, std: 14.0675, min: 0.0000, max: 698.3568
Log mel spectrogram stats - mean: -6.3973, std: 4.0614, min: -13.7959, max: 6.5487
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8217, max: 3.1876
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8213, max: 3.1875
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 46592
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 46592
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 326144
Inf count: 0
audio_emb.shape torch.Size([1, 91, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE HAD WRITTEN A NUMBER OF BOOKS HIMSELF AMONG THEM A HISTORY OF DANCING A HISTORY OF COSTUME A KEY TO SHAKESPEARE'S SONNETS A STUDY OF THE POETRY OF ERNEST DOWSON ET CETERA
Prediction: 0000000000000000000000000000000000000000000000000000000000
Loss: 15.5649
outputs.loss tensor(15.5649, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0011.flac
Waveform stats - mean: -0.0007, std: 0.0945, min: -0.4991, max: 0.6645
Resampled waveform stats - mean: -0.0007, std: 0.0945, min: -0.4991, max: 0.6645
Raw mel spectrogram stats - mean: 3.3414, std: 27.2227, min: 0.0000, max: 1130.5690
Log mel spectrogram stats - mean: -6.5190, std: 4.4115, min: -13.7331, max: 7.0305
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6353, max: 3.0714
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6357, max: 3.0723
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE PULLED UP A WINDOW AS IF THE AIR WERE HEAVY
Prediction: 0000000000000000
Loss: 15.3647
outputs.loss tensor(15.3647, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0012.flac
Waveform stats - mean: -0.0001, std: 0.0568, min: -0.7290, max: 0.4854
Resampled waveform stats - mean: -0.0001, std: 0.0568, min: -0.7290, max: 0.4854
Raw mel spectrogram stats - mean: 1.2041, std: 8.5341, min: 0.0000, max: 685.1193
Log mel spectrogram stats - mean: -4.5301, std: 3.5050, min: -13.3602, max: 6.5296
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5192, max: 3.1554
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5195, max: 3.1562
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE FACT WAS THAT SCARCELY ANY ONE OF THEM BUT EXPECTED SOME GOODS BY THE PERIODICAL VESSEL
Prediction: 0000000000000000000000000
Loss: 16.3449
outputs.loss tensor(16.3449, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0019.flac
Waveform stats - mean: 0.0000, std: 0.0627, min: -0.4637, max: 0.6871
Resampled waveform stats - mean: 0.0000, std: 0.0627, min: -0.4637, max: 0.6871
Raw mel spectrogram stats - mean: 1.4740, std: 10.0717, min: 0.0000, max: 489.8757
Log mel spectrogram stats - mean: -5.7431, std: 4.5183, min: -13.8132, max: 6.1942
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7861, max: 2.6420
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7861, max: 2.6426
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28672
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28672
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 200704
Inf count: 0
audio_emb.shape torch.Size([1, 56, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I FREQUENTLY THOUGHT IT WOULD BE PLEASANT TO SPLIT THE DIFFERENCE WITH THAT MULE AND I WOULD GLADLY HAVE DONE SO IF I COULD HAVE GOTTEN ONE HALF OF HIS NO
Prediction: 0000000000000000000000000000000000000000000000000
Loss: 16.2538
outputs.loss tensor(16.2538, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43363/5536-43363-0009.flac
Waveform stats - mean: -0.0001, std: 0.0690, min: -0.3275, max: 0.5174
Resampled waveform stats - mean: -0.0001, std: 0.0690, min: -0.3275, max: 0.5174
Raw mel spectrogram stats - mean: 1.7814, std: 8.8676, min: 0.0000, max: 348.5649
Log mel spectrogram stats - mean: -5.1476, std: 3.9310, min: -13.6674, max: 5.8538
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1673, max: 2.7986
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1680, max: 2.7988
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 67072
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 67072
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 469504
Inf count: 0
audio_emb.shape torch.Size([1, 131, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS WELL KNOWN THAT THE AMERICAN INDIAN HAD SOMEHOW DEVELOPED OCCULT POWER AND ALTHOUGH IN THE LATTER DAYS THERE HAVE BEEN MANY IMPOSTORS AND ALLOWING FOR THE VANITY AND WEAKNESS OF HUMAN NATURE IT IS FAIR TO ASSUME THAT THERE MUST HAVE BEEN SOME EVEN IN THE OLD DAYS YET THERE ARE WELL ATTESTED INSTANCES OF REMARKABLE PROPHECIES AND OTHER MYSTIC PRACTICE
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.5100
outputs.loss tensor(16.5100, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0029.flac
Waveform stats - mean: -0.0000, std: 0.0616, min: -0.3692, max: 0.2235
Resampled waveform stats - mean: -0.0000, std: 0.0616, min: -0.3692, max: 0.2235
Raw mel spectrogram stats - mean: 1.4134, std: 7.5193, min: 0.0000, max: 270.7735
Log mel spectrogram stats - mean: -4.2102, std: 3.6494, min: -13.6147, max: 5.6013
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5770, max: 2.6885
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5762, max: 2.6895
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ONLY NAME IT WHATEVER I HAVE I OFFER THEE
Prediction: 0000000000000
Loss: 15.1917
outputs.loss tensor(15.1917, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/168635/174-168635-0020.flac
Waveform stats - mean: -0.0001, std: 0.0459, min: -0.5032, max: 0.4770
Resampled waveform stats - mean: -0.0001, std: 0.0459, min: -0.5032, max: 0.4770
Raw mel spectrogram stats - mean: 0.7860, std: 5.5449, min: 0.0000, max: 217.6672
Log mel spectrogram stats - mean: -6.0164, std: 4.0192, min: -13.7740, max: 5.3830
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9301, max: 2.8362
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9297, max: 2.8359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALAS HE WALKED WITH NO LESS INDECISION THAN COSETTE
Prediction: 0000000000000000
Loss: 15.5953
outputs.loss tensor(15.5953, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0013.flac
Waveform stats - mean: 0.0000, std: 0.0648, min: -0.5305, max: 0.5069
Resampled waveform stats - mean: 0.0000, std: 0.0648, min: -0.5305, max: 0.5069
Raw mel spectrogram stats - mean: 1.5711, std: 10.1818, min: 0.0000, max: 429.6413
Log mel spectrogram stats - mean: -4.6563, std: 3.4615, min: -13.5214, max: 6.0630
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5610, max: 3.0967
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5605, max: 3.0977
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BEAT THE EGGS STIR TO THEM THE MILK AND POUNDED SUGAR AND PUT THE MIXTURE INTO A JUG
Prediction: 0000000000000000000000000000
Loss: 15.6693
outputs.loss tensor(15.6693, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0003.flac
Waveform stats - mean: -0.0000, std: 0.0681, min: -0.4852, max: 0.4357
Resampled waveform stats - mean: -0.0000, std: 0.0681, min: -0.4852, max: 0.4357
Raw mel spectrogram stats - mean: 1.7354, std: 16.6854, min: 0.0000, max: 926.0620
Log mel spectrogram stats - mean: -6.1736, std: 4.3442, min: -13.8134, max: 6.8309
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7586, max: 2.9936
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7588, max: 2.9941
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE WERE NO BREASTWORKS YET THAT ONE LITTLE BRIGADE OF HAMILTON'S DIVISION STOOD THERE IN THE OPEN AND REPULSED ASSAULT AFTER ASSAULT
Prediction: 000000000000000000000000000000000000000000
Loss: 16.2815
outputs.loss tensor(16.2815, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0025.flac
Waveform stats - mean: 0.0000, std: 0.1032, min: -0.7505, max: 0.5193
Resampled waveform stats - mean: 0.0000, std: 0.1032, min: -0.7505, max: 0.5193
Raw mel spectrogram stats - mean: 3.9876, std: 22.9546, min: 0.0000, max: 697.2282
Log mel spectrogram stats - mean: -5.3797, std: 4.4269, min: -13.6661, max: 6.5471
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8718, max: 2.6942
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8721, max: 2.6934
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: KIRKLAND JUMPED FOR THE JETTY MISSED HIS FOOTING AND FELL INTO THE ARMS OF THE CHAPLAIN
Prediction: 000000000000000000000000000000
Loss: 15.8549
outputs.loss tensor(15.8549, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143396/1673-143396-0015.flac
Waveform stats - mean: -0.0000, std: 0.1167, min: -0.8024, max: 0.8139
Resampled waveform stats - mean: -0.0000, std: 0.1167, min: -0.8024, max: 0.8139
Raw mel spectrogram stats - mean: 5.1040, std: 41.4718, min: 0.0000, max: 2205.9006
Log mel spectrogram stats - mean: -3.7416, std: 3.6466, min: -13.3949, max: 7.6989
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.6472, max: 3.1373
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.6465, max: 3.1367
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEIR MURMURS WERE VARIOUSLY SILENCED BY THE SECTARIES WHO ESPOUSED AND MODIFIED THE DOUBLE SYSTEM OF CERINTHUS
Prediction: 000000000000000000000000000000000000000
Loss: 15.7239
outputs.loss tensor(15.7239, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130737/652-130737-0009.flac
Waveform stats - mean: -0.0000, std: 0.0695, min: -0.5328, max: 0.5137
Resampled waveform stats - mean: -0.0000, std: 0.0695, min: -0.5328, max: 0.5137
Raw mel spectrogram stats - mean: 1.6462, std: 10.3015, min: 0.0000, max: 568.9404
Log mel spectrogram stats - mean: -5.4665, std: 4.2803, min: -13.7181, max: 6.3438
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9278, max: 2.7592
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9277, max: 2.7598
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: LACRIMA CHRISTI A STILL WINE OF EXCELLENT FLAVOR AND BOUQUET
Prediction: 0000000000000000000000
Loss: 15.5989
outputs.loss tensor(15.5989, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0028.flac
Waveform stats - mean: -0.0001, std: 0.0659, min: -0.4853, max: 0.6066
Resampled waveform stats - mean: -0.0001, std: 0.0659, min: -0.4853, max: 0.6066
Raw mel spectrogram stats - mean: 1.6203, std: 9.5403, min: 0.0000, max: 380.5437
Log mel spectrogram stats - mean: -4.9568, std: 3.8216, min: -13.6223, max: 5.9416
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2675, max: 2.8518
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2676, max: 2.8516
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 36864
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 36864
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 258048
Inf count: 0
audio_emb.shape torch.Size([1, 72, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MOVE IT BACKWARDS AND FORWARDS UNTIL THE BREAD IS NICELY COLOURED THEN TURN IT AND TOAST THE OTHER SIDE AND DO NOT PLACE IT SO NEAR THE FIRE THAT IT BLACKENS
Prediction: 000000000000000000000000000000000000000000
Loss: 16.0266
outputs.loss tensor(16.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121123/84-121123-0014.flac
Waveform stats - mean: -0.0000, std: 0.0692, min: -0.3936, max: 0.3925
Resampled waveform stats - mean: -0.0000, std: 0.0692, min: -0.3936, max: 0.3925
Raw mel spectrogram stats - mean: 1.7890, std: 14.0935, min: 0.0000, max: 451.8656
Log mel spectrogram stats - mean: -6.7064, std: 3.6293, min: -12.8705, max: 6.1134
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6984, max: 3.5323
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6982, max: 3.5332
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MUST I LEAVE ALONE NO
Prediction: 00000000
Loss: 15.9136
outputs.loss tensor(15.9136, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147960/2035-147960-0016.flac
Waveform stats - mean: -0.0001, std: 0.0832, min: -0.5559, max: 0.6703
Resampled waveform stats - mean: -0.0001, std: 0.0832, min: -0.5559, max: 0.6703
Raw mel spectrogram stats - mean: 2.5958, std: 20.8004, min: 0.0000, max: 988.7060
Log mel spectrogram stats - mean: -4.8081, std: 3.6756, min: -13.3064, max: 6.8964
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3121, max: 3.1844
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3125, max: 3.1836
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A SNAKE OF HIS SIZE IN FIGHTING TRIM WOULD BE MORE THAN ANY BOY COULD HANDLE
Prediction: 000000000000000000000000
Loss: 16.2439
outputs.loss tensor(16.2439, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147964/1993-147964-0008.flac
Waveform stats - mean: -0.0001, std: 0.0686, min: -0.4978, max: 0.6435
Resampled waveform stats - mean: -0.0001, std: 0.0686, min: -0.4978, max: 0.6435
Raw mel spectrogram stats - mean: 1.7584, std: 9.6982, min: 0.0000, max: 310.9995
Log mel spectrogram stats - mean: -4.3741, std: 3.1756, min: -13.0242, max: 5.7398
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.7239, max: 3.1849
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.7246, max: 3.1855
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23040
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23040
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 161280
Inf count: 0
audio_emb.shape torch.Size([1, 45, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BY THE TIME WE HAD PLACED THE COLD FRESH SMELLING LITTLE TREE IN A CORNER OF THE SITTING ROOM IT WAS ALREADY CHRISTMAS EVE
Prediction: 0000000000000000000000000000000000000
Loss: 16.2527
outputs.loss tensor(16.2527, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0020.flac
Waveform stats - mean: 0.0009, std: 0.1865, min: -0.6791, max: 0.6980
Resampled waveform stats - mean: 0.0009, std: 0.1865, min: -0.6791, max: 0.6980
Raw mel spectrogram stats - mean: 13.0213, std: 94.9092, min: 0.0000, max: 3172.4531
Log mel spectrogram stats - mean: -2.6004, std: 3.3066, min: -12.0872, max: 8.0623
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.8690, max: 3.2246
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.8691, max: 3.2246
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE'S A KIND NEIGHBORLY MAN AND HIS BOY WILL TAKE MY PLACE ABOUT THE HOUSE AND PROTECT YOU FAITHFULLY
Prediction: 0000000000000000000000000000
Loss: 16.5630
outputs.loss tensor(16.5630, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0041.flac
Waveform stats - mean: -0.0000, std: 0.0698, min: -0.4329, max: 0.3983
Resampled waveform stats - mean: -0.0000, std: 0.0698, min: -0.4329, max: 0.3983
Raw mel spectrogram stats - mean: 1.8265, std: 15.7946, min: 0.0000, max: 1058.7787
Log mel spectrogram stats - mean: -6.1233, std: 4.3620, min: -13.8079, max: 6.9649
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7617, max: 3.0005
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7617, max: 3.0000
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY WERE NOT ONLY GOOD CHRISTIANS AND FAITHFUL TO THE CHURCH BUT EVEN REAL DEVOTEES AND FULL OF SCRUPLES
Prediction: 0000000000000000000000000000000
Loss: 16.1761
outputs.loss tensor(16.1761, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93302/6345-93302-0022.flac
Waveform stats - mean: -0.0000, std: 0.0957, min: -0.5679, max: 0.5691
Resampled waveform stats - mean: -0.0000, std: 0.0957, min: -0.5679, max: 0.5691
Raw mel spectrogram stats - mean: 3.4186, std: 51.0399, min: 0.0000, max: 2656.9058
Log mel spectrogram stats - mean: -7.2757, std: 4.5506, min: -13.8113, max: 7.8849
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4362, max: 3.3316
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4365, max: 3.3320
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NO IT'S ONLY PAINFUL FOR BOTH OF US
Prediction: 00000000000
Loss: 16.1949
outputs.loss tensor(16.1949, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0028.flac
Waveform stats - mean: 0.0000, std: 0.0400, min: -0.1944, max: 0.3310
Resampled waveform stats - mean: 0.0000, std: 0.0400, min: -0.1944, max: 0.3310
Raw mel spectrogram stats - mean: 0.5979, std: 3.5698, min: 0.0000, max: 144.6097
Log mel spectrogram stats - mean: -7.5632, std: 4.7273, min: -13.8072, max: 4.9740
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3208, max: 2.6521
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.3213, max: 2.6523
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: PHOEBE MERELY GLANCED AT IT AND GAVE IT BACK
Prediction: 000000000000000
Loss: 15.7091
outputs.loss tensor(15.7091, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1993/147149/1993-147149-0030.flac
Waveform stats - mean: -0.0000, std: 0.0060, min: -0.0731, max: 0.0474
Resampled waveform stats - mean: -0.0000, std: 0.0060, min: -0.0731, max: 0.0474
Raw mel spectrogram stats - mean: 0.0133, std: 0.1197, min: 0.0000, max: 14.6270
Log mel spectrogram stats - mean: -9.2665, std: 2.9205, min: -13.7995, max: 2.6829
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5521, max: 4.0916
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5518, max: 4.0898
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 50176
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 50176
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 351232
Inf count: 0
audio_emb.shape torch.Size([1, 98, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT THE BEST OF HER PLANS THE HOLIEST THAT WHICH IN SOME MEASURE REDEEMED THE VANITY OF THE REST WERE THOSE RELATING TO HER FATHER HER DEAR FATHER NOW OPPRESSED WITH CARE AND ALWAYS A DISHEARTENED GLOOMY PERSON
Prediction: 0000000000000000000000000000000000000000000000000000000000000
Loss: 16.1771
outputs.loss tensor(16.1771, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0008.flac
Waveform stats - mean: 0.0000, std: 0.0842, min: -0.6198, max: 0.5801
Resampled waveform stats - mean: 0.0000, std: 0.0842, min: -0.6198, max: 0.5801
Raw mel spectrogram stats - mean: 2.6496, std: 16.3115, min: 0.0000, max: 887.4434
Log mel spectrogram stats - mean: -4.3711, std: 3.5791, min: -13.6118, max: 6.7883
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5819, max: 3.1180
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5820, max: 3.1172
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 65024
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 65024
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 455168
Inf count: 0
audio_emb.shape torch.Size([1, 127, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: PUT THE SUGAR WITH ONE QUARTER PINT OF WATER IN A SAUCEPAN OVER THE FIRE REMOVE THE SCUM AS IT RISES AND ADD THE LEMON PEEL AND GINGER WITH THE OUTSIDE SCRAPED OFF WHEN THE SYRUP IS TOLERABLY THICK TAKE IT OFF THE FIRE AND WHEN COLD WIPE THE CUCUMBERS DRY AND PUT THEM IN
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.6736
outputs.loss tensor(15.6736, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110124/7976-110124-0022.flac
Waveform stats - mean: -0.0000, std: 0.0515, min: -0.3674, max: 0.2480
Resampled waveform stats - mean: -0.0000, std: 0.0515, min: -0.3674, max: 0.2480
Raw mel spectrogram stats - mean: 0.9923, std: 7.1526, min: 0.0000, max: 266.5799
Log mel spectrogram stats - mean: -6.5258, std: 4.1317, min: -13.8027, max: 5.5857
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7612, max: 2.9313
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7607, max: 2.9316
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHEN SHE RETURNED HIS HAND WAS STICKING THROUGH THE HOLE IN THE DOOR
Prediction: 0000000000000000000
Loss: 16.1783
outputs.loss tensor(16.1783, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0012.flac
Waveform stats - mean: -0.0001, std: 0.0768, min: -0.4102, max: 0.4636
Resampled waveform stats - mean: -0.0001, std: 0.0768, min: -0.4102, max: 0.4636
Raw mel spectrogram stats - mean: 2.2007, std: 12.4068, min: 0.0000, max: 422.7634
Log mel spectrogram stats - mean: -4.6615, std: 3.8155, min: -13.5092, max: 6.0468
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3189, max: 2.8066
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3184, max: 2.8066
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34304
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34304
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 240128
Inf count: 0
audio_emb.shape torch.Size([1, 67, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE CHILDREN PROVED THEMSELVES UNUSUALLY BRIGHT PUPILS AND BY THE TIME WABI WAS SIXTEEN AND MINNETAKI TWELVE ONE WOULD NOT HAVE KNOWN FROM THEIR MANNER OF SPEECH THAT INDIAN BLOOD RAN IN THEIR VEINS
Prediction: 00000000000000000000000000000000000000000000000000000000000000
Loss: 16.1904
outputs.loss tensor(16.1904, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130737/652-130737-0001.flac
Waveform stats - mean: -0.0001, std: 0.0622, min: -0.5427, max: 0.4283
Resampled waveform stats - mean: -0.0001, std: 0.0622, min: -0.5427, max: 0.4283
Raw mel spectrogram stats - mean: 1.3972, std: 6.7084, min: 0.0000, max: 237.2135
Log mel spectrogram stats - mean: -5.5860, std: 4.4702, min: -13.7860, max: 5.4690
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8343, max: 2.4730
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8340, max: 2.4727
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WITH SOUP AND FISH SERVE WHITE WINES SUCH AS RHEIN WINE SAUTERNE OR WHITE BURGUNDY
Prediction: 00000000000000000000000000000
Loss: 15.4600
outputs.loss tensor(15.4600, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0014.flac
Waveform stats - mean: 0.0001, std: 0.0834, min: -0.3716, max: 0.4363
Resampled waveform stats - mean: 0.0001, std: 0.0834, min: -0.3716, max: 0.4363
Raw mel spectrogram stats - mean: 2.6040, std: 12.9097, min: 0.0000, max: 287.7599
Log mel spectrogram stats - mean: -5.7455, std: 4.7401, min: -13.8088, max: 5.6621
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7011, max: 2.4066
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7012, max: 2.4062
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ON THE DAIS UNDER THE GOLDEN DOME THE KING CRIED OUT AGAIN RACKED BY AWFUL PAROXYSMS
Prediction: 0000000000000000000000000000
Loss: 15.6261
outputs.loss tensor(15.6261, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0014.flac
Waveform stats - mean: 0.0000, std: 0.0424, min: -0.2786, max: 0.2865
Resampled waveform stats - mean: 0.0000, std: 0.0424, min: -0.2786, max: 0.2865
Raw mel spectrogram stats - mean: 0.6722, std: 4.3229, min: 0.0000, max: 102.1499
Log mel spectrogram stats - mean: -7.7392, std: 4.6910, min: -13.8128, max: 4.6264
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.2947, max: 2.6360
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.2949, max: 2.6367
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7168
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7168
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 50176
Inf count: 0
audio_emb.shape torch.Size([1, 14, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS A HORRIBLE JOURNEY
Prediction: 000000000
Loss: 16.3706
outputs.loss tensor(16.3706, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0005.flac
Waveform stats - mean: -0.0000, std: 0.0382, min: -0.4413, max: 0.4092
Resampled waveform stats - mean: -0.0000, std: 0.0382, min: -0.4413, max: 0.4092
Raw mel spectrogram stats - mean: 0.5456, std: 2.5479, min: 0.0000, max: 91.8729
Log mel spectrogram stats - mean: -5.9119, std: 4.0216, min: -13.6796, max: 4.5204
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9315, max: 2.5941
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9316, max: 2.5938
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29184
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29184
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 204288
Inf count: 0
audio_emb.shape torch.Size([1, 57, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE WAS NOTHING SAID ABOUT THE SORT OF ACCOMMODATION WHICH WOULD BE PROVIDED NOTHING ABOUT THE KIND OF ESTABLISHMENT WHICH WAS MAINTAINED OR THE TABLE WHICH WAS KEPT
Prediction: 0000000000000000000000000000000000000000
Loss: 16.9806
outputs.loss tensor(16.9806, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0031.flac
Waveform stats - mean: 0.0001, std: 0.0973, min: -0.9912, max: 0.5987
Resampled waveform stats - mean: 0.0001, std: 0.0973, min: -0.9912, max: 0.5987
Raw mel spectrogram stats - mean: 3.2466, std: 36.9729, min: 0.0000, max: 4773.4907
Log mel spectrogram stats - mean: -4.9141, std: 4.7638, min: -13.8095, max: 8.4708
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8673, max: 2.8097
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8672, max: 2.8105
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE AIR TOO WAS UNLIKE THAT OF SOUTH CAROLINA THERE WAS A SHARPER TANG TO IT
Prediction: 0000000000000000000000
Loss: 16.2036
outputs.loss tensor(16.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0013.flac
Waveform stats - mean: -0.0000, std: 0.0562, min: -0.4262, max: 0.4122
Resampled waveform stats - mean: -0.0000, std: 0.0562, min: -0.4262, max: 0.4122
Raw mel spectrogram stats - mean: 1.1788, std: 5.4252, min: 0.0000, max: 173.9576
Log mel spectrogram stats - mean: -5.5332, std: 4.4618, min: -13.7800, max: 5.1588
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8483, max: 2.3963
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8486, max: 2.3965
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT IN HIS HANDS SOLITUDE AND A VIOLIN WERE SURE TO MARRY IN MUSIC
Prediction: 0000000000000000000000
Loss: 15.7362
outputs.loss tensor(15.7362, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0031.flac
Waveform stats - mean: -0.0000, std: 0.0571, min: -0.3376, max: 0.4327
Resampled waveform stats - mean: -0.0000, std: 0.0571, min: -0.3376, max: 0.4327
Raw mel spectrogram stats - mean: 1.2222, std: 6.9113, min: 0.0000, max: 369.2253
Log mel spectrogram stats - mean: -5.2232, std: 4.3838, min: -13.8132, max: 5.9114
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9595, max: 2.5399
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9600, max: 2.5391
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BEFORE WE ARRIVED AT THE HOUSE WE SAW A BODY OF YANKEES APPROACHING AND AS WE STARTED TO RUN BACK THEY FIRED UPON US
Prediction: 00000000000000000000000000000000000
Loss: 16.0348
outputs.loss tensor(16.0348, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0025.flac
Waveform stats - mean: -0.0000, std: 0.0598, min: -0.4278, max: 0.6428
Resampled waveform stats - mean: -0.0000, std: 0.0598, min: -0.4278, max: 0.6428
Raw mel spectrogram stats - mean: 1.3376, std: 9.5912, min: 0.0000, max: 428.0960
Log mel spectrogram stats - mean: -6.8681, std: 4.7240, min: -13.8151, max: 6.0593
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4706, max: 2.7366
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4707, max: 2.7363
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHILE TRAVELING WITH A COMPANY OF HUNTERS I SAW ABOUT FIFTY IN ONE FLOCK
Prediction: 000000000000000000000000
Loss: 15.4222
outputs.loss tensor(15.4222, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0017.flac
Waveform stats - mean: 0.0139, std: 0.0543, min: -0.3533, max: 0.4039
Resampled waveform stats - mean: 0.0139, std: 0.0543, min: -0.3533, max: 0.4039
Raw mel spectrogram stats - mean: 1.1250, std: 7.5980, min: 0.0000, max: 307.7632
Log mel spectrogram stats - mean: -5.6754, std: 4.2051, min: -13.5671, max: 5.7293
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8767, max: 2.7121
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8770, max: 2.7129
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS SO LONG THAT IT BUSHED OUT BEHIND HIS EARS AND MADE HIM LOOK LIKE THE OLD PORTRAITS I REMEMBERED IN VIRGINIA
Prediction: 0000000000000000000000000000000000
Loss: 16.1053
outputs.loss tensor(16.1053, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0031.flac
Waveform stats - mean: 0.0000, std: 0.0542, min: -0.5560, max: 0.5581
Resampled waveform stats - mean: 0.0000, std: 0.0542, min: -0.5560, max: 0.5581
Raw mel spectrogram stats - mean: 1.0639, std: 7.3924, min: 0.0000, max: 428.7787
Log mel spectrogram stats - mean: -6.9747, std: 4.5977, min: -13.7867, max: 6.0609
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.4816, max: 2.8353
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4814, max: 2.8359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS TREASURE TAKEN TYPE OF HIS SELF AND A WOMAN GIVEN HIM INSTEAD
Prediction: 00000000000000000000
Loss: 16.2931
outputs.loss tensor(16.2931, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0069.flac
Waveform stats - mean: -0.0000, std: 0.0302, min: -0.2385, max: 0.2039
Resampled waveform stats - mean: -0.0000, std: 0.0302, min: -0.2385, max: 0.2039
Raw mel spectrogram stats - mean: 0.3405, std: 1.9945, min: 0.0000, max: 82.2956
Log mel spectrogram stats - mean: -6.2492, std: 3.9959, min: -13.7915, max: 4.4103
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8875, max: 2.6676
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8877, max: 2.6680
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IF I HAD KNOWN THEY WERE COMING TO NIGHT I WOULD HAVE SEEN TO IT THAT HE WENT TO BED AT THE SAME TIME I DID
Prediction: 000000000000000000000000000000000
Loss: 15.8841
outputs.loss tensor(15.8841, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0014.flac
Waveform stats - mean: -0.0000, std: 0.0414, min: -0.5344, max: 0.5774
Resampled waveform stats - mean: -0.0000, std: 0.0414, min: -0.5344, max: 0.5774
Raw mel spectrogram stats - mean: 0.6433, std: 5.3917, min: 0.0000, max: 245.1076
Log mel spectrogram stats - mean: -5.3505, std: 3.5126, min: -13.8070, max: 5.5017
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4075, max: 3.0895
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.4082, max: 3.0898
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE MOVEMENT SENT HIS BODY SWAYING GIDDILY FROM SIDE TO SIDE
Prediction: 00000000000000000
Loss: 16.7664
outputs.loss tensor(16.7664, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66125/6313-66125-0007.flac
Waveform stats - mean: -0.0000, std: 0.0498, min: -0.5686, max: 0.6978
Resampled waveform stats - mean: -0.0000, std: 0.0498, min: -0.5686, max: 0.6978
Raw mel spectrogram stats - mean: 0.9285, std: 9.4020, min: 0.0000, max: 480.1407
Log mel spectrogram stats - mean: -5.7715, std: 3.7887, min: -13.8027, max: 6.1741
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1198, max: 3.1529
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1191, max: 3.1523
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I AM THE ONE TO GO AFTER WALT IF ANYONE HAS TO I'LL GO DOWN MISTER THOMAS
Prediction: 000000000000000000000000
Loss: 15.3207
outputs.loss tensor(15.3207, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0019.flac
Waveform stats - mean: -0.0000, std: 0.0813, min: -0.4635, max: 0.4866
Resampled waveform stats - mean: -0.0000, std: 0.0813, min: -0.4635, max: 0.4866
Raw mel spectrogram stats - mean: 2.4667, std: 14.0866, min: 0.0000, max: 501.7838
Log mel spectrogram stats - mean: -5.1488, std: 4.2529, min: -13.6918, max: 6.2182
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0088, max: 2.6728
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0078, max: 2.6719
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE COULD BE LITTLE ART IN THIS LAST AND FINAL ROUND OF FENCING
Prediction: 000000000000000000
Loss: 16.1150
outputs.loss tensor(16.1150, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153948/2412-153948-0009.flac
Waveform stats - mean: -0.0001, std: 0.0409, min: -0.3610, max: 0.3419
Resampled waveform stats - mean: -0.0001, std: 0.0409, min: -0.3610, max: 0.3419
Raw mel spectrogram stats - mean: 0.6237, std: 3.8214, min: 0.0000, max: 139.0241
Log mel spectrogram stats - mean: -5.6815, std: 3.7854, min: -13.7778, max: 4.9346
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1388, max: 2.8045
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1387, max: 2.8047
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS A MONOTONOUS LIFE BUT IT WAS VERY HEALTHY AND ONE DOES NOT MUCH MIND ANYTHING WHEN ONE IS WELL
Prediction: 000000000000000000000000000
Loss: 16.0815
outputs.loss tensor(16.0815, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0004.flac
Waveform stats - mean: -0.0007, std: 0.0845, min: -0.4891, max: 0.6449
Resampled waveform stats - mean: -0.0007, std: 0.0845, min: -0.4891, max: 0.6449
Raw mel spectrogram stats - mean: 2.6757, std: 19.3061, min: 0.0000, max: 1011.8694
Log mel spectrogram stats - mean: -5.5571, std: 4.1090, min: -13.7527, max: 6.9196
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9945, max: 3.0364
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9941, max: 3.0371
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 23552
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 23552
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 164864
Inf count: 0
audio_emb.shape torch.Size([1, 46, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: A COAL FIRE WAS CRACKLING IN THE GRATE AND THE LAMPS WERE LIT FOR IT WAS ALREADY BEGINNING TO GROW DARK OUTSIDE
Prediction: 0000000000000000000000000000000000
Loss: 15.9778
outputs.loss tensor(15.9778, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110523/7976-110523-0007.flac
Waveform stats - mean: -0.0000, std: 0.0510, min: -0.6080, max: 0.3683
Resampled waveform stats - mean: -0.0000, std: 0.0510, min: -0.6080, max: 0.3683
Raw mel spectrogram stats - mean: 0.9726, std: 8.6662, min: 0.0000, max: 639.9810
Log mel spectrogram stats - mean: -6.5839, std: 4.4978, min: -13.8137, max: 6.4614
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6074, max: 2.9004
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6074, max: 2.9004
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT HER HUSBAND FELT HEAVY AT HEART AND THOUGHT IT WERE BETTER TO SHARE THE LAST CRUST WITH THE CHILDREN
Prediction: 0000000000000000000000000000000000
Loss: 16.0053
outputs.loss tensor(16.0053, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0027.flac
Waveform stats - mean: -0.0000, std: 0.0202, min: -0.1448, max: 0.1736
Resampled waveform stats - mean: -0.0000, std: 0.0202, min: -0.1448, max: 0.1736
Raw mel spectrogram stats - mean: 0.1526, std: 1.1016, min: 0.0000, max: 53.2287
Log mel spectrogram stats - mean: -7.2290, std: 3.9611, min: -13.7757, max: 3.9746
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6527, max: 2.8284
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6523, max: 2.8281
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS OWN SKIN HAD SIZZLED UNDER THE RED HOT BRAND HE MURMURED SOFTLY
Prediction: 000000000000000000000000
Loss: 15.9421
outputs.loss tensor(15.9421, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/8226/3536-8226-0017.flac
Waveform stats - mean: -0.0000, std: 0.0362, min: -0.3509, max: 0.3770
Resampled waveform stats - mean: -0.0000, std: 0.0362, min: -0.3509, max: 0.3770
Raw mel spectrogram stats - mean: 0.4824, std: 6.3006, min: 0.0000, max: 283.2854
Log mel spectrogram stats - mean: -7.1838, std: 3.7717, min: -13.7972, max: 5.6465
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7534, max: 3.4017
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7529, max: 3.4023
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN BOZZLE CAME FORWARD AND INTRODUCED HIS WIFE
Prediction: 0000000000000000
Loss: 15.4583
outputs.loss tensor(15.4583, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/135031/1272-135031-0009.flac
Waveform stats - mean: -0.0001, std: 0.0796, min: -0.5035, max: 0.4930
Resampled waveform stats - mean: -0.0001, std: 0.0796, min: -0.5035, max: 0.4930
Raw mel spectrogram stats - mean: 2.3616, std: 17.2716, min: 0.0000, max: 701.2930
Log mel spectrogram stats - mean: -5.3952, std: 3.5561, min: -12.3119, max: 6.5529
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9450, max: 3.3598
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9453, max: 3.3594
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6144
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6144
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 43008
Inf count: 0
audio_emb.shape torch.Size([1, 12, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE DOESN'T WORK AT ALL
Prediction: 0000000
Loss: 15.2645
outputs.loss tensor(15.2645, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4943/3752-4943-0027.flac
Waveform stats - mean: -0.0000, std: 0.1377, min: -0.9244, max: 0.9332
Resampled waveform stats - mean: -0.0000, std: 0.1377, min: -0.9244, max: 0.9332
Raw mel spectrogram stats - mean: 7.0947, std: 50.1163, min: 0.0000, max: 1656.6981
Log mel spectrogram stats - mean: -5.2114, std: 4.9146, min: -13.7977, max: 7.4126
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7471, max: 2.5687
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7471, max: 2.5684
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MUST STOP THAT FIFTY LASHES TROKE
Prediction: 000000000000
Loss: 16.4131
outputs.loss tensor(16.4131, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/141231/1272-141231-0009.flac
Waveform stats - mean: -0.0000, std: 0.0673, min: -0.3666, max: 0.5105
Resampled waveform stats - mean: -0.0000, std: 0.0673, min: -0.3666, max: 0.5105
Raw mel spectrogram stats - mean: 1.6951, std: 11.9514, min: 0.0000, max: 478.7212
Log mel spectrogram stats - mean: -5.7735, std: 4.1924, min: -13.7957, max: 6.1711
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9135, max: 2.8491
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9131, max: 2.8496
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27648
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27648
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 193536
Inf count: 0
audio_emb.shape torch.Size([1, 54, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE CONTESTANTS IN THE TWENTIES NEEDED UNDISTURBED REST THEREFORE NIGHTS IN THE DORMITORIES WERE AS QUIET AS DEATH
Prediction: 0000000000000000000000000000000000
Loss: 16.0885
outputs.loss tensor(16.0885, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0025.flac
Waveform stats - mean: -0.0000, std: 0.0361, min: -0.5868, max: 0.5078
Resampled waveform stats - mean: -0.0000, std: 0.0361, min: -0.5868, max: 0.5078
Raw mel spectrogram stats - mean: 0.4873, std: 3.7408, min: 0.0000, max: 258.6663
Log mel spectrogram stats - mean: -5.6534, std: 3.5282, min: -13.8087, max: 5.5555
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3115, max: 3.1770
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3105, max: 3.1777
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GRASPING THE POMMEL WITH THE LEFT HAND HE APPEARED TO DIVE HEAD FIRST TOWARD THE GROUND
Prediction: 000000000000000000000000000
Loss: 15.7222
outputs.loss tensor(15.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/154328/2803-154328-0006.flac
Waveform stats - mean: -0.0001, std: 0.0654, min: -0.5284, max: 0.4962
Resampled waveform stats - mean: -0.0001, std: 0.0654, min: -0.5284, max: 0.4962
Raw mel spectrogram stats - mean: 1.6016, std: 11.5593, min: 0.0000, max: 457.4424
Log mel spectrogram stats - mean: -6.6207, std: 4.2390, min: -13.6926, max: 6.1257
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6683, max: 3.0069
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6680, max: 3.0078
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GLENARVAN'S VOICE FIRM TILL NOW FALTERED
Prediction: 0000000000000000
Loss: 15.7967
outputs.loss tensor(15.7967, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2803/161169/2803-161169-0004.flac
Waveform stats - mean: -0.0000, std: 0.0268, min: -0.2336, max: 0.1945
Resampled waveform stats - mean: -0.0000, std: 0.0268, min: -0.2336, max: 0.1945
Raw mel spectrogram stats - mean: 0.2659, std: 1.5559, min: 0.0000, max: 80.4462
Log mel spectrogram stats - mean: -7.5487, std: 4.0767, min: -13.7807, max: 4.3876
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5287, max: 2.9279
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5283, max: 2.9277
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 36352
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 36352
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 254464
Inf count: 0
audio_emb.shape torch.Size([1, 71, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE ENTRANCE IS LIGHT BECAUSE IT OPENS SO WIDE BUT WE CAN SEE THAT THE FLOOR SLOPES DOWNWARD AND THE WAY LOOKS DARK AND NARROW BEFORE US
Prediction: 00000000000000000000000000000000000000000
Loss: 15.7047
outputs.loss tensor(15.7047, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0019.flac
Waveform stats - mean: -0.0000, std: 0.0502, min: -0.4098, max: 0.4232
Resampled waveform stats - mean: -0.0000, std: 0.0502, min: -0.4098, max: 0.4232
Raw mel spectrogram stats - mean: 0.9440, std: 4.9272, min: 0.0000, max: 179.7507
Log mel spectrogram stats - mean: -6.8044, std: 4.8767, min: -13.8155, max: 5.1916
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4377, max: 2.4599
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4375, max: 2.4590
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE WAS A TRAP AT THE BOY AND BLUNDERBUSS BUT THAT REQUIRED FETCHING
Prediction: 00000000000000000000
Loss: 15.9952
outputs.loss tensor(15.9952, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/284437/5338-284437-0030.flac
Waveform stats - mean: -0.0000, std: 0.0720, min: -0.4991, max: 0.4424
Resampled waveform stats - mean: -0.0000, std: 0.0720, min: -0.4991, max: 0.4424
Raw mel spectrogram stats - mean: 1.9351, std: 14.1412, min: 0.0000, max: 609.9515
Log mel spectrogram stats - mean: -5.1098, std: 4.0343, min: -13.8038, max: 6.4134
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1550, max: 2.8563
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1543, max: 2.8555
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU ARE NOT LIKE MY PEOPLE THE PINKIES AND THERE IS NO PLACE FOR YOU IN OUR COUNTRY
Prediction: 000000000000000000000
Loss: 15.6123
outputs.loss tensor(15.6123, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0007.flac
Waveform stats - mean: -0.0001, std: 0.0634, min: -0.4229, max: 0.5228
Resampled waveform stats - mean: -0.0001, std: 0.0634, min: -0.4229, max: 0.5228
Raw mel spectrogram stats - mean: 1.5050, std: 8.5188, min: 0.0000, max: 361.3266
Log mel spectrogram stats - mean: -5.1175, std: 4.0253, min: -13.4704, max: 5.8898
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0751, max: 2.7345
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0742, max: 2.7344
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 30208
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 30208
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 211456
Inf count: 0
audio_emb.shape torch.Size([1, 59, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: KNOWING THAT GOD SETS NO VALUE UPON MATERIAL THINGS HE TOOK WITH HIM NO OFFERINGS OR SACRIFICES OTHER THAN SYMBOLIC OBJECTS SUCH AS PAINTS AND TOBACCO
Prediction: 00000000000000000000000000000000000000000000
Loss: 15.6354
outputs.loss tensor(15.6354, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/281318/7850-281318-0018.flac
Waveform stats - mean: -0.0000, std: 0.0600, min: -0.5765, max: 0.7058
Resampled waveform stats - mean: -0.0000, std: 0.0600, min: -0.5765, max: 0.7058
Raw mel spectrogram stats - mean: 1.3461, std: 18.7838, min: 0.0000, max: 1356.7101
Log mel spectrogram stats - mean: -6.5690, std: 3.8165, min: -13.8121, max: 7.2128
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8979, max: 3.6111
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8975, max: 3.6113
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: CRISS CROSS CRISS CROSS SO INTERRUPTED THE WOOD PIGEON
Prediction: 0000000000000000000
Loss: 15.2493
outputs.loss tensor(15.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/66129/6313-66129-0000.flac
Waveform stats - mean: -0.0000, std: 0.0334, min: -0.3972, max: 0.2646
Resampled waveform stats - mean: -0.0000, std: 0.0334, min: -0.3972, max: 0.2646
Raw mel spectrogram stats - mean: 0.4180, std: 3.9502, min: 0.0000, max: 184.7757
Log mel spectrogram stats - mean: -5.6809, std: 3.3325, min: -13.7763, max: 5.2191
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4293, max: 3.2709
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4297, max: 3.2715
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE NO DOUBT WOULD BRING FOOD OF SOME KIND WITH HIM
Prediction: 000000000000000
Loss: 16.2357
outputs.loss tensor(16.2357, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0006.flac
Waveform stats - mean: -0.0000, std: 0.0499, min: -0.4259, max: 0.3887
Resampled waveform stats - mean: -0.0000, std: 0.0499, min: -0.4259, max: 0.3887
Raw mel spectrogram stats - mean: 0.9312, std: 9.6254, min: 0.0000, max: 548.4736
Log mel spectrogram stats - mean: -5.7489, std: 3.5834, min: -13.7806, max: 6.3071
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2414, max: 3.3644
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.3652
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NONE OF YOU WILL BE FIT FOR DUTY TO MORROW
Prediction: 0000000000000
Loss: 15.2531
outputs.loss tensor(15.2531, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0016.flac
Waveform stats - mean: 0.0000, std: 0.0844, min: -0.4880, max: 0.5244
Resampled waveform stats - mean: 0.0000, std: 0.0844, min: -0.4880, max: 0.5244
Raw mel spectrogram stats - mean: 2.6716, std: 16.2279, min: 0.0000, max: 1024.3368
Log mel spectrogram stats - mean: -5.2539, std: 5.0527, min: -13.8149, max: 6.9318
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6944, max: 2.4117
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6943, max: 2.4121
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33792
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33792
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 236544
Inf count: 0
audio_emb.shape torch.Size([1, 66, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I CALLED LIEUTENANT COLONEL FRIERSON'S ATTENTION TO THE YANKEES AND HE REMARKED WELL I DON'T KNOW WHETHER THEY ARE YANKEES OR NOT BUT IF THEY ARE THEY WILL COME OUT OF THERE MIGHTY QUICK
Prediction: 0000000000000000000000000000000000000000000000000000000000
Loss: 16.0552
outputs.loss tensor(16.0552, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0024.flac
Waveform stats - mean: 0.0013, std: 0.2216, min: -0.7509, max: 0.7113
Resampled waveform stats - mean: 0.0013, std: 0.2216, min: -0.7509, max: 0.7113
Raw mel spectrogram stats - mean: 18.3991, std: 117.4570, min: 0.0000, max: 4475.0469
Log mel spectrogram stats - mean: -3.0727, std: 4.5156, min: -13.8154, max: 8.4063
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3790, max: 2.5421
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3789, max: 2.5430
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 47104
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 47104
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 329728
Inf count: 0
audio_emb.shape torch.Size([1, 92, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NEXT EVENING AS MISSUS STERLING SAT ALONE IN THE TWILIGHT A TALL MAN IN ARMY BLUE ENTERED QUIETLY STOOD WATCHING THE TRANQUIL FIGURE FOR A MOMENT THEN WENT AND KNELT DOWN BESIDE IT SAYING WITH A MOST UNSOLDIERLY CHOKE IN THE VOICE
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2843
outputs.loss tensor(16.2843, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8842/302203/8842-302203-0011.flac
Waveform stats - mean: -0.0000, std: 0.0458, min: -0.4407, max: 0.5371
Resampled waveform stats - mean: -0.0000, std: 0.0458, min: -0.4407, max: 0.5371
Raw mel spectrogram stats - mean: 0.7852, std: 7.0829, min: 0.0000, max: 388.1401
Log mel spectrogram stats - mean: -6.8688, std: 3.9416, min: -13.8115, max: 5.9614
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7614, max: 3.2551
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7617, max: 3.2559
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22016
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22016
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 154112
Inf count: 0
audio_emb.shape torch.Size([1, 43, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE SECOND PART BEGINS HERE SAYING BE NOW THE THIRD HERE THEN WHILE IT WAS HIS PLEASURE
Prediction: 0000000000000000000000
Loss: 16.3864
outputs.loss tensor(16.3864, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0025.flac
Waveform stats - mean: 0.0000, std: 0.0596, min: -0.4808, max: 0.5254
Resampled waveform stats - mean: 0.0000, std: 0.0596, min: -0.4808, max: 0.5254
Raw mel spectrogram stats - mean: 1.3311, std: 15.2606, min: 0.0000, max: 1333.9128
Log mel spectrogram stats - mean: -6.8392, std: 4.1287, min: -13.8011, max: 7.1959
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6862, max: 3.3994
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6865, max: 3.3984
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 37376
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 37376
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 261632
Inf count: 0
audio_emb.shape torch.Size([1, 73, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS PLAIN THAT HIS CASTANET GIRL HIS MOTHER AND SISTER TOOK A PLEASURE IN CREDITING HER DAILY WITH SOME FRESH AND UNPLEASING INSTRUMENT COULD HAVE HAD NEITHER TASTE MONEY NOR HONESTY TO SUCH A POINT AS THIS
Prediction: 000000000000000000000000000000000000000000000000000000000000000
Loss: 16.1305
outputs.loss tensor(16.1305, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/251/118436/251-118436-0008.flac
Waveform stats - mean: 0.0000, std: 0.0697, min: -0.3236, max: 0.3347
Resampled waveform stats - mean: 0.0000, std: 0.0697, min: -0.3236, max: 0.3347
Raw mel spectrogram stats - mean: 1.8184, std: 9.5847, min: 0.0000, max: 246.1136
Log mel spectrogram stats - mean: -6.4768, std: 4.7237, min: -13.8113, max: 5.5058
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5527, max: 2.5367
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5527, max: 2.5371
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WITH A LONG STAINED FINGERNAIL HE MAPPED THE CONSTELLATIONS ON THE MARBLE TILED FLOOR
Prediction: 000000000000000000000000
Loss: 17.1140
outputs.loss tensor(17.1140, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/128104/1272-128104-0014.flac
Waveform stats - mean: -0.0000, std: 0.0548, min: -0.2598, max: 0.2650
Resampled waveform stats - mean: -0.0000, std: 0.0548, min: -0.2598, max: 0.2650
Raw mel spectrogram stats - mean: 1.1266, std: 6.6021, min: 0.0000, max: 178.1450
Log mel spectrogram stats - mean: -6.1038, std: 4.3435, min: -13.7972, max: 5.1826
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7712, max: 2.5984
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7715, max: 2.5977
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 7680
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 7680
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 53760
Inf count: 0
audio_emb.shape torch.Size([1, 15, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BY HARRY QUILTER M A
Prediction: 00000000
Loss: 15.6887
outputs.loss tensor(15.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0019.flac
Waveform stats - mean: -0.0001, std: 0.0524, min: -0.2249, max: 0.3278
Resampled waveform stats - mean: -0.0001, std: 0.0524, min: -0.2249, max: 0.3278
Raw mel spectrogram stats - mean: 1.0292, std: 4.7870, min: 0.0000, max: 173.0166
Log mel spectrogram stats - mean: -5.7988, std: 3.9928, min: -13.5978, max: 5.1534
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9532, max: 2.7430
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9531, max: 2.7422
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE HISTORIANS OF THE WHITE RACE ADMIT THAT THE INDIAN WAS NEVER THE FIRST TO REPUDIATE HIS OATH
Prediction: 000000000000000000000000000000
Loss: 15.8401
outputs.loss tensor(15.8401, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/129742/652-129742-0000.flac
Waveform stats - mean: -0.0001, std: 0.0523, min: -0.4698, max: 0.4238
Resampled waveform stats - mean: -0.0001, std: 0.0523, min: -0.4698, max: 0.4238
Raw mel spectrogram stats - mean: 1.0054, std: 5.5509, min: 0.0000, max: 202.8686
Log mel spectrogram stats - mean: -6.1946, std: 4.4637, min: -13.7855, max: 5.3126
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7006, max: 2.5779
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7002, max: 2.5781
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19456
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19456
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 136192
Inf count: 0
audio_emb.shape torch.Size([1, 38, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ASPARAGUS SALAD COOK THE ASPARAGUS IN SALTED WATER DRAIN AND CHILL
Prediction: 0000000000000000000000
Loss: 15.7271
outputs.loss tensor(15.7271, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170138/1462-170138-0002.flac
Waveform stats - mean: -0.0006, std: 0.0631, min: -0.3666, max: 0.3887
Resampled waveform stats - mean: -0.0006, std: 0.0631, min: -0.3666, max: 0.3887
Raw mel spectrogram stats - mean: 1.4897, std: 8.8767, min: 0.0000, max: 341.1385
Log mel spectrogram stats - mean: -5.7523, std: 3.9998, min: -13.7612, max: 5.8323
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0024, max: 2.8963
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0020, max: 2.8965
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15360
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15360
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 107520
Inf count: 0
audio_emb.shape torch.Size([1, 30, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I HAPPEN TO HAVE MAC CONNELL'S BOX FOR TONIGHT OR THERE'D BE NO CHANCE OF OUR GETTING PLACES
Prediction: 0000000000000000000000000000
Loss: 16.2633
outputs.loss tensor(16.2633, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0001.flac
Waveform stats - mean: 0.0001, std: 0.0534, min: -0.4995, max: 0.4196
Resampled waveform stats - mean: 0.0001, std: 0.0534, min: -0.4995, max: 0.4196
Raw mel spectrogram stats - mean: 1.0138, std: 4.8781, min: 0.0000, max: 261.1948
Log mel spectrogram stats - mean: -4.6370, std: 3.7568, min: -13.7689, max: 5.5653
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4308, max: 2.7157
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4316, max: 2.7148
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34816
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34816
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 243712
Inf count: 0
audio_emb.shape torch.Size([1, 68, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THOMPSON OPENED A LARGE RESTAURANT IN O'FARRELL STREET JUST ABOVE FILLMORE AND FOR TWO YEARS OR MORE DID A THRIVING BUSINESS HIS PLACE BEING NOTED FOR ITS GOOD COOKING AND ITS SPLENDID SERVICE
Prediction: 000000000000000000000000000000000000000000000000000000
Loss: 15.8888
outputs.loss tensor(15.8888, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0008.flac
Waveform stats - mean: -0.0001, std: 0.0443, min: -0.5903, max: 0.4300
Resampled waveform stats - mean: -0.0001, std: 0.0443, min: -0.5903, max: 0.4300
Raw mel spectrogram stats - mean: 0.7287, std: 6.3373, min: 0.0000, max: 296.7831
Log mel spectrogram stats - mean: -7.5349, std: 4.9995, min: -13.8155, max: 5.6930
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.2562, max: 2.6459
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.2559, max: 2.6465
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE MEN WERE AS HANDSOME AS THE WOMEN BEAUTIFUL
Prediction: 0000000000000000
Loss: 15.6251
outputs.loss tensor(15.6251, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0048.flac
Waveform stats - mean: 0.0009, std: 0.0155, min: -0.1218, max: 0.1355
Resampled waveform stats - mean: 0.0009, std: 0.0155, min: -0.1218, max: 0.1355
Raw mel spectrogram stats - mean: 0.0866, std: 0.3859, min: 0.0000, max: 11.0754
Log mel spectrogram stats - mean: -6.3622, std: 3.0861, min: -13.4277, max: 2.4047
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2894, max: 2.8408
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2891, max: 2.8398
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WELL WELL THAT'S HONEST AT ALL EVENTS
Prediction: 00000000000
Loss: 16.6319
outputs.loss tensor(16.6319, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0011.flac
Waveform stats - mean: 0.0000, std: 0.0745, min: -0.4479, max: 0.5574
Resampled waveform stats - mean: 0.0000, std: 0.0745, min: -0.4479, max: 0.5574
Raw mel spectrogram stats - mean: 2.0783, std: 10.9769, min: 0.0000, max: 412.3693
Log mel spectrogram stats - mean: -5.7513, std: 4.7808, min: -13.8115, max: 6.0219
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6859, max: 2.4626
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6855, max: 2.4629
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 22528
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 22528
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 157696
Inf count: 0
audio_emb.shape torch.Size([1, 44, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I HAD BEEN FEELING MEAN ALL THE MORNING AS IF I HAD STOLEN A SHEEP BUT WHEN THE ORDER TO CHARGE WAS GIVEN I GOT HAPPY
Prediction: 0000000000000000000000000000000000000
Loss: 15.5845
outputs.loss tensor(15.5845, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149897/2277-149897-0015.flac
Waveform stats - mean: 0.0000, std: 0.0735, min: -0.6113, max: 0.5529
Resampled waveform stats - mean: 0.0000, std: 0.0735, min: -0.6113, max: 0.5529
Raw mel spectrogram stats - mean: 2.0199, std: 19.8161, min: 0.0000, max: 914.1916
Log mel spectrogram stats - mean: -5.8834, std: 4.0948, min: -13.7728, max: 6.8180
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9267, max: 3.1018
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9268, max: 3.1016
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN ABOUT AN HOUR AND THREE QUARTERS THE BOY RETURNED
Prediction: 00000000000000
Loss: 15.8337
outputs.loss tensor(15.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0019.flac
Waveform stats - mean: -0.0000, std: 0.0582, min: -0.3971, max: 0.3756
Resampled waveform stats - mean: -0.0000, std: 0.0582, min: -0.3971, max: 0.3756
Raw mel spectrogram stats - mean: 1.2667, std: 8.9139, min: 0.0000, max: 385.0211
Log mel spectrogram stats - mean: -5.7177, std: 4.2027, min: -13.7666, max: 5.9533
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9152, max: 2.7770
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9150, max: 2.7773
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I PICKED IT UP AND COMING UP TO HIM JUST AS HE WAS GOING DOWN THE STEPS I HANDED IT TO HIM
Prediction: 0000000000000000000000000000
Loss: 15.9482
outputs.loss tensor(15.9482, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0036.flac
Waveform stats - mean: 0.0007, std: 0.0169, min: -0.1370, max: 0.1711
Resampled waveform stats - mean: 0.0007, std: 0.0169, min: -0.1370, max: 0.1711
Raw mel spectrogram stats - mean: 0.0969, std: 0.8769, min: 0.0000, max: 61.4994
Log mel spectrogram stats - mean: -6.5476, std: 3.0504, min: -13.4754, max: 4.1190
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2711, max: 3.4968
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2715, max: 3.4961
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT'S A CASE IN A THOUSAND GRYCE
Prediction: 000000000000
Loss: 15.1062
outputs.loss tensor(15.1062, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110124/7976-110124-0017.flac
Waveform stats - mean: -0.0000, std: 0.0525, min: -0.4012, max: 0.3318
Resampled waveform stats - mean: -0.0000, std: 0.0525, min: -0.4012, max: 0.3318
Raw mel spectrogram stats - mean: 1.0300, std: 10.1765, min: 0.0000, max: 534.6733
Log mel spectrogram stats - mean: -6.6548, std: 4.5077, min: -13.8145, max: 6.2817
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5883, max: 2.8698
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.5879, max: 2.8691
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8704
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8704
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 60928
Inf count: 0
audio_emb.shape torch.Size([1, 17, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: PERHAPS YOU CAN OUTWIT HER YET CRIED ANOTHER
Prediction: 0000000000000000
Loss: 15.1564
outputs.loss tensor(15.1564, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34629/5895-34629-0025.flac
Waveform stats - mean: -0.0000, std: 0.0512, min: -0.4189, max: 0.5045
Resampled waveform stats - mean: -0.0000, std: 0.0512, min: -0.4189, max: 0.5045
Raw mel spectrogram stats - mean: 0.9799, std: 6.6749, min: 0.0000, max: 336.0774
Log mel spectrogram stats - mean: -4.9296, std: 3.8593, min: -13.4755, max: 5.8173
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2144, max: 2.7847
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2148, max: 2.7852
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS CONNOISSEUR WAS SUDDENLY FASCINATED AND HAD ADOPTED THE LAUGHING MAN
Prediction: 000000000000000000000000000
Loss: 15.6165
outputs.loss tensor(15.6165, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0018.flac
Waveform stats - mean: -0.0000, std: 0.0608, min: -0.5097, max: 0.4734
Resampled waveform stats - mean: -0.0000, std: 0.0608, min: -0.5097, max: 0.4734
Raw mel spectrogram stats - mean: 1.3769, std: 8.0620, min: 0.0000, max: 295.0031
Log mel spectrogram stats - mean: -4.5603, std: 3.8774, min: -13.7361, max: 5.6870
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3665, max: 2.6428
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3672, max: 2.6426
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN ANY CASE I SHALL TRUST RATHER TO MY OWN INTELLIGENCE THAN THEIRS
Prediction: 0000000000000000000
Loss: 16.5395
outputs.loss tensor(16.5395, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170142/1462-170142-0013.flac
Waveform stats - mean: -0.0006, std: 0.0778, min: -0.5715, max: 0.5228
Resampled waveform stats - mean: -0.0006, std: 0.0778, min: -0.5715, max: 0.5228
Raw mel spectrogram stats - mean: 2.2659, std: 14.9240, min: 0.0000, max: 405.0663
Log mel spectrogram stats - mean: -6.6164, std: 4.2014, min: -13.8025, max: 6.0041
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7104, max: 3.0039
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7100, max: 3.0039
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12800
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12800
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 89600
Inf count: 0
audio_emb.shape torch.Size([1, 25, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IT HASN'T ALWAYS MADE YOU MISERABLE HAS IT
Prediction: 0000000000000
Loss: 16.4810
outputs.loss tensor(16.4810, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0010.flac
Waveform stats - mean: -0.0000, std: 0.0441, min: -0.2916, max: 0.4262
Resampled waveform stats - mean: -0.0000, std: 0.0441, min: -0.2916, max: 0.4262
Raw mel spectrogram stats - mean: 0.7282, std: 3.8020, min: 0.0000, max: 113.4960
Log mel spectrogram stats - mean: -5.5542, std: 4.2090, min: -13.7641, max: 4.7318
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9506, max: 2.4438
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9502, max: 2.4434
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND I COULD NEVER GET AS MANY AS THREE SUCH MEN TOGETHER
Prediction: 000000000000000
Loss: 16.0609
outputs.loss tensor(16.0609, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0002.flac
Waveform stats - mean: 0.0001, std: 0.0753, min: -0.5014, max: 0.3931
Resampled waveform stats - mean: 0.0001, std: 0.0753, min: -0.5014, max: 0.3931
Raw mel spectrogram stats - mean: 1.9699, std: 11.2778, min: 0.0000, max: 1098.6714
Log mel spectrogram stats - mean: -6.3726, std: 5.1096, min: -13.8053, max: 7.0019
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4546, max: 2.6175
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4551, max: 2.6172
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 19968
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 19968
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 139776
Inf count: 0
audio_emb.shape torch.Size([1, 39, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT THE EMOTIONS OF HARRY AND HIS COMRADES WERE FOR THE MOMENT THOSE OF VICTORY ONLY
Prediction: 000000000000000000000000000
Loss: 16.4378
outputs.loss tensor(16.4378, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/84/121550/84-121550-0001.flac
Waveform stats - mean: 0.0000, std: 0.0675, min: -0.3497, max: 0.3903
Resampled waveform stats - mean: 0.0000, std: 0.0675, min: -0.3497, max: 0.3903
Raw mel spectrogram stats - mean: 1.7057, std: 9.9087, min: 0.0000, max: 431.7571
Log mel spectrogram stats - mean: -5.0688, std: 3.8209, min: -13.8155, max: 6.0679
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2892, max: 2.9147
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2891, max: 2.9141
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALL WATERS THAT ON EARTH MOST LIMPID ARE WOULD SEEM TO HAVE WITHIN THEMSELVES SOME MIXTURE COMPARED WITH THAT WHICH NOTHING DOTH CONCEAL
Prediction: 00000000000000000000000000000000000000
Loss: 16.3604
outputs.loss tensor(16.3604, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1673/143396/1673-143396-0016.flac
Waveform stats - mean: -0.0000, std: 0.1137, min: -0.7517, max: 0.8101
Resampled waveform stats - mean: -0.0000, std: 0.1137, min: -0.7517, max: 0.8101
Raw mel spectrogram stats - mean: 4.8482, std: 53.1385, min: 0.0000, max: 3244.6572
Log mel spectrogram stats - mean: -4.3831, std: 3.6296, min: -13.2924, max: 8.0848
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4546, max: 3.4351
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4551, max: 3.4355
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 46592
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 46592
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 326144
Inf count: 0
audio_emb.shape torch.Size([1, 91, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE WORTHY FRIEND OF ATHANASIUS THE WORTHY ANTAGONIST OF JULIAN HE BRAVELY WRESTLED WITH THE ARIANS AND POLYTHEISTS AND THOUGH HE AFFECTED THE RIGOR OF GEOMETRICAL DEMONSTRATION HIS COMMENTARIES REVEALED THE LITERAL AND ALLEGORICAL SENSE OF THE SCRIPTURES
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2709
outputs.loss tensor(16.2709, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5895/34622/5895-34622-0022.flac
Waveform stats - mean: 0.0000, std: 0.0468, min: -0.4047, max: 0.4164
Resampled waveform stats - mean: 0.0000, std: 0.0468, min: -0.4047, max: 0.4164
Raw mel spectrogram stats - mean: 0.8201, std: 7.0263, min: 0.0000, max: 510.4887
Log mel spectrogram stats - mean: -5.3547, std: 3.7273, min: -13.8107, max: 6.2354
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2687, max: 3.1095
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2695, max: 3.1094
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS OPENING LOOKED FOR ALL THE WORLD LIKE A MOUTH OF HELL IN THE WORDS OF THE ITINERANT PURITAN PREACHERS WHO TURNED AWAY FROM IT WITH HORROR
Prediction: 0000000000000000000000000000000000000000000
Loss: 15.4874
outputs.loss tensor(15.4874, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0035.flac
Waveform stats - mean: -0.0001, std: 0.0772, min: -0.4924, max: 0.6349
Resampled waveform stats - mean: -0.0001, std: 0.0772, min: -0.4924, max: 0.6349
Raw mel spectrogram stats - mean: 2.2303, std: 25.7310, min: 0.0000, max: 1320.6602
Log mel spectrogram stats - mean: -6.1712, std: 3.8268, min: -13.6772, max: 7.1859
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9614, max: 3.4904
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9609, max: 3.4902
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEY WORKED IN CHICAGO DES MOINES FORT WAYNE BUT THEY WERE ALWAYS UNFORTUNATE
Prediction: 0000000000000000000000000
Loss: 15.9642
outputs.loss tensor(15.9642, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0029.flac
Waveform stats - mean: -0.0000, std: 0.0555, min: -0.4032, max: 0.3355
Resampled waveform stats - mean: -0.0000, std: 0.0555, min: -0.4032, max: 0.3355
Raw mel spectrogram stats - mean: 1.1512, std: 9.3265, min: 0.0000, max: 258.1990
Log mel spectrogram stats - mean: -6.4232, std: 4.1349, min: -13.7545, max: 5.5537
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7730, max: 2.8965
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7734, max: 2.8965
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE ENTREATED ME TO TELL HIM THE TRUTH
Prediction: 00000000000
Loss: 17.0154
outputs.loss tensor(17.0154, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0021.flac
Waveform stats - mean: 0.0000, std: 0.0584, min: -0.3177, max: 0.5584
Resampled waveform stats - mean: 0.0000, std: 0.0584, min: -0.3177, max: 0.5584
Raw mel spectrogram stats - mean: 1.2774, std: 11.9755, min: 0.0000, max: 651.5112
Log mel spectrogram stats - mean: -6.9676, std: 4.3796, min: -13.7464, max: 6.4793
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5478, max: 3.0704
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5479, max: 3.0703
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 12288
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 12288
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 86016
Inf count: 0
audio_emb.shape torch.Size([1, 24, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALL THE SAME HE ADDED IRRELEVANTLY YOU SHALL HAVE THE LATCH KEY
Prediction: 000000000000000000
Loss: 16.8185
outputs.loss tensor(16.8185, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0004.flac
Waveform stats - mean: -0.0000, std: 0.0846, min: -0.5055, max: 0.5179
Resampled waveform stats - mean: -0.0000, std: 0.0846, min: -0.5055, max: 0.5179
Raw mel spectrogram stats - mean: 2.6826, std: 15.6393, min: 0.0000, max: 733.4119
Log mel spectrogram stats - mean: -5.4380, std: 4.1724, min: -13.5577, max: 6.5977
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9460, max: 2.8846
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9463, max: 2.8848
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHY NOT ALLOW YOUR SILVER TUFTS TO LUXURIATE IN A NATURAL MANNER
Prediction: 000000000000000000000
Loss: 15.8019
outputs.loss tensor(15.8019, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0007.flac
Waveform stats - mean: -0.0000, std: 0.0714, min: -0.6227, max: 0.4343
Resampled waveform stats - mean: -0.0000, std: 0.0714, min: -0.6227, max: 0.4343
Raw mel spectrogram stats - mean: 1.9079, std: 12.1025, min: 0.0000, max: 834.6205
Log mel spectrogram stats - mean: -4.7634, std: 3.8575, min: -13.2653, max: 6.7270
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2040, max: 2.9787
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2031, max: 2.9785
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 66560
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 66560
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 465920
Inf count: 0
audio_emb.shape torch.Size([1, 130, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN PLACE THE PAN ON A STRONG CHAIR OR DRESSER OR TABLE OF CONVENIENT HEIGHT POUR INTO THE SPONGE THE REMAINDER OF THE WARM MILK AND WATER STIR INTO IT AS MUCH OF THE FLOUR AS YOU CAN WITH THE SPOON THEN WIPE IT OUT CLEAN WITH YOUR FINGERS AND LAY IT ASIDE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.6875
outputs.loss tensor(15.6875, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0027.flac
Waveform stats - mean: -0.0000, std: 0.0542, min: -0.4164, max: 0.4071
Resampled waveform stats - mean: -0.0000, std: 0.0542, min: -0.4164, max: 0.4071
Raw mel spectrogram stats - mean: 1.1002, std: 9.0517, min: 0.0000, max: 239.2459
Log mel spectrogram stats - mean: -5.5551, std: 3.5695, min: -13.1583, max: 5.4775
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1300, max: 3.0908
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1309, max: 3.0898
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HURSTWOOD ALMOST EXCLAIMED OUT LOUD AT THE INSISTENCY OF THIS THING
Prediction: 000000000000000000000
Loss: 16.5295
outputs.loss tensor(16.5295, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0081.flac
Waveform stats - mean: 0.0000, std: 0.0202, min: -0.1283, max: 0.1215
Resampled waveform stats - mean: 0.0000, std: 0.0202, min: -0.1283, max: 0.1215
Raw mel spectrogram stats - mean: 0.1518, std: 0.7727, min: 0.0000, max: 14.8687
Log mel spectrogram stats - mean: -7.7497, std: 3.7689, min: -13.7354, max: 2.6993
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5882, max: 2.7724
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5879, max: 2.7715
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 5120
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 5120
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 35840
Inf count: 0
audio_emb.shape torch.Size([1, 10, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: COMFORTABLE DEAR
Prediction: 000000
Loss: 15.2458
outputs.loss tensor(15.2458, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0029.flac
Waveform stats - mean: 0.0136, std: 0.0707, min: -0.4220, max: 0.4173
Resampled waveform stats - mean: 0.0136, std: 0.0707, min: -0.4220, max: 0.4173
Raw mel spectrogram stats - mean: 1.8881, std: 15.0355, min: 0.0000, max: 725.3647
Log mel spectrogram stats - mean: -5.4992, std: 4.4094, min: -13.6472, max: 6.5867
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8479, max: 2.7410
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.8477, max: 2.7402
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18432
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18432
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 129024
Inf count: 0
audio_emb.shape torch.Size([1, 36, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I BECAME SOMEWHAT EMBARRASSED FOR I WAS USED TO BEING TAKEN FOR GRANTED BY MY ELDERS
Prediction: 00000000000000000000000000000
Loss: 15.7316
outputs.loss tensor(15.7316, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0008.flac
Waveform stats - mean: -0.0001, std: 0.0699, min: -0.5572, max: 0.4217
Resampled waveform stats - mean: -0.0001, std: 0.0699, min: -0.5572, max: 0.4217
Raw mel spectrogram stats - mean: 1.8288, std: 10.5986, min: 0.0000, max: 817.7444
Log mel spectrogram stats - mean: -4.7151, std: 3.8566, min: -13.8149, max: 6.7065
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3596, max: 2.9616
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3594, max: 2.9609
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 62464
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 62464
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 437248
Inf count: 0
audio_emb.shape torch.Size([1, 122, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: TURN IT THEN ON TO A PASTE BOARD OR VERY CLEAN DRESSER AND WITH A LARGE SHARP KNIFE DIVIDE IT IN TWO MAKE IT UP QUICKLY INTO LOAVES AND DISPATCH IT TO THE OVEN MAKE ONE OR TWO INCISIONS ACROSS THE TOPS OF THE LOAVES AS THEY WILL RISE MORE EASILY IF THIS BE DONE
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.7788
outputs.loss tensor(15.7788, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0009.flac
Waveform stats - mean: -0.0000, std: 0.0703, min: -0.4788, max: 0.4624
Resampled waveform stats - mean: -0.0000, std: 0.0703, min: -0.4788, max: 0.4624
Raw mel spectrogram stats - mean: 1.8482, std: 11.5986, min: 0.0000, max: 371.5422
Log mel spectrogram stats - mean: -6.0319, std: 4.1311, min: -13.7180, max: 5.9177
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.8605, max: 2.8926
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8604, max: 2.8926
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: INDEED NOT A FLOWER ESCAPED HIS MISCHIEVOUS SUGGESTIONS
Prediction: 00000000000000000000
Loss: 15.8319
outputs.loss tensor(15.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61943/6241-61943-0023.flac
Waveform stats - mean: -0.0000, std: 0.0667, min: -0.5351, max: 0.3929
Resampled waveform stats - mean: -0.0000, std: 0.0667, min: -0.5351, max: 0.3929
Raw mel spectrogram stats - mean: 1.6645, std: 10.5594, min: 0.0000, max: 383.9561
Log mel spectrogram stats - mean: -4.6709, std: 3.6475, min: -13.7916, max: 5.9505
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.5005, max: 2.9120
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.5000, max: 2.9121
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THOUGH NOT VERY LARGE IT APPEARED NOT LIKELY TO BE FILLED FOR CENTURIES
Prediction: 000000000000000000000
Loss: 16.6746
outputs.loss tensor(16.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83699/2428-83699-0026.flac
Waveform stats - mean: -0.0000, std: 0.0575, min: -0.4873, max: 0.5675
Resampled waveform stats - mean: -0.0000, std: 0.0575, min: -0.4873, max: 0.5675
Raw mel spectrogram stats - mean: 1.2356, std: 8.1455, min: 0.0000, max: 268.1525
Log mel spectrogram stats - mean: -7.9807, std: 5.1143, min: -13.8149, max: 5.5916
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.1407, max: 2.6538
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.1406, max: 2.6543
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HERE WE BE THAT MIGHT BE SO
Prediction: 00000000
Loss: 15.4452
outputs.loss tensor(15.4452, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0018.flac
Waveform stats - mean: -0.0001, std: 0.0615, min: -0.4030, max: 0.5459
Resampled waveform stats - mean: -0.0001, std: 0.0615, min: -0.4030, max: 0.5459
Raw mel spectrogram stats - mean: 1.4180, std: 8.7730, min: 0.0000, max: 442.6687
Log mel spectrogram stats - mean: -5.4791, std: 4.0029, min: -13.5859, max: 6.0928
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0252, max: 2.8909
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0254, max: 2.8906
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 38912
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 38912
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 272384
Inf count: 0
audio_emb.shape torch.Size([1, 76, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IN HIS OWN THOUGHT HE ROSE SUPERIOR TO THEM HE SCORNED THEM EVEN AS A LOFTY SPIRIT ABSORBED IN ITS STERN TASK REJECTS THE SOFT BEDS THE LUXURIOUS FOOD THE PLEASURE WORSHIPING DALLIANCE OF A RICH NEIGHBOR
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2332
outputs.loss tensor(16.2332, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6345/93306/6345-93306-0019.flac
Waveform stats - mean: -0.0000, std: 0.0654, min: -0.4510, max: 0.4935
Resampled waveform stats - mean: -0.0000, std: 0.0654, min: -0.4510, max: 0.4935
Raw mel spectrogram stats - mean: 1.6036, std: 25.3186, min: 0.0000, max: 1397.5994
Log mel spectrogram stats - mean: -7.3354, std: 4.0219, min: -13.6946, max: 7.2425
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.5811, max: 3.6246
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5811, max: 3.6250
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24576
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24576
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 172032
Inf count: 0
audio_emb.shape torch.Size([1, 48, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YOU SEE PAPA'S SO VERY RICH AND AT HOME THEY EXPECT ME TO TO GET ACQUAINTED WITH DUKES AND THINGS AND SHE STOPPED
Prediction: 0000000000000000000000000000000000
Loss: 15.7592
outputs.loss tensor(15.7592, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110124/7976-110124-0023.flac
Waveform stats - mean: -0.0000, std: 0.0839, min: -0.6799, max: 0.4503
Resampled waveform stats - mean: -0.0000, std: 0.0839, min: -0.6799, max: 0.4503
Raw mel spectrogram stats - mean: 2.6329, std: 20.9122, min: 0.0000, max: 1031.0394
Log mel spectrogram stats - mean: -5.7528, std: 4.4266, min: -13.8128, max: 6.9383
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8208, max: 2.8670
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8213, max: 2.8672
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE CRIES AND CURSES OF THE ROBBERS FILLED THE AIR
Prediction: 000000000000000
Loss: 16.3047
outputs.loss tensor(16.3047, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153954/2412-153954-0005.flac
Waveform stats - mean: -0.0001, std: 0.0314, min: -0.2997, max: 0.4266
Resampled waveform stats - mean: -0.0001, std: 0.0314, min: -0.2997, max: 0.4266
Raw mel spectrogram stats - mean: 0.3684, std: 2.2844, min: 0.0000, max: 99.3073
Log mel spectrogram stats - mean: -7.6063, std: 4.6474, min: -13.8155, max: 4.5982
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3361, max: 2.6261
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.3359, max: 2.6270
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: EACH FEATURE WAS FINISHED EYELIDS EYELASHES AND EARS BEING ALMOST INVARIABLY PERFECT
Prediction: 0000000000000000000000000000
Loss: 16.2577
outputs.loss tensor(16.2577, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6313/76958/6313-76958-0004.flac
Waveform stats - mean: -0.0000, std: 0.0437, min: -0.4421, max: 0.4626
Resampled waveform stats - mean: -0.0000, std: 0.0437, min: -0.4421, max: 0.4626
Raw mel spectrogram stats - mean: 0.7152, std: 5.4027, min: 0.0000, max: 301.6186
Log mel spectrogram stats - mean: -5.5094, std: 3.7031, min: -13.8113, max: 5.7092
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2419, max: 3.0295
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2422, max: 3.0293
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE PONY DID MOST OF IT ADMITTED THE LAD I JUST GAVE HIM HIS HEAD AND THAT'S ALL THERE WAS TO IT
Prediction: 0000000000000000000000000000
Loss: 16.3334
outputs.loss tensor(16.3334, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0049.flac
Waveform stats - mean: 0.0009, std: 0.0187, min: -0.1172, max: 0.1403
Resampled waveform stats - mean: 0.0009, std: 0.0187, min: -0.1172, max: 0.1403
Raw mel spectrogram stats - mean: 0.1214, std: 0.5480, min: 0.0000, max: 17.4014
Log mel spectrogram stats - mean: -6.3032, std: 3.1651, min: -13.7906, max: 2.8565
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3656, max: 2.8940
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3652, max: 2.8945
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT I'M IN NO POSITION TO MAKE PROMISES
Prediction: 000000000000
Loss: 15.4383
outputs.loss tensor(15.4383, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2428/83705/2428-83705-0035.flac
Waveform stats - mean: -0.0000, std: 0.0536, min: -0.5373, max: 0.6645
Resampled waveform stats - mean: -0.0000, std: 0.0536, min: -0.5373, max: 0.6645
Raw mel spectrogram stats - mean: 1.0769, std: 6.9432, min: 0.0000, max: 605.0079
Log mel spectrogram stats - mean: -7.0933, std: 4.9954, min: -13.8148, max: 6.4052
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.3455, max: 2.7022
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.3457, max: 2.7031
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: AND WHAT INQUIRED MISSUS MACPHERSON HAS MARY ANN GIVEN YOU HER LOVE
Prediction: 000000000000000000
Loss: 16.7221
outputs.loss tensor(16.7221, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/64301/6295-64301-0011.flac
Waveform stats - mean: -0.0000, std: 0.0484, min: -0.3876, max: 0.4235
Resampled waveform stats - mean: -0.0000, std: 0.0484, min: -0.3876, max: 0.4235
Raw mel spectrogram stats - mean: 0.8707, std: 4.4052, min: 0.0000, max: 234.0798
Log mel spectrogram stats - mean: -6.0779, std: 4.3335, min: -13.7780, max: 5.4557
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7769, max: 2.6615
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7773, max: 2.6621
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 32768
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 32768
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 229376
Inf count: 0
audio_emb.shape torch.Size([1, 64, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: LETTY FINDING HERSELF NOT QUITE EQUAL TO THE EMERGENCY CAME IN HER TURN TO CALL MARY SHE WENT AS QUIETLY AS IF SHE WERE LEAVING A TIRESOME VISITOR
Prediction: 00000000000000000000000000000000000000000000
Loss: 16.5322
outputs.loss tensor(16.5322, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5338/24615/5338-24615-0013.flac
Waveform stats - mean: -0.0000, std: 0.0568, min: -0.8311, max: 0.6354
Resampled waveform stats - mean: -0.0000, std: 0.0568, min: -0.8311, max: 0.6354
Raw mel spectrogram stats - mean: 1.2076, std: 14.7981, min: 0.0000, max: 1380.8630
Log mel spectrogram stats - mean: -5.7748, std: 3.9404, min: -13.7713, max: 7.2305
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0294, max: 3.3005
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0293, max: 3.3008
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 26112
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 26112
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 182784
Inf count: 0
audio_emb.shape torch.Size([1, 51, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE COURT WAS SPACIOUS WELL PAVED AND PERFECTLY CLEAN THERE BEING PROBABLY ANOTHER ENTRANCE BEHIND THE STABLES FOR REMOVING THE LITTER
Prediction: 000000000000000000000000000000000000000
Loss: 16.4423
outputs.loss tensor(16.4423, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3170/137482/3170-137482-0034.flac
Waveform stats - mean: -0.0000, std: 0.0657, min: -0.3951, max: 0.3511
Resampled waveform stats - mean: -0.0000, std: 0.0657, min: -0.3951, max: 0.3511
Raw mel spectrogram stats - mean: 1.6183, std: 10.6510, min: 0.0000, max: 433.0760
Log mel spectrogram stats - mean: -6.0020, std: 4.7091, min: -13.7997, max: 6.0709
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6559, max: 2.5637
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6562, max: 2.5645
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 25600
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 25600
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 179200
Inf count: 0
audio_emb.shape torch.Size([1, 50, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I DECLARED MYSELF QUITE WILLING FOR IT WAS NECESSARY TO BRAZEN IT OUT AFTER HAVING VENTURED AS FAR AS I HAD DONE
Prediction: 0000000000000000000000000000000000
Loss: 16.1774
outputs.loss tensor(16.1774, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3000/15664/3000-15664-0011.flac
Waveform stats - mean: 0.0000, std: 0.0645, min: -0.6472, max: 0.7628
Resampled waveform stats - mean: 0.0000, std: 0.0645, min: -0.6472, max: 0.7628
Raw mel spectrogram stats - mean: 1.5629, std: 9.9481, min: 0.0000, max: 732.9709
Log mel spectrogram stats - mean: -6.5265, std: 4.9842, min: -13.8150, max: 6.5971
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4623, max: 2.6330
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.4619, max: 2.6328
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34816
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34816
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 243712
Inf count: 0
audio_emb.shape torch.Size([1, 68, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SLIGHT RAINSTORMS ARE LIKELY TO BE ENCOUNTERED IN A TRIP ROUND THE MOUNTAIN BUT ONE MAY EASILY FIND SHELTER BENEATH WELL THATCHED TREES THAT SHED THE RAIN LIKE A ROOF
Prediction: 0000000000000000000000000000000000000000000000000000000
Loss: 15.8845
outputs.loss tensor(15.8845, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0022.flac
Waveform stats - mean: -0.0000, std: 0.0634, min: -0.7141, max: 0.7121
Resampled waveform stats - mean: -0.0000, std: 0.0634, min: -0.7141, max: 0.7121
Raw mel spectrogram stats - mean: 1.5018, std: 11.5214, min: 0.0000, max: 1005.4001
Log mel spectrogram stats - mean: -4.3848, std: 3.3346, min: -12.7138, max: 6.9131
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4978, max: 3.3881
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4980, max: 3.3887
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 52224
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 52224
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 365568
Inf count: 0
audio_emb.shape torch.Size([1, 102, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ALSO IN ALL LOVES AND FRIENDSHIPS ONE HAS THE EXPERIENCE THAT NOTHING OF THE KIND CONTINUES WHEN THE DISCOVERY HAS BEEN MADE THAT IN USING THE SAME WORDS ONE OF THE TWO PARTIES HAS FEELINGS THOUGHTS INTUITIONS WISHES OR FEARS DIFFERENT FROM THOSE OF THE OTHER
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.2385
outputs.loss tensor(16.2385, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3536/23268/3536-23268-0025.flac
Waveform stats - mean: -0.0001, std: 0.0582, min: -0.5068, max: 0.5289
Resampled waveform stats - mean: -0.0001, std: 0.0582, min: -0.5068, max: 0.5289
Raw mel spectrogram stats - mean: 1.2695, std: 12.0957, min: 0.0000, max: 707.1653
Log mel spectrogram stats - mean: -5.0798, std: 3.6117, min: -13.5876, max: 6.5613
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3556, max: 3.2231
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3555, max: 3.2227
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16384
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16384
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 114688
Inf count: 0
audio_emb.shape torch.Size([1, 32, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT IS OFTEN THE UNGRATEFUL TASK OF A FRIEND TO BE TROUBLESOME SOMETIMES UNMANNERLY
Prediction: 00000000000000000000000000000
Loss: 15.9307
outputs.loss tensor(15.9307, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0065.flac
Waveform stats - mean: -0.0000, std: 0.0211, min: -0.1638, max: 0.1890
Resampled waveform stats - mean: -0.0000, std: 0.0211, min: -0.1638, max: 0.1890
Raw mel spectrogram stats - mean: 0.1675, std: 1.3525, min: 0.0000, max: 73.6456
Log mel spectrogram stats - mean: -7.0637, std: 3.8172, min: -13.7912, max: 4.2993
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7624, max: 2.9768
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.7627, max: 2.9766
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THIS DREAD LED HIM TO MAKE THE REMARK THAT STEVIE HAD DISREGARDED HIS SUGGESTION TO GO TO BED
Prediction: 00000000000000000000000000000
Loss: 15.9668
outputs.loss tensor(15.9668, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0006.flac
Waveform stats - mean: -0.0000, std: 0.0693, min: -0.7033, max: 0.7094
Resampled waveform stats - mean: -0.0000, std: 0.0693, min: -0.7033, max: 0.7094
Raw mel spectrogram stats - mean: 1.7940, std: 20.5533, min: 0.0000, max: 2455.4043
Log mel spectrogram stats - mean: -4.5474, std: 3.4883, min: -13.1031, max: 7.8060
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.4527, max: 3.5413
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4531, max: 3.5410
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 46080
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 46080
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 322560
Inf count: 0
audio_emb.shape torch.Size([1, 90, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HERE IS THE SEAT OF THE ORIGIN OF THE FAMOUS ANTITHESIS GOOD AND EVIL POWER AND DANGEROUSNESS ARE ASSUMED TO RESIDE IN THE EVIL A CERTAIN DREADFULNESS SUBTLETY AND STRENGTH WHICH DO NOT ADMIT OF BEING DESPISED
Prediction: 00000000000000000000000000000000000000000000000000000000000000000
Loss: 16.1020
outputs.loss tensor(16.1020, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/652/130726/652-130726-0003.flac
Waveform stats - mean: -0.0000, std: 0.0570, min: -0.5244, max: 0.4534
Resampled waveform stats - mean: -0.0000, std: 0.0570, min: -0.5244, max: 0.4534
Raw mel spectrogram stats - mean: 1.1790, std: 6.6150, min: 0.0000, max: 387.3661
Log mel spectrogram stats - mean: -4.5812, std: 3.8640, min: -13.7105, max: 5.9594
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.3627, max: 2.7279
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3633, max: 2.7285
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HERE AS WELL AS IN A NUMBER OF OTHER PLACES ONE CAN WELL APPRECIATE THE COLLOQUIAL DEFINITION OF CABARET
Prediction: 00000000000000000000000000000
Loss: 15.8004
outputs.loss tensor(15.8004, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0018.flac
Waveform stats - mean: 0.0000, std: 0.0754, min: -0.4283, max: 0.4263
Resampled waveform stats - mean: 0.0000, std: 0.0754, min: -0.4283, max: 0.4263
Raw mel spectrogram stats - mean: 2.1312, std: 19.6826, min: 0.0000, max: 856.7350
Log mel spectrogram stats - mean: -5.5510, std: 3.8122, min: -13.7527, max: 6.7531
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1515, max: 3.2276
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1523, max: 3.2285
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HIS FIRST IMPULSE WAS TO WRITE BUT FOUR WORDS IN REPLY GO TO THE DEVIL
Prediction: 00000000000000000000
Loss: 15.9165
outputs.loss tensor(15.9165, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6319/275224/6319-275224-0017.flac
Waveform stats - mean: -0.0000, std: 0.0557, min: -0.3480, max: 0.2737
Resampled waveform stats - mean: -0.0000, std: 0.0557, min: -0.3480, max: 0.2737
Raw mel spectrogram stats - mean: 1.1607, std: 7.3373, min: 0.0000, max: 270.6776
Log mel spectrogram stats - mean: -6.5519, std: 4.2417, min: -13.7195, max: 5.6009
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6898, max: 2.8650
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6895, max: 2.8652
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27136
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27136
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 189952
Inf count: 0
audio_emb.shape torch.Size([1, 53, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WEEDS MEANWHILE SPRANG UP AND A DREARY CONFUSION REIGNED IN THE ONCE ORDERLY AND BRILLIANT LITTLE GARDEN
Prediction: 000000000000000000000000000000000000
Loss: 15.6778
outputs.loss tensor(15.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0018.flac
Waveform stats - mean: 0.0000, std: 0.0941, min: -0.4961, max: 0.4209
Resampled waveform stats - mean: 0.0000, std: 0.0941, min: -0.4961, max: 0.4209
Raw mel spectrogram stats - mean: 3.3097, std: 15.2512, min: 0.0000, max: 289.1328
Log mel spectrogram stats - mean: -5.1138, std: 4.4698, min: -13.7327, max: 5.6669
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9283, max: 2.4119
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9287, max: 2.4121
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 24064
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 24064
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 168448
Inf count: 0
audio_emb.shape torch.Size([1, 47, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I SUPPOSE SEVERITY IS NECESSARY RETURNED MEEKIN THOUGH TO MY EARS A FLOGGING SOUNDS A LITTLE DISTASTEFUL
Prediction: 0000000000000000000000000000000000
Loss: 16.2261
outputs.loss tensor(16.2261, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64025/5694-64025-0023.flac
Waveform stats - mean: 0.0000, std: 0.0570, min: -0.3844, max: 0.3310
Resampled waveform stats - mean: 0.0000, std: 0.0570, min: -0.3844, max: 0.3310
Raw mel spectrogram stats - mean: 1.2144, std: 7.6910, min: 0.0000, max: 304.8065
Log mel spectrogram stats - mean: -6.2706, std: 4.5043, min: -13.8152, max: 5.7197
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6750, max: 2.6620
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6748, max: 2.6621
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27648
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27648
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 193536
Inf count: 0
audio_emb.shape torch.Size([1, 54, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ON MY TAKING THE ROPE OFF HE SHOOK HIMSELF AND SEEMED TO SAY YOU THINK THAT YOU ARE MIGHTY SMART FOLKS BUT YOU ARE A LEETLE TOO SMART
Prediction: 00000000000000000000000000000000000000000
Loss: 15.7975
outputs.loss tensor(15.7975, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/422/122949/422-122949-0019.flac
Waveform stats - mean: -0.0000, std: 0.0694, min: -0.6075, max: 0.6030
Resampled waveform stats - mean: -0.0000, std: 0.0694, min: -0.6075, max: 0.6030
Raw mel spectrogram stats - mean: 1.8043, std: 13.1598, min: 0.0000, max: 1035.8698
Log mel spectrogram stats - mean: -4.4635, std: 3.4200, min: -13.7655, max: 6.9430
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.7199, max: 3.3352
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.7207, max: 3.3359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 61440
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 61440
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 430080
Inf count: 0
audio_emb.shape torch.Size([1, 120, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE NOBLE SOUL ACCEPTS THE FACT OF HIS EGOISM WITHOUT QUESTION AND ALSO WITHOUT CONSCIOUSNESS OF HARSHNESS CONSTRAINT OR ARBITRARINESS THEREIN BUT RATHER AS SOMETHING THAT MAY HAVE ITS BASIS IN THE PRIMARY LAW OF THINGS IF HE SOUGHT A DESIGNATION FOR IT HE WOULD SAY IT IS JUSTICE ITSELF
Prediction: 0000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.5222
outputs.loss tensor(16.5222, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0080.flac
Waveform stats - mean: 0.0000, std: 0.0259, min: -0.1442, max: 0.1332
Resampled waveform stats - mean: 0.0000, std: 0.0259, min: -0.1442, max: 0.1332
Raw mel spectrogram stats - mean: 0.2513, std: 1.2207, min: 0.0000, max: 29.2031
Log mel spectrogram stats - mean: -6.5861, std: 3.7659, min: -13.8087, max: 3.3743
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9179, max: 2.6449
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9180, max: 2.6445
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 8192
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 8192
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 57344
Inf count: 0
audio_emb.shape torch.Size([1, 16, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE CAN'T STAND THE NOTION OF ANY CRUELTY
Prediction: 0000000000000
Loss: 16.1514
outputs.loss tensor(16.1514, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1272/128104/1272-128104-0012.flac
Waveform stats - mean: 0.0000, std: 0.0892, min: -0.6395, max: 0.5342
Resampled waveform stats - mean: 0.0000, std: 0.0892, min: -0.6395, max: 0.5342
Raw mel spectrogram stats - mean: 2.9768, std: 20.2752, min: 0.0000, max: 700.3299
Log mel spectrogram stats - mean: -5.3317, std: 4.2547, min: -13.7209, max: 6.5516
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 2.7929
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.9717, max: 2.7930
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17408
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17408
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 121856
Inf count: 0
audio_emb.shape torch.Size([1, 34, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: ONLY UNFORTUNATELY HIS OWN WORK NEVER DOES GET GOOD
Prediction: 00000000000000
Loss: 16.4077
outputs.loss tensor(16.4077, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43359/5536-43359-0017.flac
Waveform stats - mean: -0.0001, std: 0.0793, min: -0.3991, max: 0.4985
Resampled waveform stats - mean: -0.0001, std: 0.0793, min: -0.3991, max: 0.4985
Raw mel spectrogram stats - mean: 2.3617, std: 12.4280, min: 0.0000, max: 447.1294
Log mel spectrogram stats - mean: -4.7987, std: 4.2286, min: -13.6536, max: 6.1028
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0940, max: 2.5780
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0938, max: 2.5781
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 17920
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 17920
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 125440
Inf count: 0
audio_emb.shape torch.Size([1, 35, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT TO HAVE A FRIEND AND TO BE TRUE UNDER ANY AND ALL TRIALS IS THE MARK OF A MAN
Prediction: 0000000000000000000000
Loss: 15.2713
outputs.loss tensor(15.2713, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147960/2035-147960-0003.flac
Waveform stats - mean: -0.0001, std: 0.0503, min: -0.3207, max: 0.4095
Resampled waveform stats - mean: -0.0001, std: 0.0503, min: -0.3207, max: 0.4095
Raw mel spectrogram stats - mean: 0.9483, std: 7.6776, min: 0.0000, max: 450.9011
Log mel spectrogram stats - mean: -5.4755, std: 3.5012, min: -13.1585, max: 6.1112
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1944, max: 3.3094
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.1953, max: 3.3086
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 18944
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 18944
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 132608
Inf count: 0
audio_emb.shape torch.Size([1, 37, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THERE HAD BEEN ANOTHER BLACK FROST THE NIGHT BEFORE AND THE AIR WAS CLEAR AND HEADY AS WINE
Prediction: 0000000000000000000000000
Loss: 15.7223
outputs.loss tensor(15.7223, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0022.flac
Waveform stats - mean: 0.0013, std: 0.0219, min: -0.2719, max: 0.2871
Resampled waveform stats - mean: 0.0013, std: 0.0219, min: -0.2719, max: 0.2871
Raw mel spectrogram stats - mean: 0.1683, std: 1.4551, min: 0.0000, max: 172.0246
Log mel spectrogram stats - mean: -5.5309, std: 2.9556, min: -12.5503, max: 5.1476
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3749, max: 3.6130
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.3750, max: 3.6133
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 34304
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 34304
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 240128
Inf count: 0
audio_emb.shape torch.Size([1, 67, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE APPEARED TO KNOW FOR HE TOLD ME AT ONCE THAT HE WAS DETECTIVE GRYCE A MAN WHO HAD GROWN OLD IN SOLVING JUST SUCH BAFFLING PROBLEMS AS THESE
Prediction: 000000000000000000000000000000000000000000000
Loss: 16.0035
outputs.loss tensor(16.0035, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/174/84280/174-84280-0005.flac
Waveform stats - mean: -0.0001, std: 0.0585, min: -0.5655, max: 0.6281
Resampled waveform stats - mean: -0.0001, std: 0.0585, min: -0.5655, max: 0.6281
Raw mel spectrogram stats - mean: 1.2842, std: 7.8977, min: 0.0000, max: 299.0074
Log mel spectrogram stats - mean: -5.3821, std: 4.2686, min: -13.8153, max: 5.7005
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.9757, max: 2.5963
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9756, max: 2.5957
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 27648
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 27648
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 193536
Inf count: 0
audio_emb.shape torch.Size([1, 54, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: FOR A TIME THE DEATH OF MARY OBSCURED HER LIFE FOR ME BUT NOW HER LIVING PRESENCE IS MORE IN MY MIND AGAIN
Prediction: 0000000000000000000000000000000
Loss: 16.0084
outputs.loss tensor(16.0084, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1919/142785/1919-142785-0007.flac
Waveform stats - mean: 0.0000, std: 0.0826, min: -0.6158, max: 0.6604
Resampled waveform stats - mean: 0.0000, std: 0.0826, min: -0.6158, max: 0.6604
Raw mel spectrogram stats - mean: 2.5527, std: 15.0535, min: 0.0000, max: 914.5298
Log mel spectrogram stats - mean: -4.4045, std: 3.5622, min: -13.3183, max: 6.8184
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.5023, max: 3.1505
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.5020, max: 3.1504
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 85504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 85504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 598528
Inf count: 0
audio_emb.shape torch.Size([1, 167, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MODE CHOOSE THE GREENEST CUCUMBERS AND THOSE THAT ARE MOST FREE FROM SEEDS PUT THEM IN STRONG SALT AND WATER WITH A CABBAGE LEAF TO KEEP THEM DOWN TIE A PAPER OVER THEM AND PUT THEM IN A WARM PLACE TILL THEY ARE YELLOW THEN WASH THEM AND SET THEM OVER THE FIRE IN FRESH WATER WITH A VERY LITTLE SALT AND ANOTHER CABBAGE LEAF OVER THEM COVER VERY CLOSELY BUT TAKE CARE THEY DO NOT BOIL
Prediction: 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 16.0378
outputs.loss tensor(16.0378, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2412/153947/2412-153947-0007.flac
Waveform stats - mean: -0.0000, std: 0.0337, min: -0.5840, max: 0.4223
Resampled waveform stats - mean: -0.0000, std: 0.0337, min: -0.5840, max: 0.4223
Raw mel spectrogram stats - mean: 0.4208, std: 3.3627, min: 0.0000, max: 202.5472
Log mel spectrogram stats - mean: -6.2360, std: 3.6255, min: -13.7151, max: 5.3110
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.0629, max: 3.1849
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0625, max: 3.1855
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I MUST NOT CONCLUDE WITHOUT EXPRESSING MY MOST SINCERE THANKS TO MY CRITICS AND TO THE PUBLIC FOR THE LENIENCY AND CONSIDERATION WITH WHICH THEY HAVE TREATED MY ADVENTURES
Prediction: 00000000000000000000000000000000000000000000
Loss: 16.4333
outputs.loss tensor(16.4333, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149896/2277-149896-0011.flac
Waveform stats - mean: -0.0000, std: 0.0493, min: -0.4224, max: 0.3181
Resampled waveform stats - mean: -0.0000, std: 0.0493, min: -0.4224, max: 0.3181
Raw mel spectrogram stats - mean: 0.9108, std: 6.9663, min: 0.0000, max: 319.9955
Log mel spectrogram stats - mean: -5.5289, std: 3.5777, min: -13.6293, max: 5.7683
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.2641, max: 3.1577
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2637, max: 3.1582
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14336
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14336
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 100352
Inf count: 0
audio_emb.shape torch.Size([1, 28, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: HE GREW RESTLESS AS HE RUMINATED AND THEN DECIDED THAT PERHAPS IT WAS NOTHING
Prediction: 00000000000000000000000
Loss: 15.7118
outputs.loss tensor(15.7118, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6295/244435/6295-244435-0012.flac
Waveform stats - mean: 0.0000, std: 0.0974, min: -0.7712, max: 0.7553
Resampled waveform stats - mean: 0.0000, std: 0.0974, min: -0.7712, max: 0.7553
Raw mel spectrogram stats - mean: 3.3662, std: 21.4424, min: 0.0000, max: 1109.9572
Log mel spectrogram stats - mean: -5.1289, std: 4.8031, min: -13.7968, max: 7.0121
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.8046, max: 2.5277
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.8047, max: 2.5273
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 14848
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 14848
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 103936
Inf count: 0
audio_emb.shape torch.Size([1, 29, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: EUROPE WHICH MUST HAVE ITS COTTON WOULD FAVOR THE SUCCESS OF THE SOUTH
Prediction: 0000000000000000000
Loss: 16.4428
outputs.loss tensor(16.4428, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0037.flac
Waveform stats - mean: 0.0011, std: 0.2364, min: -0.8328, max: 0.8192
Resampled waveform stats - mean: 0.0011, std: 0.2364, min: -0.8328, max: 0.8192
Raw mel spectrogram stats - mean: 20.9441, std: 156.7751, min: 0.0000, max: 6726.5884
Log mel spectrogram stats - mean: -2.9339, std: 4.4628, min: -13.8154, max: 8.8138
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.4383, max: 2.6324
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.4375, max: 2.6328
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 21504
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 21504
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 150528
Inf count: 0
audio_emb.shape torch.Size([1, 42, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THEN THE GOOD SOUL OPENLY SHOULDERED THE BURDEN SHE HAD BORNE SO LONG IN SECRET AND BRAVELY TRUDGED ON ALONE
Prediction: 000000000000000000000000000000000000
Loss: 15.5616
outputs.loss tensor(15.5616, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1462/170145/1462-170145-0009.flac
Waveform stats - mean: -0.0005, std: 0.0560, min: -0.3500, max: 0.3054
Resampled waveform stats - mean: -0.0005, std: 0.0560, min: -0.3500, max: 0.3054
Raw mel spectrogram stats - mean: 1.1739, std: 10.8854, min: 0.0000, max: 352.6710
Log mel spectrogram stats - mean: -7.8708, std: 4.0736, min: -13.7717, max: 5.8655
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4486, max: 3.3720
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4482, max: 3.3711
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: OH BARTLEY DID YOU WRITE TO ME
Prediction: 000000000
Loss: 15.7165
outputs.loss tensor(15.7165, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5536/43358/5536-43358-0000.flac
Waveform stats - mean: -0.0001, std: 0.0510, min: -0.3093, max: 0.4705
Resampled waveform stats - mean: -0.0001, std: 0.0510, min: -0.3093, max: 0.4705
Raw mel spectrogram stats - mean: 0.9742, std: 6.1645, min: 0.0000, max: 224.7565
Log mel spectrogram stats - mean: -6.2272, std: 4.1047, min: -13.5083, max: 5.4150
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7738, max: 2.8363
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7734, max: 2.8359
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE SAVAGE PHILOSOPHER THE DUAL MIND
Prediction: 0000000000000
Loss: 16.1509
outputs.loss tensor(16.1509, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0002.flac
Waveform stats - mean: 0.0013, std: 0.0212, min: -0.1902, max: 0.1855
Resampled waveform stats - mean: 0.0013, std: 0.0212, min: -0.1902, max: 0.1855
Raw mel spectrogram stats - mean: 0.1622, std: 0.8812, min: 0.0000, max: 37.9381
Log mel spectrogram stats - mean: -5.6717, std: 2.9992, min: -12.5441, max: 3.6360
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2914, max: 3.1034
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.2910, max: 3.1035
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: YES AND A VERY RESPECTABLE ONE
Prediction: 00000000
Loss: 16.1276
outputs.loss tensor(16.1276, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/1988/147956/1988-147956-0005.flac
Waveform stats - mean: 0.0135, std: 0.0800, min: -0.4048, max: 0.4104
Resampled waveform stats - mean: 0.0135, std: 0.0800, min: -0.4048, max: 0.4104
Raw mel spectrogram stats - mean: 2.4072, std: 17.1022, min: 0.0000, max: 744.4454
Log mel spectrogram stats - mean: -5.7626, std: 4.4928, min: -13.5766, max: 6.6126
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.7392, max: 2.7544
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7393, max: 2.7539
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11264
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11264
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 78848
Inf count: 0
audio_emb.shape torch.Size([1, 22, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: VERY GLAD VERY GLAD SHE EJACULATED
Prediction: 000000000000
Loss: 15.6763
outputs.loss tensor(15.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0018.flac
Waveform stats - mean: 0.0009, std: 0.0277, min: -0.1895, max: 0.2252
Resampled waveform stats - mean: 0.0009, std: 0.0277, min: -0.1895, max: 0.2252
Raw mel spectrogram stats - mean: 0.2669, std: 1.9341, min: 0.0000, max: 71.2565
Log mel spectrogram stats - mean: -5.8914, std: 3.0715, min: -12.4672, max: 4.2663
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.1409, max: 3.3071
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1406, max: 3.3066
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 6144
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 6144
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 43008
Inf count: 0
audio_emb.shape torch.Size([1, 12, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: WHAT DOES HE WANT
Prediction: 0000
Loss: 17.1954
outputs.loss tensor(17.1954, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2035/147961/2035-147961-0006.flac
Waveform stats - mean: -0.0001, std: 0.0470, min: -0.2612, max: 0.3422
Resampled waveform stats - mean: -0.0001, std: 0.0470, min: -0.2612, max: 0.3422
Raw mel spectrogram stats - mean: 0.8283, std: 4.6562, min: 0.0000, max: 172.4324
Log mel spectrogram stats - mean: -5.8477, std: 3.8414, min: -13.6351, max: 5.1500
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0272, max: 2.8629
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.0273, max: 2.8633
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9216
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9216
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 64512
Inf count: 0
audio_emb.shape torch.Size([1, 18, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I COULD NOT TAKE MY EYES OFF THE MAN IN THE BED
Prediction: 00000000000000
Loss: 15.6900
outputs.loss tensor(15.6900, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7850/281318/7850-281318-0023.flac
Waveform stats - mean: 0.0000, std: 0.0611, min: -0.5198, max: 0.4252
Resampled waveform stats - mean: 0.0000, std: 0.0611, min: -0.5198, max: 0.4252
Raw mel spectrogram stats - mean: 1.3968, std: 13.5533, min: 0.0000, max: 950.8192
Log mel spectrogram stats - mean: -5.5405, std: 3.8650, min: -13.7513, max: 6.8573
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.1244, max: 3.2077
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.1250, max: 3.2070
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: FOR SHE HAD ONLY THE FOUNDATION LAID CRISS CROSS AS THE MAGPIE HAD SHOWN HER
Prediction: 000000000000000000000000
Loss: 15.4858
outputs.loss tensor(15.4858, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2277/149874/2277-149874-0021.flac
Waveform stats - mean: 0.0000, std: 0.0515, min: -0.3847, max: 0.3702
Resampled waveform stats - mean: 0.0000, std: 0.0515, min: -0.3847, max: 0.3702
Raw mel spectrogram stats - mean: 0.9911, std: 6.1851, min: 0.0000, max: 168.9122
Log mel spectrogram stats - mean: -6.3773, std: 4.0109, min: -13.3894, max: 5.1294
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.7483, max: 2.8689
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.7480, max: 2.8691
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 9728
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 9728
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 68096
Inf count: 0
audio_emb.shape torch.Size([1, 19, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THESE VAST BUILDINGS WHAT WERE THEY
Prediction: 0000000000
Loss: 16.7143
outputs.loss tensor(16.7143, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3853/163249/3853-163249-0001.flac
Waveform stats - mean: -0.0001, std: 0.0754, min: -0.6651, max: 0.5676
Resampled waveform stats - mean: -0.0001, std: 0.0754, min: -0.6651, max: 0.5676
Raw mel spectrogram stats - mean: 2.1180, std: 9.7910, min: 0.0000, max: 334.4133
Log mel spectrogram stats - mean: -2.9508, std: 3.1159, min: -12.6578, max: 5.8124
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -3.1153, max: 2.8124
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -3.1152, max: 2.8125
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: BUT EVEN WHILE SHE ENJOYED EVERY HOUR OF LIFE AND BEGRUDGED THE TIME GIVEN TO SLEEP SHE FELT AS IF THE DREAM WAS TOO BEAUTIFUL TO LAST AND OFTEN SAID
Prediction: 00000000000000000000000000000000000000000000000
Loss: 15.6553
outputs.loss tensor(15.6553, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/110523/7976-110523-0014.flac
Waveform stats - mean: -0.0000, std: 0.0665, min: -0.6351, max: 0.5185
Resampled waveform stats - mean: -0.0000, std: 0.0665, min: -0.6351, max: 0.5185
Raw mel spectrogram stats - mean: 1.6571, std: 15.8732, min: 0.0000, max: 1009.6523
Log mel spectrogram stats - mean: -6.6462, std: 4.4877, min: -13.8148, max: 6.9174
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.5974, max: 3.0224
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.5977, max: 3.0215
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 20480
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 20480
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 143360
Inf count: 0
audio_emb.shape torch.Size([1, 40, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GRETHEL BEGAN TO CRY BUT IT WAS ALL USELESS FOR THE OLD WITCH MADE HER DO AS SHE WANTED
Prediction: 00000000000000000000000000
Loss: 15.8128
outputs.loss tensor(15.8128, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3752/4944/3752-4944-0060.flac
Waveform stats - mean: 0.0000, std: 0.0878, min: -0.6292, max: 0.5867
Resampled waveform stats - mean: 0.0000, std: 0.0878, min: -0.6292, max: 0.5867
Raw mel spectrogram stats - mean: 2.8873, std: 20.2115, min: 0.0000, max: 865.5755
Log mel spectrogram stats - mean: -6.1590, std: 4.5840, min: -13.7471, max: 6.7634
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6553, max: 2.8190
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6553, max: 2.8184
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 13312
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 13312
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 93184
Inf count: 0
audio_emb.shape torch.Size([1, 26, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: OH GOD GIVE ME STRENGTH AID ME HELP ME
Prediction: 0000000000000
Loss: 16.3866
outputs.loss tensor(16.3866, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275154/8297-275154-0024.flac
Waveform stats - mean: 0.0000, std: 0.0473, min: -0.3256, max: 0.5499
Resampled waveform stats - mean: 0.0000, std: 0.0473, min: -0.3256, max: 0.5499
Raw mel spectrogram stats - mean: 0.8352, std: 5.0275, min: 0.0000, max: 133.1400
Log mel spectrogram stats - mean: -7.5439, std: 4.7177, min: -13.8136, max: 4.8914
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.3290, max: 2.6359
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.3291, max: 2.6367
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 11776
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 11776
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 82432
Inf count: 0
audio_emb.shape torch.Size([1, 23, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: I FEEL FOR YOU HERBERT HE SAID WARMLY
Prediction: 0000000000000
Loss: 15.7424
outputs.loss tensor(15.7424, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/7976/105575/7976-105575-0000.flac
Waveform stats - mean: -0.0000, std: 0.0560, min: -0.4216, max: 0.4048
Resampled waveform stats - mean: -0.0000, std: 0.0560, min: -0.4216, max: 0.4048
Raw mel spectrogram stats - mean: 1.1726, std: 8.8540, min: 0.0000, max: 374.5091
Log mel spectrogram stats - mean: -6.3331, std: 4.4124, min: -13.8147, max: 5.9256
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.6956, max: 2.7782
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -1.6953, max: 2.7773
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 29696
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 29696
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 207872
Inf count: 0
audio_emb.shape torch.Size([1, 58, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: GRANT WAS ONLY A FEW MILES AWAY BUT ALTHOUGH COMMANDER IN CHIEF HE KNEW NOTHING OF THE HARDEST FOUGHT BATTLE OF THE CIVIL WAR UNTIL IT WAS OVER
Prediction: 000000000000000000000000000000000000000000000
Loss: 15.8239
outputs.loss tensor(15.8239, device='cuda:0', grad_fn=<NllLossBackward0>)

Gradient statistics per layer:
cnn_layers.0.weight: mean=nan, std=nan
cnn_layers.0.bias: mean=nan, std=nan
cnn_layers.1.weight: mean=nan, std=nan
cnn_layers.1.bias: mean=nan, std=nan
cnn_layers.3.weight: mean=nan, std=nan
cnn_layers.3.bias: mean=nan, std=nan
cnn_layers.4.weight: mean=nan, std=nan
cnn_layers.4.bias: mean=nan, std=nan
cnn_layers.6.weight: mean=nan, std=nan
cnn_layers.6.bias: mean=nan, std=nan
cnn_layers.7.weight: mean=nan, std=nan
cnn_layers.7.bias: mean=nan, std=nan
cnn_layers.9.weight: mean=nan, std=nan
cnn_layers.9.bias: mean=nan, std=nan
cnn_layers.10.weight: mean=nan, std=nan
cnn_layers.10.bias: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.0.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.0.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.0.linear1.weight: mean=nan, std=nan
transformer.layers.0.linear1.bias: mean=nan, std=nan
transformer.layers.0.linear2.weight: mean=nan, std=nan
transformer.layers.0.linear2.bias: mean=nan, std=nan
transformer.layers.0.norm1.weight: mean=nan, std=nan
transformer.layers.0.norm1.bias: mean=nan, std=nan
transformer.layers.0.norm2.weight: mean=nan, std=nan
transformer.layers.0.norm2.bias: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.1.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.1.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.1.linear1.weight: mean=nan, std=nan
transformer.layers.1.linear1.bias: mean=nan, std=nan
transformer.layers.1.linear2.weight: mean=nan, std=nan
transformer.layers.1.linear2.bias: mean=nan, std=nan
transformer.layers.1.norm1.weight: mean=nan, std=nan
transformer.layers.1.norm1.bias: mean=nan, std=nan
transformer.layers.1.norm2.weight: mean=nan, std=nan
transformer.layers.1.norm2.bias: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.2.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.2.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.2.linear1.weight: mean=nan, std=nan
transformer.layers.2.linear1.bias: mean=nan, std=nan
transformer.layers.2.linear2.weight: mean=nan, std=nan
transformer.layers.2.linear2.bias: mean=nan, std=nan
transformer.layers.2.norm1.weight: mean=nan, std=nan
transformer.layers.2.norm1.bias: mean=nan, std=nan
transformer.layers.2.norm2.weight: mean=nan, std=nan
transformer.layers.2.norm2.bias: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.3.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.3.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.3.linear1.weight: mean=nan, std=nan
transformer.layers.3.linear1.bias: mean=nan, std=nan
transformer.layers.3.linear2.weight: mean=nan, std=nan
transformer.layers.3.linear2.bias: mean=nan, std=nan
transformer.layers.3.norm1.weight: mean=nan, std=nan
transformer.layers.3.norm1.bias: mean=nan, std=nan
transformer.layers.3.norm2.weight: mean=nan, std=nan
transformer.layers.3.norm2.bias: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.4.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.4.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.4.linear1.weight: mean=nan, std=nan
transformer.layers.4.linear1.bias: mean=nan, std=nan
transformer.layers.4.linear2.weight: mean=nan, std=nan
transformer.layers.4.linear2.bias: mean=nan, std=nan
transformer.layers.4.norm1.weight: mean=nan, std=nan
transformer.layers.4.norm1.bias: mean=nan, std=nan
transformer.layers.4.norm2.weight: mean=nan, std=nan
transformer.layers.4.norm2.bias: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.5.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.5.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.5.linear1.weight: mean=nan, std=nan
transformer.layers.5.linear1.bias: mean=nan, std=nan
transformer.layers.5.linear2.weight: mean=nan, std=nan
transformer.layers.5.linear2.bias: mean=nan, std=nan
transformer.layers.5.norm1.weight: mean=nan, std=nan
transformer.layers.5.norm1.bias: mean=nan, std=nan
transformer.layers.5.norm2.weight: mean=nan, std=nan
transformer.layers.5.norm2.bias: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.6.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.6.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.6.linear1.weight: mean=nan, std=nan
transformer.layers.6.linear1.bias: mean=nan, std=nan
transformer.layers.6.linear2.weight: mean=nan, std=nan
transformer.layers.6.linear2.bias: mean=nan, std=nan
transformer.layers.6.norm1.weight: mean=nan, std=nan
transformer.layers.6.norm1.bias: mean=nan, std=nan
transformer.layers.6.norm2.weight: mean=nan, std=nan
transformer.layers.6.norm2.bias: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.7.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.7.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.7.linear1.weight: mean=nan, std=nan
transformer.layers.7.linear1.bias: mean=nan, std=nan
transformer.layers.7.linear2.weight: mean=nan, std=nan
transformer.layers.7.linear2.bias: mean=nan, std=nan
transformer.layers.7.norm1.weight: mean=nan, std=nan
transformer.layers.7.norm1.bias: mean=nan, std=nan
transformer.layers.7.norm2.weight: mean=nan, std=nan
transformer.layers.7.norm2.bias: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.8.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.8.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.8.linear1.weight: mean=nan, std=nan
transformer.layers.8.linear1.bias: mean=nan, std=nan
transformer.layers.8.linear2.weight: mean=nan, std=nan
transformer.layers.8.linear2.bias: mean=nan, std=nan
transformer.layers.8.norm1.weight: mean=nan, std=nan
transformer.layers.8.norm1.bias: mean=nan, std=nan
transformer.layers.8.norm2.weight: mean=nan, std=nan
transformer.layers.8.norm2.bias: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.9.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.9.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.9.linear1.weight: mean=nan, std=nan
transformer.layers.9.linear1.bias: mean=nan, std=nan
transformer.layers.9.linear2.weight: mean=nan, std=nan
transformer.layers.9.linear2.bias: mean=nan, std=nan
transformer.layers.9.norm1.weight: mean=nan, std=nan
transformer.layers.9.norm1.bias: mean=nan, std=nan
transformer.layers.9.norm2.weight: mean=nan, std=nan
transformer.layers.9.norm2.bias: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.10.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.10.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.10.linear1.weight: mean=nan, std=nan
transformer.layers.10.linear1.bias: mean=nan, std=nan
transformer.layers.10.linear2.weight: mean=nan, std=nan
transformer.layers.10.linear2.bias: mean=nan, std=nan
transformer.layers.10.norm1.weight: mean=nan, std=nan
transformer.layers.10.norm1.bias: mean=nan, std=nan
transformer.layers.10.norm2.weight: mean=nan, std=nan
transformer.layers.10.norm2.bias: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.11.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.11.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.11.linear1.weight: mean=nan, std=nan
transformer.layers.11.linear1.bias: mean=nan, std=nan
transformer.layers.11.linear2.weight: mean=nan, std=nan
transformer.layers.11.linear2.bias: mean=nan, std=nan
transformer.layers.11.norm1.weight: mean=nan, std=nan
transformer.layers.11.norm1.bias: mean=nan, std=nan
transformer.layers.11.norm2.weight: mean=nan, std=nan
transformer.layers.11.norm2.bias: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.12.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.12.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.12.linear1.weight: mean=nan, std=nan
transformer.layers.12.linear1.bias: mean=nan, std=nan
transformer.layers.12.linear2.weight: mean=nan, std=nan
transformer.layers.12.linear2.bias: mean=nan, std=nan
transformer.layers.12.norm1.weight: mean=nan, std=nan
transformer.layers.12.norm1.bias: mean=nan, std=nan
transformer.layers.12.norm2.weight: mean=nan, std=nan
transformer.layers.12.norm2.bias: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.13.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.13.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.13.linear1.weight: mean=nan, std=nan
transformer.layers.13.linear1.bias: mean=nan, std=nan
transformer.layers.13.linear2.weight: mean=nan, std=nan
transformer.layers.13.linear2.bias: mean=nan, std=nan
transformer.layers.13.norm1.weight: mean=nan, std=nan
transformer.layers.13.norm1.bias: mean=nan, std=nan
transformer.layers.13.norm2.weight: mean=nan, std=nan
transformer.layers.13.norm2.bias: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.14.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.14.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.14.linear1.weight: mean=nan, std=nan
transformer.layers.14.linear1.bias: mean=nan, std=nan
transformer.layers.14.linear2.weight: mean=nan, std=nan
transformer.layers.14.linear2.bias: mean=nan, std=nan
transformer.layers.14.norm1.weight: mean=nan, std=nan
transformer.layers.14.norm1.bias: mean=nan, std=nan
transformer.layers.14.norm2.weight: mean=nan, std=nan
transformer.layers.14.norm2.bias: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.15.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.15.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.15.linear1.weight: mean=nan, std=nan
transformer.layers.15.linear1.bias: mean=nan, std=nan
transformer.layers.15.linear2.weight: mean=nan, std=nan
transformer.layers.15.linear2.bias: mean=nan, std=nan
transformer.layers.15.norm1.weight: mean=nan, std=nan
transformer.layers.15.norm1.bias: mean=nan, std=nan
transformer.layers.15.norm2.weight: mean=nan, std=nan
transformer.layers.15.norm2.bias: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.16.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.16.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.16.linear1.weight: mean=nan, std=nan
transformer.layers.16.linear1.bias: mean=nan, std=nan
transformer.layers.16.linear2.weight: mean=nan, std=nan
transformer.layers.16.linear2.bias: mean=nan, std=nan
transformer.layers.16.norm1.weight: mean=nan, std=nan
transformer.layers.16.norm1.bias: mean=nan, std=nan
transformer.layers.16.norm2.weight: mean=nan, std=nan
transformer.layers.16.norm2.bias: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.17.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.17.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.17.linear1.weight: mean=nan, std=nan
transformer.layers.17.linear1.bias: mean=nan, std=nan
transformer.layers.17.linear2.weight: mean=nan, std=nan
transformer.layers.17.linear2.bias: mean=nan, std=nan
transformer.layers.17.norm1.weight: mean=nan, std=nan
transformer.layers.17.norm1.bias: mean=nan, std=nan
transformer.layers.17.norm2.weight: mean=nan, std=nan
transformer.layers.17.norm2.bias: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.18.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.18.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.18.linear1.weight: mean=nan, std=nan
transformer.layers.18.linear1.bias: mean=nan, std=nan
transformer.layers.18.linear2.weight: mean=nan, std=nan
transformer.layers.18.linear2.bias: mean=nan, std=nan
transformer.layers.18.norm1.weight: mean=nan, std=nan
transformer.layers.18.norm1.bias: mean=nan, std=nan
transformer.layers.18.norm2.weight: mean=nan, std=nan
transformer.layers.18.norm2.bias: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.19.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.19.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.19.linear1.weight: mean=nan, std=nan
transformer.layers.19.linear1.bias: mean=nan, std=nan
transformer.layers.19.linear2.weight: mean=nan, std=nan
transformer.layers.19.linear2.bias: mean=nan, std=nan
transformer.layers.19.norm1.weight: mean=nan, std=nan
transformer.layers.19.norm1.bias: mean=nan, std=nan
transformer.layers.19.norm2.weight: mean=nan, std=nan
transformer.layers.19.norm2.bias: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.20.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.20.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.20.linear1.weight: mean=nan, std=nan
transformer.layers.20.linear1.bias: mean=nan, std=nan
transformer.layers.20.linear2.weight: mean=nan, std=nan
transformer.layers.20.linear2.bias: mean=nan, std=nan
transformer.layers.20.norm1.weight: mean=nan, std=nan
transformer.layers.20.norm1.bias: mean=nan, std=nan
transformer.layers.20.norm2.weight: mean=nan, std=nan
transformer.layers.20.norm2.bias: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.21.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.21.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.21.linear1.weight: mean=nan, std=nan
transformer.layers.21.linear1.bias: mean=nan, std=nan
transformer.layers.21.linear2.weight: mean=nan, std=nan
transformer.layers.21.linear2.bias: mean=nan, std=nan
transformer.layers.21.norm1.weight: mean=nan, std=nan
transformer.layers.21.norm1.bias: mean=nan, std=nan
transformer.layers.21.norm2.weight: mean=nan, std=nan
transformer.layers.21.norm2.bias: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.22.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.22.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.22.linear1.weight: mean=nan, std=nan
transformer.layers.22.linear1.bias: mean=nan, std=nan
transformer.layers.22.linear2.weight: mean=nan, std=nan
transformer.layers.22.linear2.bias: mean=nan, std=nan
transformer.layers.22.norm1.weight: mean=nan, std=nan
transformer.layers.22.norm1.bias: mean=nan, std=nan
transformer.layers.22.norm2.weight: mean=nan, std=nan
transformer.layers.22.norm2.bias: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_weight: mean=nan, std=nan
transformer.layers.23.self_attn.in_proj_bias: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.weight: mean=nan, std=nan
transformer.layers.23.self_attn.out_proj.bias: mean=nan, std=nan
transformer.layers.23.linear1.weight: mean=nan, std=nan
transformer.layers.23.linear1.bias: mean=nan, std=nan
transformer.layers.23.linear2.weight: mean=nan, std=nan
transformer.layers.23.linear2.bias: mean=nan, std=nan
transformer.layers.23.norm1.weight: mean=nan, std=nan
transformer.layers.23.norm1.bias: mean=nan, std=nan
transformer.layers.23.norm2.weight: mean=nan, std=nan
transformer.layers.23.norm2.bias: mean=nan, std=nan
connector.0.weight: mean=nan, std=nan
connector.0.bias: mean=nan, std=nan
connector.2.weight: mean=nan, std=nan
connector.2.bias: mean=0.0000, std=0.0000
Gradient norm: nan
Model hidden size: 3584
Loading audio file: data/librispeech/LibriSpeech/dev-clean/5694/64029/5694-64029-0001.flac
Waveform stats - mean: 0.0000, std: 0.0515, min: -0.3306, max: 0.3042
Resampled waveform stats - mean: 0.0000, std: 0.0515, min: -0.3306, max: 0.3042
Raw mel spectrogram stats - mean: 0.9903, std: 4.9152, min: 0.0000, max: 160.1776
Log mel spectrogram stats - mean: -6.9991, std: 5.0528, min: -13.8155, max: 5.0763
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.3490, max: 2.3899
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.3486, max: 2.3906
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 15872
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 15872
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 111104
Inf count: 0
audio_emb.shape torch.Size([1, 31, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: THE FEDERAL ARMY WAS CONCENTRATING AT NASHVILLE THERE WAS NO REST FOR THE WEARY
Prediction: 00000000000000000000000
Loss: 16.2393
outputs.loss tensor(16.2393, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/8297/275156/8297-275156-0004.flac
Waveform stats - mean: 0.0000, std: 0.0587, min: -0.3316, max: 0.5975
Resampled waveform stats - mean: 0.0000, std: 0.0587, min: -0.3316, max: 0.5975
Raw mel spectrogram stats - mean: 1.2891, std: 7.7182, min: 0.0000, max: 243.3854
Log mel spectrogram stats - mean: -7.3827, std: 4.9838, min: -13.8151, max: 5.4946
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.2907, max: 2.5839
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.2910, max: 2.5840
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SUPPOSING THE REPORT HAD BEEN TRUE
Prediction: 000000000
Loss: 16.0945
outputs.loss tensor(16.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/777/126732/777-126732-0063.flac
Waveform stats - mean: -0.0000, std: 0.0213, min: -0.1849, max: 0.2746
Resampled waveform stats - mean: -0.0000, std: 0.0213, min: -0.1849, max: 0.2746
Raw mel spectrogram stats - mean: 0.1705, std: 0.9170, min: 0.0000, max: 36.0933
Log mel spectrogram stats - mean: -7.1644, std: 3.9847, min: -13.7898, max: 3.5861
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.6627, max: 2.6980
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.6631, max: 2.6973
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 58368
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 58368
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 408576
Inf count: 0
audio_emb.shape torch.Size([1, 114, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: DOWN BELOW IN THE QUIET NARROW STREET MEASURED FOOTSTEPS APPROACHED THE HOUSE THEN DIED AWAY UNHURRIED AND FIRM AS IF THE PASSER BY HAD STARTED TO PACE OUT ALL ETERNITY FROM GAS LAMP TO GAS LAMP IN A NIGHT WITHOUT END AND THE DROWSY TICKING OF THE OLD CLOCK ON THE LANDING BECAME DISTINCTLY AUDIBLE IN THE BEDROOM
Prediction: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
Loss: 15.9238
outputs.loss tensor(15.9238, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/66616/6241-66616-0013.flac
Waveform stats - mean: -0.0002, std: 0.0735, min: -0.3793, max: 0.4611
Resampled waveform stats - mean: -0.0002, std: 0.0735, min: -0.3793, max: 0.4611
Raw mel spectrogram stats - mean: 2.0070, std: 14.6566, min: 0.0000, max: 694.8355
Log mel spectrogram stats - mean: -4.6517, std: 3.8907, min: -13.6367, max: 6.5437
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -2.3094, max: 2.8775
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -2.3086, max: 2.8770
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 16896
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 16896
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 118272
Inf count: 0
audio_emb.shape torch.Size([1, 33, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IT WAS AT ABOUT THIS TIME IN THEIR LIVES THAT THE WOONGAS BECAME ESPECIALLY DARING IN THEIR DEPREDATIONS
Prediction: 0000000000000000000000000000000
Loss: 16.0426
outputs.loss tensor(16.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/6241/61946/6241-61946-0010.flac
Waveform stats - mean: -0.0000, std: 0.0679, min: -0.5829, max: 0.5194
Resampled waveform stats - mean: -0.0000, std: 0.0679, min: -0.5829, max: 0.5194
Raw mel spectrogram stats - mean: 1.7179, std: 10.6781, min: 0.0000, max: 416.5235
Log mel spectrogram stats - mean: -4.9575, std: 4.0837, min: -13.3807, max: 6.0319
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.0627, max: 2.6911
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.0625, max: 2.6914
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10752
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10752
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 75264
Inf count: 0
audio_emb.shape torch.Size([1, 21, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: MY ARMS ARE RIGHT BUT MY LEGS ARE GETTING A LITTLE STIFF
Prediction: 00000000000000000
Loss: 15.9260
outputs.loss tensor(15.9260, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2086/149220/2086-149220-0035.flac
Waveform stats - mean: 0.0000, std: 0.0721, min: -0.6439, max: 0.7632
Resampled waveform stats - mean: 0.0000, std: 0.0721, min: -0.6439, max: 0.7632
Raw mel spectrogram stats - mean: 1.9505, std: 14.8324, min: 0.0000, max: 834.1081
Log mel spectrogram stats - mean: -6.5735, std: 4.8947, min: -13.8106, max: 6.7264
Final mel spectrogram stats - mean: 0.0000, std: 1.0000, min: -1.4786, max: 2.7172
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.4785, max: 2.7168
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 33792
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 33792
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 236544
Inf count: 0
audio_emb.shape torch.Size([1, 66, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: IS THERE NOTHING WILD IN THE EYE CONTINUED HOLGRAVE SO EARNESTLY THAT IT EMBARRASSED PHOEBE AS DID ALSO THE QUIET FREEDOM WITH WHICH HE PRESUMED ON THEIR SO RECENT ACQUAINTANCE
Prediction: 00000000000000000000000000000000000000000000000000000000
Loss: 15.9238
outputs.loss tensor(15.9238, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/2078/142845/2078-142845-0027.flac
Waveform stats - mean: -0.0000, std: 0.0639, min: -0.5281, max: 0.3546
Resampled waveform stats - mean: -0.0000, std: 0.0639, min: -0.5281, max: 0.3546
Raw mel spectrogram stats - mean: 1.5201, std: 9.7131, min: 0.0000, max: 483.2047
Log mel spectrogram stats - mean: -4.9816, std: 3.7311, min: -13.5443, max: 6.1804
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -2.2949, max: 2.9916
Audio encoder input stats - mean: 0.0000, std: 1.0000, min: -2.2949, max: 2.9922
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 28160
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 28160
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 197120
Inf count: 0
audio_emb.shape torch.Size([1, 55, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: NEVER USE NEW BREAD FOR MAKING ANY KIND OF TOAST AS IT EATS HEAVY AND BESIDES IS VERY EXTRAVAGANT
Prediction: 00000000000000000000000000000000
Loss: 15.7625
outputs.loss tensor(15.7625, device='cuda:0', grad_fn=<NllLossBackward0>)
Loading audio file: data/librispeech/LibriSpeech/dev-clean/3081/166546/3081-166546-0038.flac
Waveform stats - mean: 0.0010, std: 0.0148, min: -0.0943, max: 0.1623
Resampled waveform stats - mean: 0.0010, std: 0.0148, min: -0.0943, max: 0.1623
Raw mel spectrogram stats - mean: 0.0800, std: 0.3608, min: 0.0000, max: 10.0723
Log mel spectrogram stats - mean: -6.5917, std: 3.1617, min: -12.7341, max: 2.3098
Final mel spectrogram stats - mean: -0.0000, std: 1.0000, min: -1.9427, max: 2.8154
Audio encoder input stats - mean: -0.0000, std: 1.0000, min: -1.9424, max: 2.8145
CNN output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after CNN!
NaN count: 10240
Inf count: 0
Transformer output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected after transformer!
NaN count: 10240
Inf count: 0
Final output stats - mean: nan, std: nan, min: nan, max: nan
Warning: NaN or Inf values detected in final output!
NaN count: 71680
Inf count: 0
audio_emb.shape torch.Size([1, 20, 3584])
Audio embedding stats - mean: 0.0000, std: 0.0000

Sample prediction:
Target: SHE HAD NO COMPANION NEAR HER
Prediction: 00000000000
Loss: 15.3453
outputs.loss tensor(15.3453, device='cuda:0', grad_fn=<NllLossBackward0>)
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mworthy-elevator-45[0m at: [34mhttps://wandb.ai/aidando73-personal/jarvis-social-iq-module/runs/lcyv3j48[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250415_061652-lcyv3j48/logs[0m
